---
title: "STT481_Hw1"
author: "Sam Isken"
date: "September 19, 2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
---
The chunk below imports packages needed for all chunks in the RMD. 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(knitr)
```

This RMD file contains all the homework questions for STT 481 - Assignment 1

Question 1. 

Find your teammates (up to 3 students in a group) and create an account on Kaggle. 

```{r}
include_graphics('./Kaggle screen shot STTT 481.png')
```

Question 2. 

Finish the swirl course “R Programming”. Finish Section 1-15. 

```{r}
#include_graphics('./swirl_hw_1_Ex1.PNG')
#include_graphics('./swirl_hw_1_Ex2.PNG')
#include_graphics('./swirl_hw_1_Ex3.PNG')
#include_graphics('./swirl_hw_1_Ex4.PNG')
#include_graphics('./swirl_hw_1_Ex5.PNG')
#include_graphics('./swirl_hw_1_Ex6.PNG')
#include_graphics('./swirl_hw_1_Ex7.PNG')
#include_graphics('./swirl_hw_1_Ex8.PNG')
#include_graphics('./swirl_hw_1_Ex9.PNG')
#include_graphics('./swirl_hw_1_Ex10.PNG')
#include_graphics('./swirl_hw_1_Ex11.PNG')
#include_graphics('./swirl_hw_1_Ex12.PNG')
#include_graphics('./swirl_hw_1_Ex13.PNG')
#include_graphics('./swirl_hw_1_Ex14.PNG')
#include_graphics('./swirl_hw_1_Ex15.PNG')
```

# Section 2.4: Question #7

The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

```{r}
#Let's first replicate the df in question #7 from section 2.4 
X1 <- c(0,2,0,0,-1,1)
X2 <- c(3,0,1,1,0,1)
X3 <- c(0,0,3,2,1,1)
Y <- c("Red","Red","Red","Green","Green","Red")
training_data <- data.frame(X1,X2,X3,Y) 
training_data
```

The Euclidean distance of points p and q is the length of the line segement connecting them. 

$$ d(p,q) =  \sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + ... + (q_n-p_n)^2} = \sqrt{\Sigma(q_i - p_i)^2} $$

(a) Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$.

Given the Euclidean distance formula above we can compute the points as follows. 

Observation 1: $\sqrt{(0-0)^2 + (3-0)^2 + (0-0)^2} = 3$

Observation 2: $\sqrt{(2-0)^2 + (0-0)^2 + (0-0)^2} = 2$

Observation 3: $\sqrt{(0-0)^2 + (1-0)^2 + (3-0)^2} = 3.16$

Observation 4: $\sqrt{(0-0)^2 + (1-0)^2 + (2-0)^2} = 2.24$

Observation 5: $\sqrt{(-1-0)^2 + (0-0)^2 + (1-0)^2} = 1.41$

Observation 6: $\sqrt{(1-0)^2 + (1-0)^2 + (1-0)^2} = 1.73$


(b) What is our prediction with $K = 1$? Why?

When K=1 we predict Green as most of the outcomes surrounding K=1 are green. 

(c) What is our prediction with $K = 3$? Why? 

When K=3 we predict red as most of the outcomes surrounding K=3 are red. 

(d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?

As K becomes large K approaches a linear shape. We want our KNN alogorithm to be flexible, thus we think the best K value would be small. 

# Section 2.4: Question #8

(a) Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data

```{r}
#(a)
college <- read.csv("College.csv",header = TRUE)
#college
```

(b) Look at the data using the fix() function. You should notice that the first column is just the name of each university. We don't really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:

```{r}
#(b)
#Look at data using fix()
fix(college)

#Try the following commands
rownames(college)=college[,1]
fix(college)
#college

college=college[,-1]
fix(college)

#Display college, see row.names is set and that extra (prior) column is removed 
college
```

(c) 

i. Use the summary() function to produce a numerical summary of the variables in the data set. 

```{r}
#(c) i.
#Use summary function on data set 'college'
summary(college)
```

ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10]. 

```{r}
#(c) ii. 
#Use pairs function to create scatterplot matrix of first 10 columns of data set 'college'
pairs(college[1:10])
```

iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private.

```{r}
#(c) iii.
boxplot(college$Outstate, college$Private)
```


iv. Create a new qualitative variable, called Elite, bybinning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.

```{r}
#(c) iv.

#Creates a vector of "No"'s dependant on the number of rows in the table
Elite=rep("No",nrow(college))  

#Sets the values with a percent of over 50% to be a string "Yes"
Elite[college$Top10perc>50]="Yes"

#Coded "Elite" as a factory (aka category or enumerated type)
Elite=as.factor(Elite)

#Adds 'Elite' column to data set 'college'
college <- data.frame(college ,Elite)
college

#Run summary function on cllege and ensure Eliste is contained 
summary(college)

#Get Count of Elite Colleges
number_of_elite <- sum(college$Elite=="Yes")

#Get Percent of Colleges Elite
percent_of_elite <- number_of_elite / (length(college$Elite))

#Boxplot of Out of State vs. Elise 
boxplot(college$Outstate,college$Elite)
```
v. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.

```{r}
#(c) v.
#Histograms for College$P.Undergrad

#Divide window into  2x2 matrix
par(mfrow=c(2,2))

#Produce 4 histograms with differing numbers of bins (designated by 'break' parameter)
hist(College$P.Undergrad, breaks= 2 )
hist(College$P.Undergrad, breaks= 6 )
hist(College$P.Undergrad, breaks= 9 )
hist(College$P.Undergrad, breaks= 45 )
```

```{r}
#(c) v.
#Histograms for College$Room.Board

#Divide window into  2x2 matrix
par(mfrow=c(2,2))

#Produce 4 histograms with differing numbers of bins (designated by 'break' parameter)
hist(College$Room.Board, breaks= 2 )
hist(College$Room.Board, breaks= 6 )
hist(College$Room.Board, breaks= 9 )
hist(College$Room.Board, breaks= 45 )
```

```{r}
#(c) v.
#Histograms for College$Books

#Divide window into  2x2 matrix
par(mfrow=c(2,2))

#Produce 4 histograms with differing numbers of bins (designated by 'break' parameter)
hist(College$Books, breaks= 2 )
hist(College$Books, breaks= 6 )
hist(College$Books, breaks= 9 )
hist(College$Books, breaks= 45 )
```
vi. Continue exploring the data, and provide a brief summary of what you discover

```{r}
#(c) vi.
#This did not end up being useful as there are too many variables 
pairs(College)
College_Private = College[College$Private == 'Yes',]
College_Public = College[College$Private == 'No',]
#Create histograms to compare college cost 
par(mfrow=c(2,2))
hist(College_Private$Books)
hist(College_Public$Books)

hist(College_Private$Room.Board)
hist(College_Public$Room.Board)

```

I decided to specifically explore the relationship between a school being public or private and how this effects "room and board" and "book" costs. From the histograms I produced and the datasets I created to do this it is clear that statistically the costs of "room and board" and "books" are, on average, lower at public schools and higher at private. 

```{r}
#(c) vi.
avg_g_rate_priv <- mean(College_Private$Grad.Rate)
avg_g_rate_pub <- mean(College_Public$Grad.Rate)

t.test(College_Private$Grad.Rate,College_Public$Grad.Rate)
```
I then examined the graduation rates between the two categories of insitutions and found there was a significant difference in their graduation rates. 

This was interesting because I was wondering if higher costs would actually be reflected in the graduation rates. 

Section 2.4: Question #9. 

This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.

```{r}
#9
#View data and remove columns with missing data points 
#Auto

#Removes any rows with a missing data point 
Auto_rm <- na.omit(Auto)
#Auto_rm
```

(a) Which of the predictors are quantitative, and which are qualitative?

```{r}
#View all variables using summary function  
summary(Auto_rm)

#Create a vector of all the quantitative variables 
quant_vars <- c("mpg","cylinders","displacement","horsepower","weight","acceleration")

#Create a vector of all the qualitative variables 
qual_vars <- c("name","year","origin",NA,NA,NA)

#Create and display columns containing quant_vars and qual_vars (quantitative variables and qualitative variables respectively)
Variable_Type <- data.frame(quant_vars,qual_vars)
Variable_Type
```

(b) What is the range of each quantitative predictor? You can answer this using the range() function.

```{r}
#Calculates range of all quantitative variables variables 
range(Auto_rm$mpg)
range(Auto_rm$cylinders)	
range(Auto_rm$displacement)		
range(Auto_rm$horsepower)		
range(Auto_rm$weight)		
range(Auto_rm$acceleration)		
```

(c) What is the mean and standard deviation of each quantitative predictor? 

```{r}
#(c)
#Calc mean and sd of mpg
mean(Auto_rm$mpg)
sd(Auto_rm$mpg)

#Calc mean and sd of cylinders
mean(Auto_rm$cylinders)
sd(Auto_rm$cylinders)

#Calc mean and sd of displacement
mean(Auto_rm$displacement)
sd(Auto_rm$displacement)

#Calc mean and sd of horsepower
mean(Auto_rm$horsepower)
sd(Auto_rm$horsepower)

#Calc mean and sd of weight
mean(Auto_rm$weight)
sd(Auto_rm$weight)

#Calc mean and sd of acceleration
mean(Auto_rm$acceleration)
sd(Auto_rm$acceleration)
```

(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains

```{r}
#(d)
#Create  data set of rows 10 - 85 of 'Auto_rm'
Auto_rm2 <- Auto_rm[c(-10:-85),]
#Display new data set 'Auto_rm2'
#Auto_rm2


#Returns means of key quantitative columns 
colMeans(Auto_rm2[1:6])
sapply(Auto_rm2[1:6], sd)
sapply(Auto_rm2[1:6], range)
```

(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. 

```{r}
pairs(Auto_rm[1:6])
plot(lm(mpg~ ., data = Auto_rm[1:6]))
plot(lm(cylinders~ ., data = Auto_rm[1:6]))
plot(lm(displacement~ ., data = Auto_rm[1:6]))
plot(lm(horsepower~ ., data = Auto_rm[1:6]))
plot(lm(weight~ ., data = Auto_rm[1:6]))
plot(lm(acceleration~ ., data = Auto_rm[1:6]))

cor(Auto_rm[1:6])
cor(Auto_rm[1:6])>.75
#I created a heat map but it did not end up being useful 
#heatmap(as.matrix(Auto_rm[1:6]), scale="column", col = cm.colors(256))
```

From this correlation matrix we can see that many of the variables correlate with eachother with a value >.75. This means further examination of the regression plots could lead to determing some meaningful data for regressions. The first plot shows those relationships. The plots with upward sloping, correlated data points would be useful predictors. 


(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer

```{r}
lm(mpg~.,data=Auto_rm[1:6])
summary(lm(mpg~.,data=Auto_rm[1:6]))
plot(lm(mpg~.,data=Auto_rm[1:6]))
```

The only statistically significant predictors in this case are horsepower and weight. These would be the two predictors I would use. 

# Section 3.7 Exercises: Question #3 

3. Suppose we have a data set with five predictors, $X_1 =$ GPA,$X_2 =$ IQ, $X_3 =$ Gender (1 for Female and 0 for Male), $X_4 =$ Interaction between GPA and IQ, and $X_5 =$ Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to ï¬t the model, and get $\hat \beta_0 = 50$, $\hat \beta_1 = 20$, $\hat \beta_2 =0 .07$, $\hat \beta_3 = 35$ , $\hat \beta_4 =0 .01$, $\hat \beta_5= -10$.

$$ \hat{Y} =  20\text{GPA} + .07\text{IQ}  + 35\text{GENDER} + .01\text{GPA*IQ}+-10 \text{(GPA * GENDER)}+50$$
 
(a) Which answer is correct, and why? 

i. For a fixed value of IQ and GPA, males earn more on average than females.
ii. For a fixed value of IQ and GPA, females earn more on average than males. 
iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough. 
iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough. 

iii. is the correct answer since when the indicator variable (GENDER) is equal to one (which occurs when data point is female), given GPA fixed the predicted value will decrease. 

(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0. 

```{r}
IQ=110
GPA=4
GENDER=1
Yhat_salary = (20*GPA) + (.07*IQ)+(35*GENDER)+(.01*GPA*IQ)+(-10*(GPA*GENDER))+50
print(Yhat_salary)
```

(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.

This is not true. To confirm whether or not this was statistically significant we would examine the F-Statistic of the model overall as well as the p-value of the interaction variable. This would all be shown in the summary of our model. 

# Section 3.7 Exercises: Question #4

I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then ï¬t a linear regression model to the data, as well as a separate cubic regression, i.e. $ Y = \beta_0 + \beta_1 X +\beta_2 X^2 + \beta_3 X^3 + \epsilon$.

(a) Suppose that the true relationship between X and Y is linear, i.e. $Y=\beta_0 + \beta_1 X +\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

$$ R S S=\sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2} $$
  
RSS tends to decrease (slightly) as higher power polynomial models are used. 

```{r}
resid_matrix = matrix(rep(0,5000), nrow=1000)
for(i in 1:1000){
  n = 100
  x = rnorm(n)
  y = 5 + 2 * x + rnorm(n, 0.5)
  for(j in 1:5){
  resid_matrix[i,j] = sum(residuals(lm(y ~ poly(x,j,raw=T)))^2)
  }              
}
boxplot(resid_matrix)
```

(b) Answer (a) using test rather than training RSS. 

This would cause the inverse to occur as our predicted values are known (a cubic regression model would cause overfitting). We would then see that the cubic RSS would be higher.  

(c) Suppose that the true relationship between X and Y is not linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer. 

We would expect the cubic RSS to be lower based on our logic and boxplot from the prior question. 

(d) Answer (c) using test rather than training RSS.

By our prior logic the cubic regression RSS would be higher. 

# Section 3.7 Exercises: Question #9

This question involves the use of multiple linear regression on the Auto data set.

(a) Produce a scatterplot matrix which includes all of the variables in the data set. 

```{r}
#(a)
#Summarizes 
summary(Auto_rm)
#Creates matrix of scatter plots containing all variables in data set
pairs(Auto_rm)
```

(b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.

```{r}
#(b)
#create matrix of correlations, excluding last name columns
pairs(cor(Auto_rm[1:8]))
```

(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance: 

```{r}
model.lm1 <- lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin, data = Auto_rm)
summary(model.lm1)
```

i. Is there a relationship between the predictors and the response? 

Displacement, weight, year and origin are all statistically significant predictors in this regression. Since our F-Statistic is large as well we can say there appears to be a relationship between at least one of our predictors and the response.

ii. Which predictors appear to have a statistically significant relationship to the response? 

Displacement, weight, year and origin are all statistically significant predictors in this regression.

iii. What does the coefficient for the year variable suggest? 

While holding all other effects fixed a 1 unit increase year is associated with a .75 increase in mpg. 

(d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit? 

```{r}
plot(model.lm1)
```

Do the residual plots suggest any unusually large outliers? 

The QQPlot suggests that though many residuals are distributed normally with $ E(\epsilon) = 0$ there does exists some outliers. 

Does the leverage plot identify any observations with unusually high leverage? 

327,394, and 14 are all leverage points with unusually high leverage on the plot. 


(e) Use the * and : symbols to fit linear regression models with interaction effects. 

Do any interactions appear to be statistically significant?
```{r}
model.lm2 <- lm(mpg~cylinders*displacement*horsepower*weight*acceleration*year*origin, data = Auto_rm)
summary(model.lm2)

#Model taking into account my intuition and prior knowledge of the data set
model.lm3 <- lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin+ year:mpg+year:horsepower, data = Auto_rm)
summary(model.lm3)
```

I first ran this regression using interaction variables on all the variables. I then used my personal intuition to create variables examining the interaction effect between year and mpg and year and horsepower (both things one would anticipate to be improved on newer cars). Both of these interaction variables were statistically significant. 

(f) Try a few different transformations of the variables, such as $log(X), \sqrt{X}, X^2$. Comment on your findings.

```{r}
model.lm3 <- lm(log(mpg)~cylinders+displacement+horsepower+weight+acceleration+year+origin, data = Auto_rm)
summary(model.lm3)

model.lm4 <- lm(sqrt(mpg)~cylinders+displacement+horsepower+weight+acceleration+year+origin, data = Auto_rm)
summary(model.lm4)

model.lm5 <- lm(mpg~cylinders+displacement+horsepower+weight+(acceleration*year)^2+origin, data = Auto_rm)
summary(model.lm5)
```

The log transformation produced the highest F-Statistic and all predictors except acceleration and origin were classified as statistically significant. 

10. This question should be answered using the Carseats data set.

(a) Fit a multiple regression model to predict Sales using Price, Urban, and US. 

```{r}
#Carseats
Carseats_Model1 <- lm(Sales~Price+Urban+US, data = Carseats)
summary(Carseats_Model1)
```

(b) Provide an interpretation of each coeﬃcient in the model. Be careful—some of the variables in the model are qualitative! 

Price: Given a 1 unit increase in price the sales of a carseat will decrease .05.
UrbanYes: An indicator variable set equal to 1 if a customer resides in an urban area, thus if UrbanYes = 1 the Sales of a carseat will decrease by .021
USYes: An indicator variable set equal to 1 if a customer resides in the US, thus if USYes = 1 the sales of a carseat will increase by 1.2

(c) Write out the model in equation form, being careful to handle the qualitative variables properly.

$$ Sales = 13.04 + \text{Price}*(-.05)+\text{UrbanYes}*(-.02)+\text{USYes}*(1.2)+\epsilon$$

(d) For which of the predictors can you reject the null hypothesis $H_0 : β_j = 0$? 

Price an USYes

(e) On the basis of your response to the previous question, ﬁt a smaller model that only uses the predictors for which there is evidence of association with the outcome. 

```{r}
#Carseats
Carseats_Model2 <- lm(Sales~Price+US, data = Carseats)
summary(Carseats_Model2)
```

(f) How well do the models in (a) and (e) ﬁt the data? 

Our F-Statistic increased from the model in (a) to the model in (e). However, USYes became insignificant as well in the second model. This shows price is clearly the best predictor for the Sales in this data. 

(g) Using the model from (e), obtain 95% conﬁdence intervals for the coeﬃcient(s). 

```{r}
confint(Carseats_Model2)
```

(h) Is there evidence of outliers or high leverage observations in the model from (e)?

```{r}
plot(Carseats_Model2)
```
368, 26 and 50 are all high leverage points in this model. There is not evidence of outliers as shown from the QQplot.

11. In this problem we will investigate the t-statistic for the null hypothesis $H_0 : β = 0$ in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.
```{r}
set.seed(1)
x=rnorm (100)
y=2*x+rnorm (100)
```
(a) Perform a simple linear regression of y onto x, without an intercept. Report the coeﬃcient estimate $\hat{\beta}$, the standard error of this coeﬃcient estimate, and the t-statistic and p-value associated with the null hypothesis $H_0$ : β = 0. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).) 

```{r}
lm(y~x +0)
summary(lm(y~x +0))
```
This predictor is significant, our F-Statistic is high and shows our model is significant. 

(b) Now perform a simple linear regression of x onto y without an intercept, and report the coeﬃcient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis $H_0 : β = 0$. Comment on these results. 

```{r}
lm(y~x)
summary(lm(y~x))
```
The F-Statistic and Standard Error both increased. The predicited $\hat{\beta}$ stayed the same. 

(c) What is the relationship between the results obtained in (a) and (b)? 

The F-Statistic and Standard Error both increased. The predicited $\hat{\beta}$ stayed the same. 
The second model simply includes another predicted point of the same sloped line as in model 1. 

(d) For the regression of Y onto X without an intercept, the tstatistic for $H_0 : β = 0$ takes the form$\hat{\beta}/SE(\hat{\beta})$, where $\hat{\beta}$ is given by (3.38), and where

$$ \operatorname{SE}(\hat{\beta})=\sqrt{\frac{\sum_{i=1}^{n}\left(y_{i}-x_{i} \hat{\beta}\right)^{2}}{(n-1) \sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^{2}}}$$

(These formulas are slightly diﬀerent from those given in Sections 3.1.1 and 3.1.2, since here we are performing regression without an intercept.) Show algebraically, and conﬁrm numerically in R, that the t-statistic can be written as:

$$ \frac{(\sqrt{n-1}) \sum_{i=1}^{n} x_{i} y_{i}}{\sqrt{\left(\sum_{i=1}^{n} x_{i}^{2}\right)\left(\sum_{i^{\prime}=1}^{n} y_{i^{\prime}}^{2}\right)-\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}} y_{i^{\prime}}\right)^{2}}} $$ 

$$  t=\frac{\sum_{i} x_{i} y_{y} / \sum_{j} x_{j}^{2}}{\sqrt{\sum_{i}\left(y_{i}-x_{i} \hat{\beta}\right)^{2} /(n-1) \sum_{j} x_{j}^{2}}}=\frac{\sqrt{n-1} \sum_{i} x_{i} y_{i}}{\sqrt{\sum_{j} x_{j}^{2} \sum_{i}\left(y_{i}-x_{i} \sum_{j} x_{j} y_{j} / \sum_{j} x_{j}^{2}\right)^{2}}}=\frac{\sqrt{n-1} \sum_{i} x_{i} y_{i}}{\sqrt{\left(\sum_{j} x_{j}^{2}\right)\left(\sum_{j} y_{j}^{2}\right)-\left(\sum_{j} x_{j} y_{j}\right)}} $$ 
The above derived formula confirms that our t-statistic can be written in the desired form. 

```{r}
t_new <- sqrt(n - 1)*(x %*% y)/sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)
t_new
```

(e) Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y. 

As we can see this is the same t-statistic as the model which excludes our intercept. 

(f) In R, show that when regression is performed with an intercept, the t-statistic for $H_0 : β_1 = 0$ is the same for the regression of y onto x as it is for the regression of x onto y.

```{r}
print(summary(lm(y~x)))
print(summary(lm(x~y)))
```

14. This problem focuses on the collinearity problem.
(a) Perform the following commands in R:
```{r}
set.seed(1)
x1=runif (100)
x2=0.5*x1+rnorm (100)/10
y=2+2*x1+0.3*x2+rnorm (100)
```

The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coeﬃcients? 

(b) What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables. 

```{r}
cor(x1,x2)
plot(x1,x2)
```

(c) Using this data, ﬁt a least squares regression to predict y using x1 and x2. Describe the results obtained. What are $\hat{\beta_0},\hat{\beta_1},\hat{\beta_2} $? How do these relate to the true $β_0, β_1,$ and $β_2$? Can you reject the null hypothesis $H_0 : β_1 = 0$? How about the null hypothesis H0 : $β_2 = 0$?

```{r}
print(lm(y~x1+x2))
summary(lm(y~x1+x2))
```

$\hat{\beta_1} = 1.43 , \hat{\beta_2} = 1 $

We can reject the null hypothesis : $H_0 : β_1 = 0$ but NOT the null hypothesis : $H_0 : β_2 = 0$


(d) Now ﬁt a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis $H_0 : β_1 = 0$? 

```{r}
summary(lm(y~x1))
```

After running this x1 has now become NOT significant and we fail to reject the null hypothesis : $H_0 : β_1 = 0$? 

(e) Now ﬁt a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis $H_0 : β_1 = 0$

```{r}
summary(lm(y~x2))
```

This x2 has now becomes significant and we CAN reject $H_0 : β_2 = 0$ 

(f) Do the results obtained in (c)–(e) contradict each other? Explain your answer. 

The results we got do not contradict eachother because the colinearity of the variables were not allowing both to be deemed significant when run with a standard multi-linear regression. 

(g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.

```{r}
x1=c(x1, 0.1)
x2=c(x2, 0.8)
y=c(y,6)
```

Re-ﬁt the linear models from (c) to (e) using this new data. What eﬀect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.

```{r}
summary(lm(y~x1+x2))
```
This outlier did not significantly change our output. Thus, neither of these seem like high leverage points. 
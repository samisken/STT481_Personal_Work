---
title: "STT481_Hw1"
author: "Sam Isken"
date: "September 19, 2019"
output: html_document
---
The chunk below imports packages needed for all chunks in the RMD. 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
```

# Section 2.4: Question #7

The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

```{r}
#Let's first replicate the df in question #7 from section 2.4 
X1 <- c(0,2,0,0,-1,1)
X2 <- c(3,0,1,1,0,1)
X3 <- c(0,0,3,2,1,1)
Y <- c("Red","Red","Red","Green","Green","Red")
training_data <- data.frame(X1,X2,X3,Y) 
training_data
```

The Euclidean distance of points p and q is the length of the line segement connecting them. 

$$ d(p,q) =  \sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + ... + (q_n-p_n)^2} = \sqrt{\Sigma(q_i - p_i)^2} $$

(a) Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$.

Given the Euclidean distance formula above we can compute the points as follows. 

Observation 1: $\sqrt{(0-0)^2 + (3-0)^2 + (0-0)^2} = 3$

Observation 2: $\sqrt{(2-0)^2 + (0-0)^2 + (0-0)^2} = 2$

Observation 3: $\sqrt{(0-0)^2 + (1-0)^2 + (3-0)^2} = 3.16$

Observation 4: $\sqrt{(0-0)^2 + (1-0)^2 + (2-0)^2} = 2.24$

Observation 5: $\sqrt{(-1-0)^2 + (0-0)^2 + (1-0)^2} = 1.41$

Observation 6: $\sqrt{(1-0)^2 + (1-0)^2 + (1-0)^2} = 1.73$


(b) What is our prediction with $K = 1$? Why?



(c) What is our prediction with $K = 3$? Why? 


(d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?



# Section 2.4: Question #8

(a) Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data

```{r}
#(a)
college <- read.csv("College.csv",header = TRUE)
college
```

(b) Look at the data using the fix() function. You should notice that the first column is just the name of each university. We don't really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:

```{r}
#(b)
#Look at data using fix()
fix(college)

#Try the following commands
rownames(college)=college[,1]
fix(college)
college

college=college[,-1]
fix(college)

#Display college, see row.names is set and that extra (prior) column is removed 
college
```

(c) 

i. Use the summary() function to produce a numerical summary of the variables in the data set. 

```{r}
#(c) i.
#Use summary function on data set 'college'
summary(college)
```

ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10]. 

```{r}
#(c) ii. 
#Use pairs function to create scatterplot matrix of first 10 columns of data set 'college'
pairs(college[1:10])
```

iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private.

```{r}
#(c) iii.
boxplot(college$Outstate, college$Private)
```


iv. Create a new qualitative variable, called Elite, bybinning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.

```{r}
#(c) iv.

#Creates a vector of "No"'s dependant on the number of rows in the table
Elite=rep("No",nrow(college))  

#Sets the values with a percent of over 50% to be a string "Yes"
Elite[college$Top10perc>50]="Yes"

#Coded "Elite" as a factory (aka category or enumerated type)
Elite=as.factor(Elite)

#Adds 'Elite' column to data set 'college'
college <- data.frame(college ,Elite)
college

#Run summary function on cllege and ensure Eliste is contained 
summary(college)

#Get Count of Elite Colleges
number_of_elite <- sum(college$Elite=="Yes")

#Get Percent of Colleges Elite
percent_of_elite <- number_of_elite / (length(college$Elite))

#Boxplot of Out of State vs. Elise 
boxplot(college$Outstate,college$Elite)
```
v. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.

```{r}
#(c) v.
#Histograms for College$P.Undergrad

#Divide window into  2x2 matrix
par(mfrow=c(2,2))

#Produce 4 histograms with differing numbers of bins (designated by 'break' parameter)
hist(College$P.Undergrad, breaks= 2 )
hist(College$P.Undergrad, breaks= 6 )
hist(College$P.Undergrad, breaks= 9 )
hist(College$P.Undergrad, breaks= 45 )
```

```{r}
#(c) v.
#Histograms for College$Room.Board

#Divide window into  2x2 matrix
par(mfrow=c(2,2))

#Produce 4 histograms with differing numbers of bins (designated by 'break' parameter)
hist(College$Room.Board, breaks= 2 )
hist(College$Room.Board, breaks= 6 )
hist(College$Room.Board, breaks= 9 )
hist(College$Room.Board, breaks= 45 )
```

```{r}
#(c) v.
#Histograms for College$Books

#Divide window into  2x2 matrix
par(mfrow=c(2,2))

#Produce 4 histograms with differing numbers of bins (designated by 'break' parameter)
hist(College$Books, breaks= 2 )
hist(College$Books, breaks= 6 )
hist(College$Books, breaks= 9 )
hist(College$Books, breaks= 45 )
```
vi. Continue exploring the data, and provide a brief summary of what you discover

```{r}
#(c) vi.
#This did not end up being useful as there are too many variables 
pairs(College)
College_Private = College[College$Private == 'Yes',]
College_Public = College[College$Private == 'No',]
#Create histograms to compare college cost 
par(mfrow=c(2,2))
hist(College_Private$Books)
hist(College_Public$Books)

hist(College_Private$Room.Board)
hist(College_Public$Room.Board)

```

Section 2.4: Question #9. 

This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.

```{r}
#9
#View data and remove columns with missing data points 
Auto

#Removes any rows with a missing data point 
Auto_rm <- na.omit(Auto)
Auto_rm
```

(a) Which of the predictors are quantitative, and which are qualitative?

```{r}
#View all variables using summary function  
summary(Auto_rm)

#Create a vector of all the quantitative variables 
quant_vars <- c("mpg","cylinders","displacement","horsepower","weight","acceleration")

#Create a vector of all the qualitative variables 
qual_vars <- c("name","year","origin",NA,NA,NA)

#Create and display columns containing quant_vars and qual_vars (quantitative variables and qualitative variables respectively)
Variable_Type <- data.frame(quant_vars,qual_vars)
Variable_Type
```

(b) What is the range of each quantitative predictor? You can answer this using the range() function.

```{r}
#Calculates range of all quantitative variables variables 
range(Auto_rm$mpg)
range(Auto_rm$cylinders)	
range(Auto_rm$displacement)		
range(Auto_rm$horsepower)		
range(Auto_rm$weight)		
range(Auto_rm$acceleration)		
```

(c) What is the mean and standard deviation of each quantitative predictor? 

```{r}
#(c)
#Calc mean and sd of mpg
mean(Auto_rm$mpg)
sd(Auto_rm$mpg)

#Calc mean and sd of cylinders
mean(Auto_rm$cylinders)
sd(Auto_rm$cylinders)

#Calc mean and sd of displacement
mean(Auto_rm$displacement)
sd(Auto_rm$displacement)

#Calc mean and sd of horsepower
mean(Auto_rm$horsepower)
sd(Auto_rm$horsepower)

#Calc mean and sd of weight
mean(Auto_rm$weight)
sd(Auto_rm$weight)

#Calc mean and sd of acceleration
mean(Auto_rm$acceleration)
sd(Auto_rm$acceleration)
```

(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains

```{r}
#(d)
#Create  data set of rows 10 - 85 of 'Auto_rm'
Auto_rm2 <- Auto_rm[c(10:85),]
#Display new data set 'Auto_rm2'
Auto_rm2


#Returns means of key quantitative columns 
colMeans(Auto_rm2[1:6])
```

(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. 

```{r}
pairs(Auto_rm)
plot(lm(mpg~ ., data = Auto_rm))
plot(lm(cylinders~ ., data = Auto_rm))
plot(lm(displacement~ ., data = Auto_rm))
plot(lm(horsepower~ ., data = Auto_rm))
plot(lm(weight~ ., data = Auto_rm))
plot(lm(acceleration~ ., data = Auto_rm))
plot(lm(year~ ., data = Auto_rm))
plot(lm(origin~ ., data = Auto_rm))
```

(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer

```{r}
lm(mpg~.,data=Auto_rm)
```

# Section 3.7 Exercises: Question #3 

3. Suppose we have a data set with five predictors, $X_1 =$ GPA,$X_2 =$ IQ, $X_3 =$ Gender (1 for Female and 0 for Male), $X_4 =$ Interaction between GPA and IQ, and $X_5 =$ Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to ï¬t the model, and get $\hat \beta_0 = 50$, $\hat \beta_1 = 20$, $\hat \beta_2 =0 .07$, $\hat \beta_3 = 35$ , $\hat \beta_4 =0 .01$, $\hat \beta_5= -10$.
 
(a) Which answer is correct, and why? 

i. For a fixed value of IQ and GPA, males earn more on average than females.
ii. For a fixed value of IQ and GPA, females earn more on average than males. 
iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough. 
iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough. 

(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0. 

(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.

# Section 3.7 Exercises: Question #4

I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then ï¬t a linear regression model to the data, as well as a separate cubic regression, i.e. $ Y = \beta_0 + \beta_1 X +\beta_2 X^2 + \beta_3 X^3 + \epsilon$.

(a) Suppose that the true relationship between X and Y is linear, i.e. $Y=\beta_0 + \beta_1 X +\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

(b) Answer (a) using test rather than training RSS. 

(c) Suppose that the true relationship between X and Y is not linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer. 

(d) Answer (c) using test rather than training RSS.



# Section 3.7 Exercises: Question #9

This question involves the use of multiple linear regression on the Auto data set.

(a) Produce a scatterplot matrix which includes all of the variables in the data set. 

```{r}
#(a)
#Summarizes 
summary(Auto_rm)
#Creates matrix of scatter plots containing all variables in data set
pairs(Auto_rm)
```

(b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.

```{r}
#(b)
#create matrix of correlations, excluding last name columns
pairs(cor(Auto_rm[1:8]))
```

(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance: 

```{r}
model.lm1 <- lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin, data = Auto_rm)
summary(model.lm1)
```

i. Is there a relationship between the predictors and the response? 

Displacement, weight, year and origin are all statistically significant predictors in this regression. 

ii. Which predictors appear to have a statistically significant relationship to the response? 

iii. What does the coefficient for the year variable suggest? 

(d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit? 

Do the residual plots suggest any unusually large outliers? 

Does the leverage plot identify any observations with unusually high leverage? 

```{r}
plot(model.lm1)
```

(e) Use the * and : symbols to fir linear regression models with interaction effects. 

Do any interactions appear to be statistically significant?
```{r}

```

(f) Try a few different transformations of the variables, such as $log(X), \sqrt{X}, X^2$. Comment on your findings.


10. This question should be answered using the Carseats data set.

(a) Fit a multiple regression model to predict Sales using Price, Urban, and US. 

```{r}
Carseats
Carseats_Model1 <- lm(Sales~Price+Urban+US, data = Carseats)
summary(Carseats_Model1)
```

(b) Provide an interpretation of each coeﬃcient in the model. Be careful—some of the variables in the model are qualitative! 

(c) Write out the model in equation form, being careful to handle the qualitative variables properly. 

(d) For which of the predictors can you reject the null hypothesis $H_0 : β_j = 0$? 

(e) On the basis of your response to the previous question, ﬁt a smaller model that only uses the predictors for which there is evidence of association with the outcome. 

(f) How well do the models in (a) and (e) ﬁt the data? 

(g) Using the model from (e), obtain 95% conﬁdence intervals for the coeﬃcient(s). 

(h) Is there evidence of outliers or high leverage observations in the model from (e)?


11. In this problem we will investigate the t-statistic for the null hypothesis $H_0 : β = 0$ in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.
```{r}
set.seed(1)
x=rnorm (100)
y=2*x+rnorm (100)
```
(a) Perform a simple linear regression of y onto x, without an intercept. Report the coeﬃcient estimate $\hat{\beta}$, the standard error of this coeﬃcient estimate, and the t-statistic and p-value associated with the null hypothesis $H_0$ : β = 0. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).) 

(b) Now perform a simple linear regression of x onto y without an intercept, and report the coeﬃcient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis $H_0 : β = 0$. Comment on these results. 

(c) What is the relationship between the results obtained in (a) and (b)? 

(d) For the regression of Y onto X without an intercept, the tstatistic for $H_0 : β = 0$ takes the form$\hat{\beta}/SE(\hat{\beta})$, where $\hat{\beta}$ is given by (3.38), and where

$$ \operatorname{SE}(\hat{\beta})=\sqrt{\frac{\sum_{i=1}^{n}\left(y_{i}-x_{i} \hat{\beta}\right)^{2}}{(n-1) \sum_{i^{\prime}=1}^{n} x_{i^{\prime}}^{2}}}$$

(These formulas are slightly diﬀerent from those given in Sections 3.1.1 and 3.1.2, since here we are performing regression without an intercept.) Show algebraically, and conﬁrm numerically in R, that the t-statistic can be written as:

$$ \frac{(\sqrt{n-1}) \sum_{i=1}^{n} x_{i} y_{i}}{\sqrt{\left(\sum_{i=1}^{n} x_{i}^{2}\right)\left(\sum_{i^{\prime}=1}^{n} y_{i^{\prime}}^{2}\right)-\left(\sum_{i^{\prime}=1}^{n} x_{i^{\prime}} y_{i^{\prime}}\right)^{2}}} $$ 


(e) Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y. 

(f) In R, show that when regression is performed with an intercept, the t-statistic for $H_0 : β_1 = 0$ is the same for the regression of y onto x as it is for the regression of x onto y.



14. This problem focuses on the collinearity problem.
(a) Perform the following commands in R:
```{r}
set.seed(1)
x1=runif (100)
x2=0.5*x1+rnorm (100)/10
y=2+2*x1+0.3*x2+rnorm (100)
```

The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coeﬃcients? 

(b) What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables. 

(c) Using this data, ﬁt a least squares regression to predict y using x1 and x2. Describe the results obtained. What are $\hat{\beta_0},\hat{\beta_1},\hat{\beta_2} $? How do these relate to the true $β_0, β_1,$ and $β_2$? Can you reject the null hypothesis $H_0 : β_1 = 0$? How about the null hypothesis H0 : $β_2 = 0$?

(d) Now ﬁt a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis $H_0 : β_1 = 0$? 

(e) Now ﬁt a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis $H_0 : β_1 = 0$? 

(f) Do the results obtained in (c)–(e) contradict each other? Explain your answer. 

(g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.

```{r}
x1=c(x1, 0.1)
x2=c(x2, 0.8)
y=c(y,6)
```

Re-ﬁt the linear models from (c) to (e) using this new data. What eﬀect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.

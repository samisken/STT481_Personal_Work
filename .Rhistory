nrow(iris)
nrow(iris3)
cl
K_num <- length(K_list)
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
train <- rbind(Default[,,2], Default[,,3], Default[,,4])
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",n_default), rep("Yes",n_default)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 10 values of k
for (k in K_list){ # we try 7 different k's
print(k)
#pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
#cv.error[k] <- mean(pred.class != cl)
}
#plot(1:100, cv.error, type = "b")
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
train <- rbind(Default[,,2], Default[,,3], Default[,,4])
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",n_default), rep("Yes",n_default)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 10 values of k
for (k in K_list){ # we try 7 different k's
print(k)
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
#cv.error[k] <- mean(pred.class != cl)
}
train
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
train <- rbind(Default[,,2], Default[,,3], Default[,,4])
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",n_default), rep("Yes",n_default)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(k)
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
#cv.error[k] <- mean(pred.class != cl)
}
dim(train)
length(cl)
train_row_count <- nrow(train)
train_row_count
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",train_row_count), rep("Yes",train_row_count))
cl
cl
length(cl)
#Pull 2, 3, 4 columns
train <- rbind(Default[,,2], Default[,,3] )#,Default[,,4])
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
train <- rbind(Default[,,2], Default[,,3] )#,Default[,,4])
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",train_row_count), rep("Yes",train_row_count))
K_num <- length(K_list)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",train_row_count), rep("Yes",train_row_count))
K_num <- length(K_list)
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
train <- rbind(Default[,,2], Default[,,3]) #,Default[,,4])
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",train_row_count),
rep("Yes",train_row_count)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(k)
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
#cv.error[k] <- mean(pred.class != cl)
}
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",train_row_count), rep("Yes",train_row_count)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(k)
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
#cv.error[k] <- mean(pred.class != cl)
}
train_row_count
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",15000), rep("Yes",15000)))
length(cl)
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("No",15000), rep("Yes",15000)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(k)
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
#cv.error[k] <- mean(pred.class != cl)
}
#View previously loaded and scaled data
head(Default)
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1))
#Pre processing to get all numeric values
library(dplyr)
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1))
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1))
Default
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1)),
Default
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1))
Default
data("Default")
data("Default")
X <- Default[, c("student", "balance", "income")]
X[,"student"] <- ifelse(X[,"student"] == "Yes", 1, 0)
X <- scale(X)
y <- Default[, "default"]
#Set K_list as a list of integers
K_list = c(1, 5, 10, 15, 20, 25, 30)
#View previously loaded and scaled data
head(Default)
#Pre processing to get all numeric values
library(dplyr)
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1))
%>% mutate(student = ifelse(student == "No",0,1))
#View previously loaded and scaled data
head(Default)
#Pre processing to get all numeric values
library(dplyr)
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1))
mutate(student = ifelse(student == "No",0,1))
#View previously loaded and scaled data
head(Default)
#Pre processing to get all numeric values
library(dplyr)
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1))
mutate(student
= ifelse(student
== "No",0,1))
X
y
head(X)
#Pull 2, 3, 4 columns
#train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train <- X
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",train_row_count), rep("No",train_row_count)))
length(cl)
dim(cl)
dim(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",5000), rep("No",5000)))
length(cl)
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
#train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train <- X
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",5000), rep("No",5000)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(k)
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
#cv.error[k] <- mean(pred.class != cl)
}
#plot(1:100, cv.error, type = "b")
plot(1:100, cv.error, type = "b")
plot(1:7, cv.error, type = "b")
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
#train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train <- X
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",5000), rep("No",5000)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(k)
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
cv.error[k] <- mean(pred.class != cl)
}
plot(1:7, cv.error, type = "b")
cv.error
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
#train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train <- X
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",5000), rep("No",5000)))
K_num <- length(K_list)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
cv.error <- rep(0,K_num)
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
cv.error[k] <- mean(pred.class != cl)
}
plot(1:7, cv.error, type = "b")
cv.error
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
#train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train <- X
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",5000), rep("No",5000)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(paste("Let k=",k))
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
for(i in 1:7){
cv.error[i] <- mean(pred.class != cl)
}
}
plot(1:7, cv.error, type = "b")
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
#train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train <- X
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",5000), rep("No",5000)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(paste("Let k=",k))
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
for(i in 1:7){
cv.error[i] <- mean(pred.class != cl)
print(cv.error[i])
}
}
plot(1:7, cv.error, type = "b")
cv.error
k_count=0
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
#train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train <- X
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",5000), rep("No",5000)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
k_count=0
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(paste("Let k=",k))
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
k_count = k_count+1
cv.error[k_count] <- mean(pred.class != cl)
print(cv.error[k_count])
}
plot(1:7, cv.error, type = "b")
cv.error
mean(cv.error)
LOOCV_est_test_error <- mean(cv.error)
print(paste(("The LOOCV Estimated Test Errir is:",LOOCV_est_test_error)))
LOOCV_est_test_error <- mean(cv.error)
print(paste(("The LOOCV Estimated Test Error is:", LOOCV_est_test_error)))
LOOCV_est_test_error <- mean(cv.error)
print(paste("The LOOCV Estimated Test Error is:", LOOCV_est_test_error))
LOOCV_est_test_error <- mean(cv.error)
print(paste("The LOOCV Estimated Test Error is:", LOOCV_est_test_error))
print(paste("K equal to the following produces the smallest test error", which.min(cv.error)))
for(k in c(1,5,10,15,20,25,30)){
cvknn <- knn.cv(X, y, k = k) ## the little k here is the number of nearest neighbors not k-fold
print(mean(cvknn != y))
}
# Initialize empty vector for cv erro (length=10, values=0)
cv.error <- rep(0,10)
cv.error_list <- rep(0,10)
# Pre process df and add factor
levels(Weekly$Direction) <- c("Up","Down")
levels(Weekly$Direction)
Weekly$Direction <- factor(as.numeric(Weekly$Direction))
levels(Weekly$Direction) <- c(1,0)
levels(Weekly$Direction)
Weekly
# Write loop
for (i in 1:10){
#Create new data set removing observation fold.index == i
Weekly_train <- Weekly[-c(fold.index==i)]
levels(Weekly_train$Direction) <- c(1,0)
# Create counts for levels
up_count <- sum(Weekly_train$Direction==1)
down_count <- sum(Weekly_train$Direction==0)
cl <- factor(c(rep(1,up_count), rep(0,down_count)))
pred.class <- knn.cv(Weekly_train, cl, k = 10,prob = FALSE)
cv.error[i] <- mean(pred.class!=cl)
cv.error_list[i] <- pred.class
}
plot(1:10, cv.error, type = "b")
which.min(cv.error)
cv.error_list
# Initialize empty vector for cv erro (length=10, values=0)
cv.error <- rep(0,10)
cv.error_list <- rep(0,10)
# Pre process df and add factor
levels(Weekly$Direction) <- c("Up","Down")
levels(Weekly$Direction)
Weekly$Direction <- factor(as.numeric(Weekly$Direction))
levels(Weekly$Direction) <- c(1,0)
levels(Weekly$Direction)
Weekly
# Write loop
for (i in 1:10){
#Create new data set removing observation fold.index == i
Weekly_train <- Weekly[-c(fold.index==i)]
levels(Weekly_train$Direction) <- c(1,0)
# Create counts for levels
up_count <- sum(Weekly_train$Direction==1)
down_count <- sum(Weekly_train$Direction==0)
cl <- factor(c(rep(1,up_count), rep(0,down_count)))
pred.class <- knn.cv(Weekly_train, cl, k = 10,prob = FALSE)
cv.error[i] <- mean(pred.class!=cl)
cv.error_list[i] <- pred.class
}
plot(1:10, cv.error, type = "b")
which.min(cv.error)
cv.error
# Initialize empty vector for cv erro (length=10, values=0)
cv.error <- rep(0,10)
cv.error_list <- rep(0,10)
# Pre process df and add factor
levels(Weekly$Direction) <- c("Up","Down")
levels(Weekly$Direction)
Weekly$Direction <- factor(as.numeric(Weekly$Direction))
levels(Weekly$Direction) <- c(1,0)
levels(Weekly$Direction)
Weekly
# Write loop
for (i in 1:10){
#Create new data set removing observation fold.index == i
Weekly_train <- Weekly[-c(fold.index==i)]
levels(Weekly_train$Direction) <- c(1,0)
# Create counts for levels
up_count <- sum(Weekly_train$Direction==1)
down_count <- sum(Weekly_train$Direction==0)
cl <- factor(c(rep(1,up_count), rep(0,down_count)))
pred.class <- knn.cv(Weekly_train, cl, k = 10,prob = FALSE)
cv.error[i] <- mean(pred.class!=cl)
cv.error_list[i] <- pred.class
}
plot(1:10, cv.error, type = "b")
which.min(cv.error)
cv.error
zipcode_test <- read.csv("zipcode_test.csv", header = TRUE)
zipcode_train <- read.csv("zipcode_train.csv", header = TRUE)
head(zipcode_test)
head(zipcode_train)
set.seed(1)
cv.error <- rep(0,100)
for (k in 1:100){ # we try 100 different k's
pred.class <- knn.cv(zipcode_train, zipcode_test, k = k, K = 10) # this k is for KNN not k-fold CV
cv.error[k] <- mean(pred.class != zipcode_test)
}
set.seed(1)
y=rnorm(100)
x=rnorm(100)
y=x-2*x^2+rnorm (100)
head(y)
head(x)
dim(y)
nrow(y)
length(y)
y
y
scatter.smooth(x,y)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(boot)
library(ISLR)
library(class)
library(glmnet)
library(MLmetrics)
#library(swirl)
data("Weekly")
head(Weekly)
n <- nrow(Weekly) # Number observations in data set (1089 rows)
n_vec <- seq(1,n, by=1) # Fixed vector containing 1 - 1089
model_calc <- function(Weekly){
#Initialize empty integer (0), everytime prediction is correct add 1 to this
#Empty integer (0) for model i
i_list <- 0
#Empty integer (0) for model i
ii_list <- 0
#for loop from 1 -> n where n is the number of observations in the data set (1089 rows)
for(i in n_vec){
Weekly_working_df <- Weekly[-i,] # Create subset of data removing ith row
#Using new df with i row removed create model i and ii
logit_model_i_weekly_sub <- glm(Direction~Lag1+Lag2, data = Weekly_working_df, family = "binomial")
logit_model_ii_weekly_sub <- glm(Direction~Lag1+Lag2+I(Lag1^2)+I(Lag2^2), data = Weekly_working_df, family = "binomial")
#Posterior Probability Calculation
posterior_prob_i <-  predict.glm(logit_model_i_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
posterior_prob_ii <- predict.glm(logit_model_ii_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
#Get  posterior probability as an integer to use in boolean logic
numeric_i <- as.numeric(posterior_prob_i[1])
numeric_ii <- as.numeric(posterior_prob_ii[1])
#Set prediction to up or down dependant on posterior probability
if(numeric_i>.5){
prediction_i <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(numeric_i<=.5){
prediction_i <- "Down"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)>.5){
prediction_ii <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)<=.5){
prediction_ii <- "Down"
}
#For both models (i & ii) compare to true value and keep a count
true_value_i = Weekly[i,]$Direction
if(prediction_i == true_value_i){
i_list=i_list+1
}
if(prediction_ii == true_value_i){
ii_list=ii_list+1
}
}
print(paste("Count of true for model i is:",i_list))
print(paste("Count of true for model ii is:",ii_list))
#return(i_list)
#return(ii_list)
}
model_calc(Weekly)
i_correct <- 599 #Set given output above, count of times model i was correct
ii_correct <- 590 #Set given output above, count of times model ii was correct
#Compute test errors 1 - (Number of times model correct / row count (n) )
test_error_i <- 1-(i_correct/n)
test_error_ii <- 1-(ii_correct/n)
#Display with easily interpretable notes:
print(paste("The test error for model i is:",test_error_i))
print(paste("The test error for model ii is:",test_error_ii))
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
rm(list=ls())
rm(list=ls())

c()
qplot(displ, hwy, data = mpg, color=drv, geom = c("point", "smooth"))
qplot(y= hwy, data = mpg, color=drv, geom = c("point", "smooth"))
qplot(y= hwy, data = mpg, color=drv)
myhigh
skip()
skip()
skip()
skip()
concat(x)
cacheGenericsMetaData(x)`
(4)
e
sss
for(x in 3){print(x)}
qplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)
skip()
skip()
summary(g)
g+geom_point()
g+geom_point()+geom_smooth()
g+geom_point()+geom_smooth(lm)
g+geom_point()+geom_smooth(string=lm)
g+geom_point()+geom_smooth(method="lm")
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
geom_line)_+ylim(-3,3)
geom_line())_+ylim(-3,3)
geom_line()_+ylim(-3,3)
geom_line()+ylim(-3,3)
g+geom_line()+ylim(-3,3)
g+geom_line()+ylim(c(-3,3)
)
g + geom_line() + coord_cartesian(ylim=c(-3,3))
mpg
g <- ggplot(mpg,aes(x=displ,y=hwy,color=factor(year)))
g +geom_point()
g +geom_point()+facet_grid(formula=drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(formula=drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method=lm,se=FALSE,color=black)
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method=lm,se=FALSE,color="black")
g +geom_point()+facet_grid(drv~cyl,margins=TRUE)+geom_smooth(size=2,method=lm,se=FALSE,color="black")
g + geom_point() + facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method="lm",size=2,se=FALSE,color="black"
)
g + geom_point() + facet_grid(drv~cyl,margins=TRUE)+geom_smooth(method="lm",size=2,se=FALSE,color="black")+labs(x="Displacement",y="Highway Mileage",title="Swirl Rules!")
str(diamonds)
2
22
2
qplot(price,data=diamonds)
range(diamonds$price)
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
skip()
(carat,price,data=diamonds and color=cut)
plot(carat,price,data=diamonds and color=cut)
plot(carat,price,data=diamonds,color=cut)
plot(data=carat,price,data=diamonds,color=cut)
plot(data=diamonds,color=cut)
skip()
skip()
2
2
skip()
summary(g)
g+geom_point(alpha=1/3)
summary(g)
cutpoints <- quantile(diamonds$carat,seq(0,1,length=4),na.rm=TRUE)
cutpoints
skip()
skip()
skip()
diamonds[myd,]
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smoth()
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smooth()
(g+geom_point(alpha=1/3)+facet_grid(cut~car2))+geom_smoth()
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)
2
2
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(boot)
library(ISLR)
library(class)
library(glmnet)
library(MLmetrics)
#library(swirl)
glm.fit <- glm(Y ~ ., data = zipcode_train)
zipcode_test <- read.csv("zipcode_test.csv", header = TRUE)
zipcode_train <- read.csv("zipcode_train.csv", header = TRUE)
head(zipcode_test)
head(zipcode_train)
glm.fit <- glm(Y ~ ., data = zipcode_train)
cv.err <- cv.glm(zipcode_train, glm.fit,K=10) # the default of K is n => LOOCV
cv.err$delta
set.seed(1)
cv.error <- rep(0,100)
for (k in 1:100){ # we try 100 different k's
pred.class <- knn.cv(zipcode_train, zipcode_test, k = k, K = 10) # this k is for KNN not k-fold CV
cv.error[k] <- mean(pred.class != zipcode_test)
}
cv.error <- rep(0,30)
i=1
if (i < 31){
glm.fit <- glm(Y ~ .,data = zipcode_train)
cv.error[i] <- cv.glm(zipcode_train, glm.fit, K = i)$delta[1]
i=i+1
}
rm(list=ls())
getwd()
cd("C:/Users/sam/Desktop/STT481_Personal_Work")
setwd("C:/Users/sam/Desktop/STT481_Personal_Work")
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(boot)
library(ISLR)
library(class)
library(glmnet)
library(MLmetrics)
#library(swirl)
data("Weekly")
head(Weekly)
n <- nrow(Weekly) # Number observations in data set (1089 rows)
n_vec <- seq(1,n, by=1) # Fixed vector containing 1 - 1089
model_calc <- function(Weekly){
#Initialize empty integer (0), everytime prediction is correct add 1 to this
#Empty integer (0) for model i
i_list <- 0
#Empty integer (0) for model i
ii_list <- 0
#for loop from 1 -> n where n is the number of observations in the data set (1089 rows)
for(i in n_vec){
Weekly_working_df <- Weekly[-i,] # Create subset of data removing ith row
#Using new df with i row removed create model i and ii
logit_model_i_weekly_sub <- glm(Direction~Lag1+Lag2, data = Weekly_working_df, family = "binomial")
logit_model_ii_weekly_sub <- glm(Direction~Lag1+Lag2+I(Lag1^2)+I(Lag2^2), data = Weekly_working_df, family = "binomial")
#Posterior Probability Calculation
posterior_prob_i <-  predict.glm(logit_model_i_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
posterior_prob_ii <- predict.glm(logit_model_ii_weekly_sub,Weekly,type="response")[i] # Calculate posterior probability of market movement on ith observation
#Get  posterior probability as an integer to use in boolean logic
numeric_i <- as.numeric(posterior_prob_i[1])
numeric_ii <- as.numeric(posterior_prob_ii[1])
#Set prediction to up or down dependant on posterior probability
if(numeric_i>.5){
prediction_i <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(numeric_i<=.5){
prediction_i <- "Down"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)>.5){
prediction_ii <- "Up"
}
#Set prediction to up or down dependant on posterior probability
if(as.numeric(numeric_ii)<=.5){
prediction_ii <- "Down"
}
#For both models (i & ii) compare to true value and keep a count
true_value_i = Weekly[i,]$Direction
if(prediction_i == true_value_i){
i_list=i_list+1
}
if(prediction_ii == true_value_i){
ii_list=ii_list+1
}
}
print(paste("Count of true for model i is:",i_list))
print(paste("Count of true for model ii is:",ii_list))
#return(i_list)
#return(ii_list)
}
model_calc(Weekly)
i_correct <- 599 #Set given output above, count of times model i was correct
ii_correct <- 590 #Set given output above, count of times model ii was correct
#Compute test errors 1 - (Number of times model correct / row count (n) )
test_error_i <- 1-(i_correct/n)
test_error_ii <- 1-(ii_correct/n)
#Display with easily interpretable notes:
print(paste("The test error for model i is:",test_error_i))
print(paste("The test error for model ii is:",test_error_ii))
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = nrow(Weekly))$delta[1]
set.seed(1) ## the seed can be arbitrary but we use 1 for the sake of consistency
fold.index <- cut(sample(1:nrow(Weekly)), breaks=10, labels=FALSE)
prediction_ii <- rep(0,length(numeric_ii))
# Initialize empty vector for cv error (length=10, values=0), this is set to 10 because we are looping for i in 1:10
cv.error <- rep(0,10)
cv.error_list <- rep(0,10)
# Pre process df and add factor
levels(Weekly$Direction) <- c("Up","Down")
levels(Weekly$Direction)
Weekly$Direction <- factor(as.numeric(Weekly$Direction))
# Change factor levels to be numeric
levels(Weekly$Direction) <- c(1,0)
levels(Weekly$Direction)
# Write loop for i in 1:10
for (i in 1:10){
i_list=0
ii_list=0
#Create new data set removing observation fold.index == i
Weekly_train <- Weekly[-c(fold.index==i)]
levels(Weekly_train$Direction) <- c(1,0)
# Create counts for levels
up_count <- sum(Weekly_train$Direction==1)
down_count <- sum(Weekly_train$Direction==0)
cl <- factor(c(rep(1,up_count), rep(0,down_count)))
logit_model_i_weekly_sub <- glm(Direction~Lag1+Lag2, data = Weekly_train, family = "binomial")
logit_model_ii_weekly_sub <- glm(Direction~Lag1+Lag2+I(Lag1^2)+I(Lag2^2), data = Weekly_train, family = "binomial")
posterior_prob_i <-  predict.glm(logit_model_i_weekly_sub,Weekly,type="response")[fold.index==i] # Calculate posterior probability of market movement on ith observation
posterior_prob_ii <- predict.glm(logit_model_ii_weekly_sub,Weekly,type="response")[fold.index==i] # Calculate posterior probability of market movement on ith observation
# Display posterior probabilities as numeric values
numeric_i <- as.numeric(posterior_prob_i)
numeric_ii <- as.numeric(posterior_prob_ii)
prediction_i <- rep(0,length(numeric_i))
prediction_ii <- rep(0,length(numeric_ii))
i_count=0
for(value in numeric_i){
i_count = i_count+1
if(value>.5){
prediction_i[i_count] <- 1
}
if(value<=.5){
prediction_i[i_count] <- 0
}
}
ii_count=0
for(value in numeric_ii){
ii_count = ii_count+1
if(value>.5){
prediction_ii[ii_count] <- 1
}
if(value<=.5){
prediction_ii[ii_count] <- 0
}
}
true_value_i = Weekly_train[fold.index==i,]$Direction
i_count=0
for(value in prediction_i)
i_count = i_count +1
if(value == true_value_i[i_count]){
i_list=i_list+1
}
ii_count=0
for(value in prediction_i)
ii_count = ii_count +1
if(value == true_value_i[ii_count]){
ii_list=ii_list+1
}
print(paste("1 = Correct, 0 = Wrong, Model i:", i_list))
print(paste("1 = Correct, 0 = Wrong, Model ii:", ii_list))
#return(i_list)
#return(ii_list)
}
# Since the response is a binary variable an
# appropriate cost function for glm.cv is
cost <- function(r, pi = 0) mean(abs(r - pi) > 0.5)
glm.fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)
cv.error.1 <- cv.glm(Weekly, glm.fit, cost, K = 10)$delta[1]
glm.fit <- glm(Direction ~ Lag1 + Lag2 + I(Lag1^2) + I(Lag2^2), data = Weekly, family = binomial)
cv.error.2 <- cv.glm(Weekly, glm.fit, cost, K = 10)$delta[1]
rm(list=ls())
data("Default")
X <- Default[, c("student", "balance", "income")]
X[,"student"] <- ifelse(X[,"student"] == "Yes", 1, 0)
X <- scale(X)
y <- Default[, "default"]
head(X)
#Set K_list as a list of integers
K_list = c(1, 5, 10, 15, 20, 25, 30)
#View previously loaded and scaled data
head(Default)
#Pre processing to get all numeric values
library(dplyr)
Default <- Default %>%
mutate(default = ifelse(default == "No",0,1))
Default
#Calculate number of rows in initial data set
n_default <- nrow(Default)
set.seed(1)
#Pull 2, 3, 4 columns
#train <- rbind(Default[,,2], Default[,,3],Default[,,4])
train <- X
train_row_count <- nrow(train)
#Create levels Default "Yes" or "No"
cl <- factor(c(rep("Yes",5000), rep("No",5000)))
K_num <- length(K_list)
cv.error <- rep(0,K_num)
k_count=0
#Write loop to try 7 values of k
for (k in K_list){ # we try 7 different k's
print(paste("Let k=",k))
pred.class <- knn.cv(train, cl, k = k) # this k is for KNN not k-fold CV
k_count = k_count+1
cv.error[k_count] <- mean(pred.class != cl)
print(cv.error[k_count])
}
plot(1:7, cv.error, type = "b")
cv.error
LOOCV_est_test_error <- mean(cv.error)
print(paste("The LOOCV Estimated Test Error is:", LOOCV_est_test_error))
print(paste("K equal to the following produces the smallest test error", which.min(cv.error)))
for(k in c(1,5,10,15,20,25,30)){
cvknn <- knn.cv(X, y, k = k) ## the little k here is the number of nearest neighbors not k-fold
print(mean(cvknn != y))
}
set.seed(10) ## the seed can be arbitrary but we use 10 for the sake of consistency
fold.index <- cut(sample(1:nrow(Default)), breaks=10, labels=FALSE)
# Initialize empty vector for cv erro (length=10, values=0)
cv.error <- rep(0,10)
cv.error_list <- rep(0,10)
# Pre process df and add factor
levels(Weekly$Direction) <- c("Up","Down")
levels(Weekly$Direction)
Weekly$Direction <- factor(as.numeric(Weekly$Direction))
levels(Weekly$Direction) <- c(1,0)
levels(Weekly$Direction)
Weekly
# Write loop
for (i in 1:10){
#Create new data set removing observation fold.index == i
Weekly_train <- Weekly[-c(fold.index==i)]
levels(Weekly_train$Direction) <- c(1,0)
# Create counts for levels
up_count <- sum(Weekly_train$Direction==1)
down_count <- sum(Weekly_train$Direction==0)
cl <- factor(c(rep(1,up_count), rep(0,down_count)))
pred.class <- knn.cv(Weekly_train, cl, k = 10,prob = FALSE)
cv.error[i] <- mean(pred.class!=cl)
cv.error_list[i] <- pred.class
}
plot(1:10, cv.error, type = "b")
which.min(cv.error)
cv.error
print(paste("K equal to the following produces the smallest test error", which.min(cv.error)))
zipcode_test <- read.csv("zipcode_test.csv", header = TRUE)
zipcode_train <- read.csv("zipcode_train.csv", header = TRUE)
head(zipcode_test)
head(zipcode_train)
glm.fit <- glm(Y ~ ., data = zipcode_train)
cv.err <- cv.glm(zipcode_train, glm.fit,K=10) # the default of K is n => LOOCV
cv.err$delta
glm.fit <- glm(Y ~ ., data = zipcode_train)
cv.err <- cv.glm(zipcode_train, glm.fit,K=30) # the default of K is n => LOOCV
cv.err$delta
K_list2 <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30)
glm.fit <- glm(Y ~ ., data = zipcode_train)
cv.err <- cv.glm(zipcode_train, glm.fit,K=K_list2) # the default of K is n => LOOCV
cv.err <- cv.glm(zipcode_train, glm.fit,method = "knn",K=10) # the default of K is n => LOOCV
cv.err <- cv.glm(zipcode_train, glm.fit,K=10) # the default of K is n => LOOCV
cv.err$delta
which.min(cv.err)
set.seed(1)
cv.error <- rep(0,100)
for (k in 1:100){ # we try 100 different k's
pred.class <- knn.cv(zipcode_train, zipcode_test, k = k, K = 10) # this k is for KNN not k-fold CV
cv.error[k] <- mean(pred.class != zipcode_test)
}
cv.error <- rep(0,30)
for (i in 1:30){
glm.fit <- glm(Y ~ ., data = zipcode_train)
cv.error[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
glm.fit <- glm(zipcode_train$Y ~ ., data = zipcode_train)
cv.error <- rep(0,30)
for (i in 1:30){
glm.fit <- glm(zipcode_train$Y ~ ., data = zipcode_train)
cv.error[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error <- rep(0,30)
for (i in 1:30){
glm.fit <- glm(Y ~ ., data = zipcode_train)
cv.error[i] <- cv.glm(zipcode_train, glm.fit, K = 10)$delta[1]
}
cv.error
suppressWarnings()
set.seed(1)
y=rnorm(100)
x=rnorm(100)
y=x-2*x^2+rnorm (100)
n=length(y)
scatter.smooth(x,y)
set.seed(1)
Data <- data.frame(x, y)
fit_glmi <- glm(y ~ poly(x,2))
#Set up
set.seed(1)
Data <- data.frame(x, y)
#Model i
fit_glmi <- glm(y ~ x)
cv.glm(Data, fit_glmi)$delta[1]
#Model ii
fit_glmii <- glm(y ~ poly(x,2))
cv.glm(Data, fit_glmii)$delta[1]
#Model iii
fit_glmiii <- glm(y ~ poly(x,3))
cv.glm(Data, fit_glmiii)$delta[1]
fit_glmiiii <- glm(y ~ poly(x,4))
cv.glm(Data, fit_glmiiii)$delta[1]
#Set up
set.seed(100)
Data <- data.frame(x, y)
#Model i
fit_glmi <- glm(y ~ x)
cv.glm(Data, fit_glmi)$delta[1]
#Model ii
fit_glmii <- glm(y ~ poly(x,2))
cv.glm(Data, fit_glmii)$delta[1]
#Model iii
fit_glmiii <- glm(y ~ poly(x,3))
cv.glm(Data, fit_glmiii)$delta[1]
fit_glmiiii <- glm(y ~ poly(x,4))
cv.glm(Data, fit_glmiiii)$delta[1]
summary(fit_glmiii)
fit_glmiii
Y <- 100 + 4.5*X + 3.2*X^2 - 7.5*X^3 + epsilon
set.seed(1)
epsilon <- rnorm(100)
Y <- 100 + 4.5*X + 3.2*X^2 - 7.5*X^3 + epsilon
data <- data.frame(X,Y)
head(data)
Y
regfit.full <- regsubsets(balance ~ . , data = data)
library(leaps)
regfit.full <- regsubsets(balance ~ . , data = data)
summary(regfit.full)
regfit.full <- regsubsets(balance ~ . , data = data,nvmax=10)
summary(regfit.full)
library(leaps)
regfit.full <- regsubsets(balance ~ . , data = data,nvmax=10)
summary(regfit.full)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
library(leaps)
regfit.full <- regsubsets(balance ~ . , data = data,nvmax=10)
summary(regfit.full)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ", ylab="RSS", type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq", type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
plot.new()
points(11,reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables ", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
plot.new()
points(11,reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables ", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10], col = "red", cex = 2, pch = 20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables ", ylab = "BIC", type = "l")
points(6,reg.summary$bic[6], col = "red", cex = 2, pch = 20)
regfit.fwd <- regsubsets(balance ~., data = data, nvmax = 10, method = "forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(balance ~., data = data, nvmax = 10, method = "backward")
summary(regfit.bwd)
lasso.mod <- glmnet(data, alpha = 1, lambda = grid)
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = grid)
lasso.mod <- glmnet(X, Y, alpha = 1, lambda = grid)
lasso.mod <- glmnet(X, y, alpha = 1, lambda = grid)
lasso_glm <- train(Y ~ poly(X, 10), data = data,method = 'glmnet',trControl = trainControl(method = 'cv', number = 10),tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001, 0.2, by = 0.005)))
lasso_glm <- train(Y ~ poly(X, 9), data = data,method = 'glmnet',trControl = trainControl(method = 'cv', number = 10),tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001, 0.2, by = 0.005)))
lasso_glm <- train(Y ~ poly(X, 10), data = data,method = 'glmnet',trControl = trainControl(method = 'cv', number = 10),tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001, 0.2, by = 0.005)))
lasso_glm <- train(Y ~ X, data = data,method = 'glmnet',trControl = trainControl(method = 'cv', number = 10),tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001, 0.2, by = 0.005)))
plot(lasso_glm)
library(faraway)
install.packages("faraway")
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
data("gavote")
gavote <- data("gavote")
head(gavote)
gavote <- data(gavote)
head(gavote)
gavote <- data(gavotte)
head(gavote)
gavote <- data(gavote)
head(gavote)
gavote
help(gavote)
data(gavote)
summary(gavote)
gavote
data(gavote)
gavote

---
title: "STT481Hw2"
author: "Sam Isken"
date: "October 5, 2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(MASS)
```

FS19 STT481: Homework 2
(Due: Friday, October 18th, beginning of the class.)

1. (10 pts) Finish the swirl course Regression Models. Finish Sections 1-13. You can install and go to the
course by using the following command lines.

```{r} 
#library(swirl)
#install_course("Regression_Models")
#swirl()
```

I have included the pictures of my Swirl completion in the attached folder (zip file)

2. (10 pts) Download the csv file sat.csv and read the description of the dataset below.

```{r}
sat <- read.csv("sat.csv")
head(sat)
summary(sat)
```

In 1982, average SAT scores were published with breakdowns of state-by-state performance in the United
States. The average SAT scores varied considerably by state, with mean scores falling between 790 (South
Carolina) to 1088 (Iowa). Two researchers examined compositional and demographic variables to examine to
what extent these characteristics were tied to SAT scores. The variables in the data set were:

state: state name

sat: mean SAT score (verbal and quantitative combined)

takers: percentage of total eligible students (high school seniors) in the state who took the exam

income: median income of families of test takers, in hundreds of dollars.

years: average number of years that test takers had in social sciences, natural sciences, and humanities
(combined)

public: percentage of test takers who attended public schools 

expend: state expenditure on secondary schools, in hundreds of dollars per student

rank: median percentile of ranking of test takers within their secondary school classes. Possible values range from 0-99, with 99th percentile students being the highest achieving.

Fit a model with the sat as the response and expend, income, public and takers as predictors. Perform
regression diagnostics on this model to answer the following questions. Display any plots that are relevent

```{r}
SAT_Model1 <- lm(sat~expend+income+public+takers ,data=sat)
summary(SAT_Model1)
par(mfrow = c(2, 2))
plot(SAT_Model1)
```

(a) Check the constant variance assumption for the errors.

```{r}
ncvTest(SAT_Model1)
plot(SAT_Model1,3)
```

From this we see our errors are relatively homoskedastic around the line with 3 outliers (25,50,29). This could be improved by transforming out outcome variable but we can say that the assumption for constant error variance us fuffilled enough to proceed with our model. 


(b) Check the normality assumption.

```{r}
#Display both plots at the same time 
par(mfrow = c(1, 2))
#Create a QQ plot to see the root - standardized residuals 
qqPlot(SAT_Model1, main="QQ Plot")

#Compute studentized residuals: Like standardized residuals, these are normalized to unit variance, but the Studentized version is fitted ignoring the current data point. (They are sometimes called jackknifed residuals).
sresid <- studres(SAT_Model1)

#Plot studentized residuals and compare how they fall in comparison to a normal distribution
hist(sresid, freq=FALSE,
   main="Distribution of Studentized Residuals")
xfit<-seq(min(sresid),max(sresid),length=40)
yfit<-dnorm(xfit)
lines(xfit, yfit)
```

As we can see from these plots our residuals are distributed roughly normal. 
Since our Normal Q-Q plot is relatively on the line we can assume normality of the residuals (shown by both regular and studentized residuals). 


(c) Check for large leverage points.
```{r}
#Display 2 plots 
par(mfrow = c(2, 2))

#added variable plots: These functions construct added-variable, also called partial-regression, plots for linear and generalized linear models
avPlots(SAT_Model1)
# Cook's D plot
#Set the cutoff value for Cook's Distance as : 4/(n-k-1)
cutoff <- 4/((nrow(sat)-length(SAT_Model1$coefficients)-2))
plot(SAT_Model1, which=4, cook.levels=cutoff)
# Influence Plot
influencePlot(SAT_Model1, id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance (Based on computed cutoff value)" )
```

As we can see points 22, 29, 35 and 50 are considered large leverage points. This makes sense as all of these points (except 35 & 22) were considered outliers so it is logical that are they are having undue influence on the model

(d) Check for the outliers.
```{r}
#Find outliers, plot standardized residuals, find leverage points 
outlierTest(SAT_Model1) 
par(mfrow = c(2, 2))
qqPlot(SAT_Model1, main="QQ Plot") 
leveragePlots(SAT_Model1)
```

As we can see points 29 and 50 are outliers. 

(e) Check the structure of the relationship between the predictors and the response.

```{r}
summary(SAT_Model1)
```

The only predictors that are significant are 'expend' and 'takers'.Thus, the structure is that this model does not appear to be a great method of predicting SAT scores. 

(f) Use pairs function in R to see the relationship between sat and other predictors. 

```{r}
pairs(sat)
```

You can see takers appears to have a quadratic relationship with sat. Include this quadratic effect in your current model
and perform the regression diagnostics. Check the structure of the relationship between the predictors
and the response again.

```{r}
takers2 <- sat$takers^2
SAT_Model2 <- lm(sat~expend+income+public+takers2 ,data=sat)
par(mfrow = c(2, 2))
plot(SAT_Model2)
summary(SAT_Model2)
```

This made our QQ plot perform better (closer to a normal distribution) and caused expend to become insignificant. Thus, the only significant predictor in this model is takers^2.

(g) Using the model in (a)-(e) (no quadratic effect), remove the large leverage points you found and perform
the regression diagnostics. Check for large leverage points again.

```{r}
data_rm <- c(22, 29, 35,50)
SAT_data2 <- sat[-data_rm,]
SAT_Model3 <- lm(sat~expend+income+public+takers ,data=SAT_data2)
summary(SAT_Model3)
par(mfrow = c(2, 2))
plot(SAT_Model3)
```



```{r}
#Display 2 plots 
par(mfrow = c(2, 2))

#added variable plots: These functions construct added-variable, also called partial-regression, plots for linear and generalized linear models
avPlots(SAT_Model3)
# Cook's D plot
#Set the cutoff value for Cook's Distance as : 4/(n-k-1)
cutoff <- 4/((nrow(SAT_data2)-length(SAT_Model3$coefficients)-2))
plot(SAT_Model3, which=4, cook.levels=cutoff)
# Influence Plot
influencePlot(SAT_Model3, id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance (Based on computed cutoff value)" )

```

(h) Comment on which model we should use. The model in (f) or the model in (a)-(e) with the removal of large leverage points that you did in (g)?

The removal of these outliers actually caused us to end up with more outliers. Thus, we should use our original model. 

3. (10 pts) Question 2 in Section 4.7.

2. It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a $\text{N}(\mu_k,\sigma^2) $ distribution, the Bayes Classifier assigns an observation to the class for which the discriminant function is maximized.

In order to implement the Bayes Classifier we have to find the class (k) for which 

$$ p_{k}(x)=\frac{\pi_{k}(1 / \sqrt{2 \pi} \sigma) e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}}}{\sum_{l=1}^{K} \pi_{l}(1 / \sqrt{2 \pi} \sigma) e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{l}\right)^{2}}}=\frac{\pi_{k} e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}}}{\sum_{l=1}^{K} \pi_{l} e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{l}\right)^{2}}} $$
is maximized. To do this we take the log of $ p_{k}(x)$

$$ \log p_{k}(x)=\log \pi_{k}-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}-\log \sum_{l=1}^{K} \pi_{l} e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{l}\right)^{2}} $$
We then set $log{\pi}_k = 0$ thus resulting in:

$$ 0 = \log \pi_{k}-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}-\log \sum_{l=1}^{K} \pi_{l} e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{l}\right)^{2}} $$
Since the last summation is independant of k we can manipulate our statement to be: 

$$ \log \pi_{k}-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}=\log \pi_{k}-\frac{1}{2 \sigma^{2}} x^{2}+\frac{\mu_{k}}{\sigma^{2}} x-\frac{\mu_{k}^{2}}{2 \sigma^{2}} $$ 

From here we can find the largest k for which: 

$$ \delta_{k}(x)=\frac{\mu_{k}}{\sigma^{2}} x-\frac{\mu_{k}^{2}}{2 \sigma^{2}}+\log \pi_{k}$$ 


4. (10 pts) Question 5 in Section 4.7.

5. We now examine the differences between LDA and QDA.

(a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set? 

We expect the QDA to perform better than the LDA on the training set because it is more flexible. We expect the LDA to perform better than the QDA on the on the test set because it is optimized using the Baye's Decision boundary shown to maximize the classification in #2.  

(b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set? 

Since LDA, given the name, assumes Multivariate normality, we expect QDA to out perform it on both sets. 

(c) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?

Because the flexibility of QDA > LDA we use LDA as n-> infinity. Since LDA assumes homogeneity of variance and QDA does not the increasing sample size is not a problem for QDA and our accuracy will increase for our test prediction. 

(d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.

False. Because LDA assumes Multivariate normality and homogeneity of residuals it will fit a linear Baye's decision boundary better. QDA could overfit this due to not recognizing the linearity. 

5. (5 pts) Question 6 in Section 4.7.

6. Suppose we collect data for a group of students in a statistics class with variables $X_1$ =hours studied, $X_2$ =undergrad GPA, and Y = receive an A. We ﬁt a logistic regression and produce estimated coeﬃcient, $\hat{\beta}_{0}=-6, \hat{\beta}_{1}=0.05, \hat{\beta}_{2}=1$

(a) Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class. 

$$ Y=-6+.05x+x+\epsilon $$
$$ \text{probability}(x) = \frac{e^Y}{1+e^Y} $$

```{r}
probx <- ((exp(1))^(-6+(.05*40)+3.5))/(1+(exp(1))^(-6+(.05*40)+3.5))
probx
```

Thus, the estimated probability a student scores an A given 40 hr study and a 3.5 GPA is .3775407 .

(b) How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?

We could do this through algebra, but, We have R! 
Let us simply use our probx and plug in a few values. 

```{r}
probx2 <- ((exp(1))^(-6+(.05*45)+3.5))/(1+(exp(1))^(-6+(.05*45)+3.5))
probx2 <- ((exp(1))^(-6+(.05*45)+3.5))/(1+(exp(1))^(-6+(.05*45)+3.5))
probx2 <- ((exp(1))^(-6+(.05*50)+3.5))/(1+(exp(1))^(-6+(.05*50)+3.5))
probx2
```

Thus, a student would need to study 50 hours. 

6. (5 pts) Question 9 in Section 4.7.

9. This problem has to do with odds.

(a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default? 

To use odds we use the funcion:

odds:

$$ \frac{p(x)}{1-p(x)} $$

after transformation this becomes: 

$$ \frac{.37}{1-.37} = .27 $$  

Thus, 27% percent will default (on average, according to our model). 

(b) Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?

The same logic applies: 

$$ \frac{.16}{1-.16} = .19 $$  
Thus, 19% percent chance of default (on average, according to our model). 


7. (10 pts) Download the csv file arthritis.csv. These data from Koch & Edwards (1988) from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. The data frame has 84 observations and 4 variables, which are:

Treatment: factor indicating treatment (Placebo, Treated).
Sex: factor indicating sex (Female, Male).
Age: age of patient.
Improved: factor indicating treatment outcome (No, Yes).

(a) Fit a logistic regression with the Improved as the response and Treatment, Sex and Age as predictors

```{r}
arthritis <- read.csv("arthritis.csv")
head(arthritis)
summary(arthritis)
```

```{r}
arth_Model1 <- glm(Improved~Treatment+Sex+Age,data=arthritis,family = binomial)
par(mfrow = c(2, 2))
summary(arth_Model1)
plot(arth_Model1)
```
(b) Use log odds to interpret the coefficients $\hat{\beta}_1 ,\hat{\beta}_2, \hat{\beta}_3$

Given someone is treated (TreatmentTreated=1) we expect a 1.76 increase in log odds of improved health (with all other variables held constant)

Given someone is Male (SexMale=1) we expect a -1.49 decrease in log odds of improved health (with all other variables held constant)

Given a 1 unit increase in age (in yearS) we expect a .049 increase in log odds of improved health (with all other variables held constant)

(c) Use odds to interpret the coefficients $\hat{\beta}_1 ,\hat{\beta}_2, \hat{\beta}_3$


Given someone is treated (TreatmentTreated=1) we expect a $e^{1.76}$ increase in odds of improved health (with all other variables held constant)

Given someone is Male (SexMale=1) we expect a $e^{-1.49}$ decrease in odds of improved health (with all other variables held constant)

Given a 1 unit increase in age (in yearS) we expect a e^{.049} increase in  odds of improved health (with all other variables held constant)


(d) Construct 95% confidence intervals for the coefficients.

```{r}
confint(arth_Model1)
```

(e) Add the interaction Sex:Age in your model. For this model, for a one year increase in age, how much does the log odds of some improvement (versus none) increase? Explain it separately with respect to Sex.

```{r}
arth_Model2 <- glm(Improved~Treatment+Sex+Age+Sex:Age,data=arthritis,family = binomial)
par(mfrow = c(2, 2))
summary(arth_Model2)
plot(arth_Model2)
```
A 1 year increase in age, with all other variables held fixed, leads to a .07734 increase in log odds of improvements, independant of sex and an additional -.07945 decrease in log odds of improvements IF the candidate is male. This is due to the interaction variable. 

8. (15 pts) Question 10 in Section 4.7.

10. This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapters lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns? 

```{r}
library(ISLR)
Weekly
pairs(Weekly)
summary(Weekly)
```

Volume is positively correleted with year. 

(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. 

```{r}
Weekly_Model1 <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Weekly,family=binomial)
```

Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones? 

```{r}
summary(Weekly_Model1)
```

Lag2 is the only significant predictor.

(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression. 

```{r}
probs <- predict(Weekly_Model1, type="response")
preds <- rep("Down", 1089)
preds[probs > 0.5] = "Up"
table(preds, Weekly$Direction)
```

Investors are much more likely to bet on a valuation increasing (and end up overvaluing an option in the market) vs. undervaluing a stock that value actually goes up. However, over 50% of predictions are correct (430 "false positives"). This is statistically not bad but I would argue dangerous when actually applied to the markets. 

(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010). 

```{r}
#Create training data
Weekly_training <- Weekly[Weekly$Year<2009,]
Weekly_test <- Weekly[Weekly$Year>2008,]
Weekly_training_glm <- glm(Direction~Lag2, data= Weekly_training, family = "binomial")
summary(Weekly_training_glm)


#Confusion Matrix
probs <- predict(Weekly_training_glm, type="response")
preds <- rep("Down", 985)
preds[probs > 0.5] = "Up"
table(preds, Weekly_training$Direction)

correctpred <- (524+23)/985
correctpred
```

(e) Repeat (d) using LDA. 

```{r}
Weekly_training_lda <- lda(Direction~Lag2, data=Weekly_training)
Weekly_training_lda

#Confusion Matrix
pred <- predict(Weekly_training_lda,newdata = Weekly_test, type="response")
class <- pred$class
table(class,Weekly_test$Direction)

correctpred <- (56+9)/(56+5+9+34)
correctpred
```

(f) Repeat (d) using QDA. 

```{r}
Weekly_training_qda <- qda(Direction~Lag2, data=Weekly_training)
Weekly_training_qda

#Confusion Matrix
pred <- predict(Weekly_training_qda,newdata = Weekly_test, type="response")
class <- pred$class
table(class,Weekly_test$Direction)

correctpred <- (43+61)/(43+61)
correctpred
```

(g) Repeat (d) using KNN with K = 1. 

```{r}
library(class)
Weekly_training_predictor <- cbind(Weekly_training$Lag2)
Weekly_test_predictor <- cbind(Weekly_test$Lag2)
Weekly_training_outcome <- cbind(Weekly_training$Direction)
Weekly_Model_knn <- knn(Weekly_training_predictor, Weekly_test_predictor, Weekly_training_outcome, k=1)
table(Weekly_Model_knn, Weekly_test$Direction)

correctpred <- (31+21)/(21+22+30+31)
correctpred
```

(h) Which of these methods appears to provide the best results on this data? 

The QDA method appears to perform the best! With 100% accuracy. 

(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier

```{r}
#EDA
cor(Weekly[,1:8])

library("Hmisc")
res2 <- rcorr(as.matrix(Weekly[,1:8]))
res2

#Chose to add predictors closest to significant 

#QDA
Weekly_training_qda <- qda(Direction~Lag2+Lag1+Lag4, data=Weekly_training)
Weekly_training_qda

#Confusion Matrix
pred <- predict(Weekly_training_qda,newdata = Weekly_test, type="response")
class <- pred$class
table(class,Weekly_test$Direction)

#KNN
Weekly_training_predictor <- cbind(Weekly_training$Lag2,Weekly_training$Lag1,Weekly_training$Lag4)
Weekly_test_predictor <- cbind(Weekly_test$Lag2,Weekly_training$Lag1,Weekly_training$Lag4)
Weekly_training_outcome <- cbind(Weekly_training$Direction)
Weekly_Model_knn <- knn(Weekly_training_predictor, Weekly_test_predictor, Weekly_training_outcome, k=1)
#table(Weekly_Model_knn, Weekly_test$Direction)
```

I stopped here as we can see that adding predictors increased our error rate and did not help us in our prediction. 

9. (15 pts) Question 11 in Section 4.7.

11. In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.

(a) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may ﬁnd it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.

```{r}
median(Auto$mpg)
Auto$mpg01 <- 0  
Auto$mpg01[Auto$mpg>median(Auto$mpg)] <- 1
head(Auto,30)
```

(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings. 

```{r}
lapply(list("displacement","cylinders","horsepower","weight","acceleration","year","origin"), 
       function(i) ggplot(Auto, aes_string(x=i, y="mpg01")) + geom_point())
```

-Cars with lower displacement, on average, have a higher mpg
-Cylinders and mpg have no real relationship
-Cars with lower horsepower tend to have higher mpg
-Lighter cars have a higher mpg
-As acceleration increases, on avg, so does mpg 

(c) Split the data into a training set and a test set. 

To do this I used a new package I found: caTools

```{r}
library(caTools)
```

```{r}
set.seed(123) #  set seed to ensure you always have same random numbers generated
sample = sample.split(Auto,SplitRatio = 0.75) # splits the data in the ratio mentioned in SplitRatio. After splitting marks these rows as logical TRUE and the the remaining are marked as logical FALSE
train1 =subset(Auto,sample ==TRUE) # creates a training dataset named train1 with rows which are marked as TRUE
test1=subset(Auto, sample==FALSE)
```

(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? 

```{r}
Auto_training_lda <- lda(mpg01~acceleration+horsepower+weight, data=train1)
Auto_training_lda

#Confusion Matrix
pred <- predict(Auto_training_lda,newdata = test1, type="response")
class <- pred$class
table(class,test1$mpg01)

correctpred <- (56+51)/(10+56+51)
correctpred
```

The test error is (1-correctpred)

```{r}
1-correctpred
```

(e) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? 

```{r}
Auto_training_qda <- qda(mpg01~acceleration+horsepower+weight, data=train1)
Auto_training_qda

#Confusion Matrix
pred <- predict(Auto_training_qda,newdata = test1, type="response")
class <- pred$class
table(class,test1$mpg01)

correctpred <- (55+52)/(55+52+10)
correctpred

error_rate <- 1-correctpred
error_rate
```

(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? 

```{r}
Auto_training_glm <- glm(mpg01~acceleration+horsepower+weight, family = "binomial", data=train1)
summary(Auto_training_glm)


#Confusion Matrix
probs <- predict(Auto_training_glm, type="response")
preds <- rep("0", 275)
preds[probs > 0.5] = "1"
table(preds, train1$mpg01)

correctpred <- (124+116)/(124+116+21+14)
correctpred
errorrate <- 1-correctpred
errorrate

```
(g) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?

```{r}
library(class)
#k=1
Auto_training_predictor <- cbind(train1$acceleration,train1$horsepower,train1$weight)
Auto_test_predictor <- cbind(test1$acceleration,test1$horsepower,test1$weight)
Auto_training_outcome <- cbind(train1$mpg01)
Auto_Model_knn <- knn(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, k=1)

print("K=1")
table(Auto_Model_knn, test1$mpg01)



#k=2
Auto_training_predictor <- cbind(train1$acceleration,train1$horsepower,train1$weight)
Auto_test_predictor <- cbind(test1$acceleration,test1$horsepower,test1$weight)
Auto_training_outcome <- cbind(train1$mpg01)
Auto_Model_knn <- knn(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, k=2)

print("K=2")
table(Auto_Model_knn, test1$mpg01)


#k=3
Auto_training_predictor <- cbind(train1$acceleration,train1$horsepower,train1$weight)
Auto_test_predictor <- cbind(test1$acceleration,test1$horsepower,test1$weight)
Auto_training_outcome <- cbind(train1$mpg01)
Auto_Model_knn <- knn(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, k=3)

print("K=3")
table(Auto_Model_knn, test1$mpg01)

#k=5
Auto_training_predictor <- cbind(train1$acceleration,train1$horsepower,train1$weight)
Auto_test_predictor <- cbind(test1$acceleration,test1$horsepower,test1$weight)
Auto_training_outcome <- cbind(train1$mpg01)
Auto_Model_knn <- knn(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, k=5)

print("K=5")
table(Auto_Model_knn, test1$mpg01)

#K=10
Auto_training_predictor <- cbind(train1$acceleration,train1$horsepower,train1$weight)
Auto_test_predictor <- cbind(test1$acceleration,test1$horsepower,test1$weight)
Auto_training_outcome <- cbind(train1$mpg01)
Auto_Model_knn <- knn(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, k=10)

print("K=10")
table(Auto_Model_knn, test1$mpg01)

#k=21
Auto_training_predictor <- cbind(train1$acceleration,train1$horsepower,train1$weight)
Auto_test_predictor <- cbind(test1$acceleration,test1$horsepower,test1$weight)
Auto_training_outcome <- cbind(train1$mpg01)
Auto_Model_knn <- knn(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, k=21)

print("K=21")
table(Auto_Model_knn, test1$mpg01)
```

K=21 performed the best with thee samples, but not significantly different from K=5=3=10. 

10. (10 pts) Download the csv files zipcode_train.csv and zipcode_test.csv. Load and visualize the
files using the following command lines. 

```{r}
train.dat <- read.csv("zipcode_train.csv")
train.dat$Y <- as.factor(train.dat$Y)
test.dat <- read.csv("zipcode_test.csv")
test.dat$Y <- as.factor(test.dat$Y)
COLORS <- c("white", "black")
CUSTOM_COLORS <- colorRampPalette(colors = COLORS)
vis <- function(i){
par(pty = "s", mar = c(1, 1, 1, 1), xaxt = "n", yaxt = "n")
z <- matrix(as.numeric(train.dat[i,1:256]), 16, 16)
image(1:16,1:16,z[,16:1], col = CUSTOM_COLORS(256))
}
vis(2) # hand written 1 (from 1 to 1005)
vis(1500) # hand written 2 (from 1006 to 1736)

```

```{r}
head(test.dat)
```

In the dataset, you have 1736 training images and 462 test images, where each image is a handwritten digit and it can be either 1 or 2. The description of columns is below:

p1-256: the gray scale from -1 to 1.
Y: the digit, which is either 1 or 2.

(a) Perform logistic regression, LDA, and KNN models.

```{r}
print("LDA")
test.dat2 <- test.dat[,-c(16,32)]
train.dat2 <- train.dat[,-c(16,32)] 
# Remove co linear variables 
image_training_lda <- lda(Y~., data=train.dat2)

#Confusion Matrix
pred <- predict(image_training_lda,newdata = test.dat2, type="response")
class <- pred$class
table(class,test.dat2$Y)

correctpred <- (196+259)/(196+259+7)
correctpred

print("glm")
image_training_glm <- lda(Y~., data=train.dat2, family=binomial)

#Confusion Matrix
pred <- predict(image_training_glm,newdata = test.dat, type="response")
class <- pred$class
table(class,test.dat$Y)

correctpred <- (196+259)/(196+259+7)
correctpred

print("KNN, K=1")


training_predictordf <- train.dat[,1:256]
test_predictordf <- test.dat[,1:256]

image_training_outcome <- cbind(train.dat$Y)
image_Model_knn <- knn(training_predictordf, test_predictordf, image_training_outcome, k=1)


print("K=1")
table(image_Model_knn, test.dat$Y)


```

(b) Using the test.dat, which of these methods appears to provide the best results on the test data?


The Knn with k=1 performs the best and has the lowest error rate


\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={STT481Hw2},
            pdfauthor={Sam Isken},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{STT481Hw2}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Sam Isken}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{October 5, 2019}


\begin{document}
\maketitle

FS19 STT481: Homework 2 (Due: Friday, October 18th, beginning of the
class.)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (10 pts) Finish the swirl course Regression Models. Finish Sections
  1-13. You can install and go to the course by using the following
  command lines.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#library(swirl)}
\CommentTok{#install_course("Regression_Models")}
\CommentTok{#swirl()}
\end{Highlighting}
\end{Shaded}

I have included the pictures of my Swirl completion in the attached
folder (zip file)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  (10 pts) Download the csv file sat.csv and read the description of the
  dataset below.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"sat.csv"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(sat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         state  sat takers income years public expend rank
## 1        Iowa 1088      3    326 16.79   87.8  25.60 89.7
## 2 SouthDakota 1075      2    264 16.07   86.2  19.95 90.6
## 3 NorthDakota 1068      3    317 16.57   88.3  20.62 89.8
## 4      Kansas 1045      5    338 16.30   83.9  27.14 86.3
## 5    Nebraska 1045      5    293 17.25   83.6  21.05 88.5
## 6     Montana 1033      8    263 15.91   93.7  29.48 86.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         state         sat             takers          income     
##  Alabama   : 1   Min.   : 790.0   Min.   : 2.00   Min.   :208.0  
##  Alaska    : 1   1st Qu.: 889.2   1st Qu.: 6.25   1st Qu.:261.5  
##  Arizona   : 1   Median : 966.0   Median :16.00   Median :295.0  
##  Arkansas  : 1   Mean   : 947.9   Mean   :26.22   Mean   :294.0  
##  California: 1   3rd Qu.: 998.5   3rd Qu.:47.75   3rd Qu.:325.0  
##  Colorado  : 1   Max.   :1088.0   Max.   :69.00   Max.   :401.0  
##  (Other)   :44                                                   
##      years           public          expend           rank      
##  Min.   :14.39   Min.   :44.80   Min.   :13.84   Min.   :69.80  
##  1st Qu.:15.91   1st Qu.:76.92   1st Qu.:19.59   1st Qu.:74.03  
##  Median :16.36   Median :80.80   Median :21.61   Median :80.85  
##  Mean   :16.21   Mean   :81.20   Mean   :22.97   Mean   :79.99  
##  3rd Qu.:16.76   3rd Qu.:88.25   3rd Qu.:26.39   3rd Qu.:85.83  
##  Max.   :17.41   Max.   :97.00   Max.   :50.10   Max.   :90.60  
## 
\end{verbatim}

In 1982, average SAT scores were published with breakdowns of
state-by-state performance in the United States. The average SAT scores
varied considerably by state, with mean scores falling between 790
(South Carolina) to 1088 (Iowa). Two researchers examined compositional
and demographic variables to examine to what extent these
characteristics were tied to SAT scores. The variables in the data set
were:

state: state name

sat: mean SAT score (verbal and quantitative combined)

takers: percentage of total eligible students (high school seniors) in
the state who took the exam

income: median income of families of test takers, in hundreds of
dollars.

years: average number of years that test takers had in social sciences,
natural sciences, and humanities (combined)

public: percentage of test takers who attended public schools

expend: state expenditure on secondary schools, in hundreds of dollars
per student

rank: median percentile of ranking of test takers within their secondary
school classes. Possible values range from 0-99, with 99th percentile
students being the highest achieving.

Fit a model with the sat as the response and expend, income, public and
takers as predictors. Perform regression diagnostics on this model to
answer the following questions. Display any plots that are relevent

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SAT_Model1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(sat}\OperatorTok{~}\NormalTok{expend}\OperatorTok{+}\NormalTok{income}\OperatorTok{+}\NormalTok{public}\OperatorTok{+}\NormalTok{takers ,}\DataTypeTok{data=}\NormalTok{sat)}
\KeywordTok{summary}\NormalTok{(SAT_Model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = sat ~ expend + income + public + takers, data = sat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -79.139 -23.673  -1.953  20.554  72.827 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 1083.1366    84.9776  12.746  < 2e-16 ***
## expend         3.1332     1.0589   2.959  0.00491 ** 
## income        -0.2533     0.1929  -1.313  0.19582    
## public        -0.5656     0.6012  -0.941  0.35177    
## takers        -3.3093     0.3692  -8.965 1.42e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 34.66 on 45 degrees of freedom
## Multiple R-squared:  0.7803, Adjusted R-squared:  0.7607 
## F-statistic: 39.94 on 4 and 45 DF,  p-value: 2.896e-14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(SAT_Model1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-3-1.pdf}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Check the constant variance assumption for the errors.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ncvTest}\NormalTok{(SAT_Model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 3.309361, Df = 1, p = 0.068886
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(SAT_Model1,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-4-1.pdf}

From this we see our errors are relatively homoskedastic around the line
with 3 outliers (25,50,29). This could be improved by transforming out
outcome variable but we can say that the assumption for constant error
variance us fuffilled enough to proceed with our model.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Check the normality assumption.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Display both plots at the same time }
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{#Create a QQ plot to see the root - standardized residuals }
\KeywordTok{qqPlot}\NormalTok{(SAT_Model1, }\DataTypeTok{main=}\StringTok{"QQ Plot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 29 50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Compute studentized residuals: Like standardized residuals, these are normalized to unit variance, but the Studentized version is fitted ignoring the current data point. (They are sometimes called jackknifed residuals).}
\NormalTok{sresid <-}\StringTok{ }\KeywordTok{studres}\NormalTok{(SAT_Model1)}

\CommentTok{#Plot studentized residuals and compare how they fall in comparison to a normal distribution}
\KeywordTok{hist}\NormalTok{(sresid, }\DataTypeTok{freq=}\OtherTok{FALSE}\NormalTok{,}
   \DataTypeTok{main=}\StringTok{"Distribution of Studentized Residuals"}\NormalTok{)}
\NormalTok{xfit<-}\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(sresid),}\KeywordTok{max}\NormalTok{(sresid),}\DataTypeTok{length=}\DecValTok{40}\NormalTok{)}
\NormalTok{yfit<-}\KeywordTok{dnorm}\NormalTok{(xfit)}
\KeywordTok{lines}\NormalTok{(xfit, yfit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-5-1.pdf}

As we can see from these plots our residuals are distributed roughly
normal. Since our Normal Q-Q plot is relatively on the line we can
assume normality of the residuals (shown by both regular and studentized
residuals).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Check for large leverage points.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Display 2 plots }
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{#added variable plots: These functions construct added-variable, also called partial-regression, plots for linear and generalized linear models}
\KeywordTok{avPlots}\NormalTok{(SAT_Model1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Cook's D plot}
\CommentTok{#Set the cutoff value for Cook's Distance as : 4/(n-k-1)}
\NormalTok{cutoff <-}\StringTok{ }\DecValTok{4}\OperatorTok{/}\NormalTok{((}\KeywordTok{nrow}\NormalTok{(sat)}\OperatorTok{-}\KeywordTok{length}\NormalTok{(SAT_Model1}\OperatorTok{$}\NormalTok{coefficients)}\OperatorTok{-}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(SAT_Model1, }\DataTypeTok{which=}\DecValTok{4}\NormalTok{, }\DataTypeTok{cook.levels=}\NormalTok{cutoff)}
\CommentTok{# Influence Plot}
\KeywordTok{influencePlot}\NormalTok{(SAT_Model1, }\DataTypeTok{id.method=}\StringTok{"identify"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Influence Plot"}\NormalTok{, }\DataTypeTok{sub=}\StringTok{"Circle size is proportial to Cook's Distance (Based on computed cutoff value)"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in plot.window(...): "id.method" is not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in plot.xy(xy, type, ...): "id.method" is not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in axis(side = side, at = at, labels = labels, ...): "id.method" is
## not a graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): "id.method" is
## not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in box(...): "id.method" is not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in title(...): "id.method" is not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in plot.xy(xy.coords(x, y), type = type, ...): "id.method" is not a
## graphical parameter
\end{verbatim}

\begin{verbatim}
##       StudRes       Hat      CookD
## 22 -0.9981088 0.3329581 0.09946233
## 29 -2.6092480 0.5294350 1.35685279
## 35  2.3979347 0.1511534 0.18523035
## 50 -2.5766150 0.1163300 0.15533113
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-6-2.pdf}

As we can see points 22, 29, 35 and 50 are considered large leverage
points. This makes sense as all of these points (except 35 \& 22) were
considered outliers so it is logical that are they are having undue
influence on the model

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Check for the outliers.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Find outliers, plot standardized residuals, find leverage points }
\KeywordTok{outlierTest}\NormalTok{(SAT_Model1) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## No Studentized residuals with Bonferroni p < 0.05
## Largest |rstudent|:
##     rstudent unadjusted p-value Bonferroni p
## 29 -2.609248           0.012353      0.61764
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{qqPlot}\NormalTok{(SAT_Model1, }\DataTypeTok{main=}\StringTok{"QQ Plot"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 29 50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{leveragePlots}\NormalTok{(SAT_Model1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-7-1.pdf}
\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-7-2.pdf}

As we can see points 29 and 50 are outliers.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Check the structure of the relationship between the predictors and the
  response.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(SAT_Model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = sat ~ expend + income + public + takers, data = sat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -79.139 -23.673  -1.953  20.554  72.827 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 1083.1366    84.9776  12.746  < 2e-16 ***
## expend         3.1332     1.0589   2.959  0.00491 ** 
## income        -0.2533     0.1929  -1.313  0.19582    
## public        -0.5656     0.6012  -0.941  0.35177    
## takers        -3.3093     0.3692  -8.965 1.42e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 34.66 on 45 degrees of freedom
## Multiple R-squared:  0.7803, Adjusted R-squared:  0.7607 
## F-statistic: 39.94 on 4 and 45 DF,  p-value: 2.896e-14
\end{verbatim}

The only predictors that are significant are `expend' and `takers'.Thus,
the structure is that this model does not appear to be a great method of
predicting SAT scores.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  Use pairs function in R to see the relationship between sat and other
  predictors.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs}\NormalTok{(sat)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-9-1.pdf}

You can see takers appears to have a quadratic relationship with sat.
Include this quadratic effect in your current model and perform the
regression diagnostics. Check the structure of the relationship between
the predictors and the response again.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{takers2 <-}\StringTok{ }\NormalTok{sat}\OperatorTok{$}\NormalTok{takers}\OperatorTok{^}\DecValTok{2}
\NormalTok{SAT_Model2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(sat}\OperatorTok{~}\NormalTok{expend}\OperatorTok{+}\NormalTok{income}\OperatorTok{+}\NormalTok{public}\OperatorTok{+}\NormalTok{takers2 ,}\DataTypeTok{data=}\NormalTok{sat)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(SAT_Model2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-10-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(SAT_Model2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = sat ~ expend + income + public + takers2, data = sat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -92.309 -28.063  -1.835  27.869  91.315 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 952.108989 110.259635   8.635 4.18e-11 ***
## expend        1.584423   1.364969   1.161    0.252    
## income        0.096563   0.248311   0.389    0.699    
## public       -0.294323   0.803357  -0.366    0.716    
## takers2      -0.038738   0.007419  -5.221 4.39e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 45.65 on 45 degrees of freedom
## Multiple R-squared:  0.6188, Adjusted R-squared:  0.5849 
## F-statistic: 18.26 on 4 and 45 DF,  p-value: 5.633e-09
\end{verbatim}

This made our QQ plot perform better (closer to a normal distribution)
and caused expend to become insignificant. Thus, the only significant
predictor in this model is takers\^{}2.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  Using the model in (a)-(e) (no quadratic effect), remove the large
  leverage points you found and perform the regression diagnostics.
  Check for large leverage points again.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_rm <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{35}\NormalTok{,}\DecValTok{50}\NormalTok{)}
\NormalTok{SAT_data2 <-}\StringTok{ }\NormalTok{sat[}\OperatorTok{-}\NormalTok{data_rm,]}
\NormalTok{SAT_Model3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(sat}\OperatorTok{~}\NormalTok{expend}\OperatorTok{+}\NormalTok{income}\OperatorTok{+}\NormalTok{public}\OperatorTok{+}\NormalTok{takers ,}\DataTypeTok{data=}\NormalTok{SAT_data2)}
\KeywordTok{summary}\NormalTok{(SAT_Model3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = sat ~ expend + income + public + takers, data = SAT_data2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -54.875 -21.679  -1.783  15.968  75.348 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 1038.2393    89.1391  11.647 1.39e-14 ***
## expend         3.8929     1.0576   3.681 0.000671 ***
## income        -0.2021     0.1940  -1.042 0.303627    
## public        -0.3601     0.5985  -0.602 0.550731    
## takers        -3.3838     0.3500  -9.669 3.91e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 30.1 on 41 degrees of freedom
## Multiple R-squared:  0.828,  Adjusted R-squared:  0.8113 
## F-statistic: 49.36 on 4 and 41 DF,  p-value: 3.809e-15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(SAT_Model3)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Display 2 plots }
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{#added variable plots: These functions construct added-variable, also called partial-regression, plots for linear and generalized linear models}
\KeywordTok{avPlots}\NormalTok{(SAT_Model3)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-12-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Cook's D plot}
\CommentTok{#Set the cutoff value for Cook's Distance as : 4/(n-k-1)}
\NormalTok{cutoff <-}\StringTok{ }\DecValTok{4}\OperatorTok{/}\NormalTok{((}\KeywordTok{nrow}\NormalTok{(SAT_data2)}\OperatorTok{-}\KeywordTok{length}\NormalTok{(SAT_Model3}\OperatorTok{$}\NormalTok{coefficients)}\OperatorTok{-}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(SAT_Model3, }\DataTypeTok{which=}\DecValTok{4}\NormalTok{, }\DataTypeTok{cook.levels=}\NormalTok{cutoff)}
\CommentTok{# Influence Plot}
\KeywordTok{influencePlot}\NormalTok{(SAT_Model3, }\DataTypeTok{id.method=}\StringTok{"identify"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Influence Plot"}\NormalTok{, }\DataTypeTok{sub=}\StringTok{"Circle size is proportial to Cook's Distance (Based on computed cutoff value)"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in plot.window(...): "id.method" is not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in plot.xy(xy, type, ...): "id.method" is not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in axis(side = side, at = at, labels = labels, ...): "id.method" is
## not a graphical parameter

## Warning in axis(side = side, at = at, labels = labels, ...): "id.method" is
## not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in box(...): "id.method" is not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in title(...): "id.method" is not a graphical parameter
\end{verbatim}

\begin{verbatim}
## Warning in plot.xy(xy.coords(x, y), type = type, ...): "id.method" is not a
## graphical parameter
\end{verbatim}

\begin{verbatim}
##       StudRes        Hat        CookD
## 1   2.0812481 0.08149061 0.0710840495
## 2   1.8840576 0.16615115 0.1331782607
## 6  -0.2267100 0.23357118 0.0032068956
## 28  2.8471633 0.09323455 0.1420751717
## 36  0.1209622 0.18778705 0.0006932506
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-12-2.pdf}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{7}
\tightlist
\item
  Comment on which model we should use. The model in (f) or the model in
  (a)-(e) with the removal of large leverage points that you did in (g)?
\end{enumerate}

The removal of these outliers actually caused us to end up with more
outliers. Thus, we should use our original model.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  (10 pts) Question 2 in Section 4.7.
\item
  It was stated in the text that classifying an observation to the class
  for which (4.12) is largest is equivalent to classifying an
  observation to the class for which (4.13) is largest. Prove that this
  is the case. In other words, under the assumption that the
  observations in the kth class are drawn from a
  \$\text{N}(\mu\_k,\sigma\^{}2) \$ distribution, the Bayes Classifier
  assigns an observation to the class for which the discriminant
  function is maximized.
\end{enumerate}

In order to implement the Bayes Classifier we have to find the class (k)
for which

\[ p_{k}(x)=\frac{\pi_{k}(1 / \sqrt{2 \pi} \sigma) e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}}}{\sum_{l=1}^{K} \pi_{l}(1 / \sqrt{2 \pi} \sigma) e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{l}\right)^{2}}}=\frac{\pi_{k} e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}}}{\sum_{l=1}^{K} \pi_{l} e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{l}\right)^{2}}} \]
is maximized. To do this we take the log of \$ p\_\{k\}(x)\$

\[ \log p_{k}(x)=\log \pi_{k}-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}-\log \sum_{l=1}^{K} \pi_{l} e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{l}\right)^{2}} \]
We then set \(log{\pi}_k = 0\) thus resulting in:

\[ 0 = \log \pi_{k}-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}-\log \sum_{l=1}^{K} \pi_{l} e^{-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{l}\right)^{2}} \]
Since the last summation is independant of k we can manipulate our
statement to be:

\[ \log \pi_{k}-\left(1 / 2 \sigma^{2}\right)\left(x-\mu_{k}\right)^{2}=\log \pi_{k}-\frac{1}{2 \sigma^{2}} x^{2}+\frac{\mu_{k}}{\sigma^{2}} x-\frac{\mu_{k}^{2}}{2 \sigma^{2}} \]

From here we can find the largest k for which:

\[ \delta_{k}(x)=\frac{\mu_{k}}{\sigma^{2}} x-\frac{\mu_{k}^{2}}{2 \sigma^{2}}+\log \pi_{k}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  (10 pts) Question 5 in Section 4.7.
\item
  We now examine the differences between LDA and QDA.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  If the Bayes decision boundary is linear, do we expect LDA or QDA to
  perform better on the training set? On the test set?
\end{enumerate}

We expect the QDA to perform better than the LDA on the training set
because it is more flexible. We expect the LDA to perform better than
the QDA on the on the test set because it is optimized using the Baye's
Decision boundary shown to maximize the classification in \#2.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  If the Bayes decision boundary is non-linear, do we expect LDA or QDA
  to perform better on the training set? On the test set?
\end{enumerate}

Since LDA, given the name, assumes Multivariate normality, we expect QDA
to out perform it on both sets.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  In general, as the sample size n increases, do we expect the test
  prediction accuracy of QDA relative to LDA to improve, decline, or be
  unchanged? Why?
\end{enumerate}

Because the flexibility of QDA \textgreater{} LDA we use LDA as
n-\textgreater{} infinity. Since LDA assumes homogeneity of variance and
QDA does not the increasing sample size is not a problem for QDA and our
accuracy will increase for our test prediction.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  True or False: Even if the Bayes decision boundary for a given problem
  is linear, we will probably achieve a superior test error rate using
  QDA rather than LDA because QDA is flexible enough to model a linear
  decision boundary. Justify your answer.
\end{enumerate}

False. Because LDA assumes Multivariate normality and homogeneity of
residuals it will fit a linear Baye's decision boundary better. QDA
could overfit this due to not recognizing the linearity.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  (5 pts) Question 6 in Section 4.7.
\item
  Suppose we collect data for a group of students in a statistics class
  with variables \(X_1\) =hours studied, \(X_2\) =undergrad GPA, and Y =
  receive an A. We ï¬t a logistic regression and produce estimated
  coeï¬cient,
  \(\hat{\beta}_{0}=-6, \hat{\beta}_{1}=0.05, \hat{\beta}_{2}=1\)
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Estimate the probability that a student who studies for 40h and has an
  undergrad GPA of 3.5 gets an A in the class.
\end{enumerate}

\[ Y=-6+.05x+x+\epsilon \]
\[ \text{probability}(x) = \frac{e^Y}{1+e^Y} \]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probx <-}\StringTok{ }\NormalTok{((}\KeywordTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\OperatorTok{+}\NormalTok{(.}\DecValTok{05}\OperatorTok{*}\DecValTok{40}\NormalTok{)}\OperatorTok{+}\FloatTok{3.5}\NormalTok{))}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\NormalTok{(}\KeywordTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\OperatorTok{+}\NormalTok{(.}\DecValTok{05}\OperatorTok{*}\DecValTok{40}\NormalTok{)}\OperatorTok{+}\FloatTok{3.5}\NormalTok{))}
\NormalTok{probx}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3775407
\end{verbatim}

Thus, the estimated probability a student scores an A given 40 hr study
and a 3.5 GPA is .3775407 .

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  How many hours would the student in part (a) need to study to have a
  50\% chance of getting an A in the class?
\end{enumerate}

We could do this through algebra, but, We have R! Let us simply use our
probx and plug in a few values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probx2 <-}\StringTok{ }\NormalTok{((}\KeywordTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\OperatorTok{+}\NormalTok{(.}\DecValTok{05}\OperatorTok{*}\DecValTok{45}\NormalTok{)}\OperatorTok{+}\FloatTok{3.5}\NormalTok{))}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\NormalTok{(}\KeywordTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\OperatorTok{+}\NormalTok{(.}\DecValTok{05}\OperatorTok{*}\DecValTok{45}\NormalTok{)}\OperatorTok{+}\FloatTok{3.5}\NormalTok{))}
\NormalTok{probx2 <-}\StringTok{ }\NormalTok{((}\KeywordTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\OperatorTok{+}\NormalTok{(.}\DecValTok{05}\OperatorTok{*}\DecValTok{45}\NormalTok{)}\OperatorTok{+}\FloatTok{3.5}\NormalTok{))}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\NormalTok{(}\KeywordTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\OperatorTok{+}\NormalTok{(.}\DecValTok{05}\OperatorTok{*}\DecValTok{45}\NormalTok{)}\OperatorTok{+}\FloatTok{3.5}\NormalTok{))}
\NormalTok{probx2 <-}\StringTok{ }\NormalTok{((}\KeywordTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\OperatorTok{+}\NormalTok{(.}\DecValTok{05}\OperatorTok{*}\DecValTok{50}\NormalTok{)}\OperatorTok{+}\FloatTok{3.5}\NormalTok{))}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\NormalTok{(}\KeywordTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))}\OperatorTok{^}\NormalTok{(}\OperatorTok{-}\DecValTok{6}\OperatorTok{+}\NormalTok{(.}\DecValTok{05}\OperatorTok{*}\DecValTok{50}\NormalTok{)}\OperatorTok{+}\FloatTok{3.5}\NormalTok{))}
\NormalTok{probx2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5
\end{verbatim}

Thus, a student would need to study 50 hours.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  (5 pts) Question 9 in Section 4.7.
\item
  This problem has to do with odds.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  On average, what fraction of people with an odds of 0.37 of defaulting
  on their credit card payment will in fact default?
\end{enumerate}

To use odds we use the funcion:

odds:

\[ \frac{p(x)}{1-p(x)} \]

after transformation this becomes:

\[ \frac{.37}{1-.37} = .27 \]

Thus, 27\% percent will default (on average, according to our model).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Suppose that an individual has a 16\% chance of defaulting on her
  credit card payment. What are the odds that she will default?
\end{enumerate}

The same logic applies:

\[ \frac{.16}{1-.16} = .19 \]\\
Thus, 19\% percent chance of default (on average, according to our
model).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  (10 pts) Download the csv file arthritis.csv. These data from Koch \&
  Edwards (1988) from a double-blind clinical trial investigating a new
  treatment for rheumatoid arthritis. The data frame has 84 observations
  and 4 variables, which are:
\end{enumerate}

Treatment: factor indicating treatment (Placebo, Treated). Sex: factor
indicating sex (Female, Male). Age: age of patient. Improved: factor
indicating treatment outcome (No, Yes).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Fit a logistic regression with the Improved as the response and
  Treatment, Sex and Age as predictors
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arthritis <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"arthritis.csv"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(arthritis)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Treatment  Sex Age Improved
## 1   Treated Male  27      Yes
## 2   Treated Male  29       No
## 3   Treated Male  30       No
## 4   Treated Male  32      Yes
## 5   Treated Male  46      Yes
## 6   Treated Male  58      Yes
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(arthritis)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Treatment      Sex          Age        Improved
##  Placebo:43   Female:59   Min.   :23.00   No :42  
##  Treated:41   Male  :25   1st Qu.:46.00   Yes:42  
##                           Median :57.00           
##                           Mean   :53.36           
##                           3rd Qu.:63.00           
##                           Max.   :74.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arth_Model1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Improved}\OperatorTok{~}\NormalTok{Treatment}\OperatorTok{+}\NormalTok{Sex}\OperatorTok{+}\NormalTok{Age,}\DataTypeTok{data=}\NormalTok{arthritis,}\DataTypeTok{family =}\NormalTok{ binomial)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(arth_Model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Improved ~ Treatment + Sex + Age, family = binomial, 
##     data = arthritis)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.10833  -0.91158   0.05362   0.91681   1.84659  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(>|z|)   
## (Intercept)      -3.01546    1.16777  -2.582  0.00982 **
## TreatmentTreated  1.75980    0.53650   3.280  0.00104 **
## SexMale          -1.48783    0.59477  -2.502  0.01237 * 
## Age               0.04875    0.02066   2.359  0.01832 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 116.449  on 83  degrees of freedom
## Residual deviance:  92.063  on 80  degrees of freedom
## AIC: 100.06
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(arth_Model1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-16-1.pdf}
(b) Use log odds to interpret the coefficients
\(\hat{\beta}_1 ,\hat{\beta}_2, \hat{\beta}_3\)

Given someone is treated (TreatmentTreated=1) we expect a 1.76 increase
in log odds of improved health (with all other variables held constant)

Given someone is Male (SexMale=1) we expect a -1.49 decrease in log odds
of improved health (with all other variables held constant)

Given a 1 unit increase in age (in yearS) we expect a .049 increase in
log odds of improved health (with all other variables held constant)

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Use odds to interpret the coefficients
  \(\hat{\beta}_1 ,\hat{\beta}_2, \hat{\beta}_3\)
\end{enumerate}

Given someone is treated (TreatmentTreated=1) we expect a \(e^{1.76}\)
increase in odds of improved health (with all other variables held
constant)

Given someone is Male (SexMale=1) we expect a \(e^{-1.49}\) decrease in
odds of improved health (with all other variables held constant)

Given a 1 unit increase in age (in yearS) we expect a e\^{}\{.049\}
increase in odds of improved health (with all other variables held
constant)

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Construct 95\% confidence intervals for the coefficients.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(arth_Model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Waiting for profiling to be done...
\end{verbatim}

\begin{verbatim}
##                         2.5 %      97.5 %
## (Intercept)      -5.477304188 -0.84351926
## TreatmentTreated  0.750803551  2.87506538
## SexMale          -2.729719456 -0.37239953
## Age               0.009951561  0.09194283
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Add the interaction Sex:Age in your model. For this model, for a one
  year increase in age, how much does the log odds of some improvement
  (versus none) increase? Explain it separately with respect to Sex.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arth_Model2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Improved}\OperatorTok{~}\NormalTok{Treatment}\OperatorTok{+}\NormalTok{Sex}\OperatorTok{+}\NormalTok{Age}\OperatorTok{+}\NormalTok{Sex}\OperatorTok{:}\NormalTok{Age,}\DataTypeTok{data=}\NormalTok{arthritis,}\DataTypeTok{family =}\NormalTok{ binomial)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(arth_Model2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Improved ~ Treatment + Sex + Age + Sex:Age, family = binomial, 
##     data = arthritis)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.30304  -0.88850   0.01037   0.91087   2.12913  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(>|z|)   
## (Intercept)      -4.55477    1.56009  -2.920  0.00351 **
## TreatmentTreated  1.79705    0.55996   3.209  0.00133 **
## SexMale           2.75066    2.34871   1.171  0.24155   
## Age               0.07734    0.02775   2.787  0.00532 **
## SexMale:Age      -0.07945    0.04313  -1.842  0.06545 . 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 116.45  on 83  degrees of freedom
## Residual deviance:  88.64  on 79  degrees of freedom
## AIC: 98.64
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(arth_Model2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-18-1.pdf} A
1 year increase in age, with all other variables held fixed, leads to a
.07734 increase in log odds of improvements, independant of sex and an
additional -.07945 decrease in log odds of improvements IF the candidate
is male. This is due to the interaction variable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  (15 pts) Question 10 in Section 4.7.
\item
  This question should be answered using the Weekly data set, which is
  part of the ISLR package. This data is similar in nature to the
  Smarket data from this chapters lab, except that it contains 1,089
  weekly returns for 21 years, from the beginning of 1990 to the end of
  2010.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Produce some numerical and graphical summaries of the Weekly data. Do
  there appear to be any patterns?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'ISLR' was built under R version 3.5.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Weekly}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Year    Lag1    Lag2    Lag3    Lag4    Lag5    Volume   Today
## 1    1990   0.816   1.572  -3.936  -0.229  -3.484 0.1549760  -0.270
## 2    1990  -0.270   0.816   1.572  -3.936  -0.229 0.1485740  -2.576
## 3    1990  -2.576  -0.270   0.816   1.572  -3.936 0.1598375   3.514
## 4    1990   3.514  -2.576  -0.270   0.816   1.572 0.1616300   0.712
## 5    1990   0.712   3.514  -2.576  -0.270   0.816 0.1537280   1.178
## 6    1990   1.178   0.712   3.514  -2.576  -0.270 0.1544440  -1.372
## 7    1990  -1.372   1.178   0.712   3.514  -2.576 0.1517220   0.807
## 8    1990   0.807  -1.372   1.178   0.712   3.514 0.1323100   0.041
## 9    1990   0.041   0.807  -1.372   1.178   0.712 0.1439720   1.253
## 10   1990   1.253   0.041   0.807  -1.372   1.178 0.1336350  -2.678
## 11   1990  -2.678   1.253   0.041   0.807  -1.372 0.1490240  -1.793
## 12   1990  -1.793  -2.678   1.253   0.041   0.807 0.1357900   2.820
## 13   1990   2.820  -1.793  -2.678   1.253   0.041 0.1398980   4.022
## 14   1990   4.022   2.820  -1.793  -2.678   1.253 0.1643420   0.750
## 15   1990   0.750   4.022   2.820  -1.793  -2.678 0.1756480  -0.017
## 16   1990  -0.017   0.750   4.022   2.820  -1.793 0.1634700   2.420
## 17   1990   2.420  -0.017   0.750   4.022   2.820 0.1726250  -1.225
## 18   1990  -1.225   2.420  -0.017   0.750   4.022 0.1684460   1.171
## 19   1990   1.171  -1.225   2.420  -0.017   0.750 0.1552920  -2.061
## 20   1990  -2.061   1.171  -1.225   2.420  -0.017 0.1433920   0.729
## 21   1990   0.729  -2.061   1.171  -1.225   2.420 0.1405540   0.112
## 22   1990   0.112   0.729  -2.061   1.171  -1.225 0.1250750   2.480
## 23   1990   2.480   0.112   0.729  -2.061   1.171 0.1716040  -1.552
## 24   1990  -1.552   2.480   0.112   0.729  -2.061 0.1669560  -2.259
## 25   1990  -2.259  -1.552   2.480   0.112   0.729 0.1717180  -2.428
## 26   1990  -2.428  -2.259  -1.552   2.480   0.112 0.2098160  -2.708
## 27   1990  -2.708  -2.428  -2.259  -1.552   2.480 0.1927060  -2.292
## 28   1990  -2.292  -2.708  -2.428  -2.259  -1.552 0.1482520  -4.978
## 29   1990  -4.978  -2.292  -2.708  -2.428  -2.259 0.1898580   3.547
## 30   1990   3.547  -4.978  -2.292  -2.708  -2.428 0.1278840   0.260
## 31   1990   0.260   3.547  -4.978  -2.292  -2.708 0.1157425  -2.032
## 32   1990  -2.032   0.260   3.547  -4.978  -2.292 0.1239240  -1.739
## 33   1990  -1.739  -2.032   0.260   3.547  -4.978 0.1490820  -1.693
## 34   1990  -1.693  -1.739  -2.032   0.260   3.547 0.1718560   1.781
## 35   1990   1.781  -1.693  -1.739  -2.032   0.260 0.1649700  -3.682
## 36   1990  -3.682   1.781  -1.693  -1.739  -2.032 0.1564540   4.150
## 37   1990   4.150  -3.682   1.781  -1.693  -1.739 0.1802800  -2.487
## 38   1990  -2.487   4.150  -3.682   1.781  -1.693 0.1439780   2.343
## 39   1990   2.343  -2.487   4.150  -3.682   1.781 0.1542920   0.606
## 40   1990   0.606   2.343  -2.487   4.150  -3.682 0.1480060   1.077
## 41   1990   1.077   0.606   2.343  -2.487   4.150 0.1635500  -0.637
## 42   1990  -0.637   1.077   0.606   2.343  -2.487 0.1265325   2.260
## 43   1990   2.260  -0.637   1.077   0.606   2.343 0.1515780   1.716
## 44   1990   1.716   2.260  -0.637   1.077   0.606 0.1979940  -0.284
## 45   1990  -0.284   1.716   2.260  -0.637   1.077 0.1558740   1.508
## 46   1990   1.508  -0.284   1.716   2.260  -0.637 0.1767000  -0.913
## 47   1990  -0.913   1.508  -0.284   1.716   2.260 0.0874650  -2.349
## 48   1991  -2.349  -0.913   1.508  -0.284   1.716 0.1306700  -1.798
## 49   1991  -1.798  -2.349  -0.913   1.508  -0.284 0.1425320   5.393
## 50   1991   5.393  -1.798  -2.349  -0.913   1.508 0.1822480   1.156
## 51   1991   1.156   5.393  -1.798  -2.349  -0.913 0.1798940   2.077
## 52   1991   2.077   1.156   5.393  -1.798  -2.349 0.1949980   4.751
## 53   1991   4.751   2.077   1.156   5.393  -1.798 0.2596560   2.702
## 54   1991   2.702   4.751   2.077   1.156   5.393 0.2381400  -0.924
## 55   1991  -0.924   2.702   4.751   2.077   1.156 0.1937775   1.318
## 56   1991   1.318  -0.924   2.702   4.751   2.077 0.2027840   1.209
## 57   1991   1.209   1.318  -0.924   2.702   4.751 0.2239460  -0.363
## 58   1991  -0.363   1.209   1.318  -0.924   2.702 0.1967520  -1.635
## 59   1991  -1.635  -0.363   1.209   1.318  -0.924 0.1795400   2.106
## 60   1991   2.106  -1.635  -0.363   1.209   1.318 0.1763050   0.037
## 61   1991   0.037   2.106  -1.635  -0.363   1.209 0.1865580   1.343
## 62   1991   1.343   0.037   2.106  -1.635  -0.363 0.1743280   0.999
## 63   1991   0.999   1.343   0.037   2.106  -1.635 0.2072280  -1.348
## 64   1991  -1.348   0.999   1.343   0.037   2.106 0.1641080   0.470
## 65   1991   0.470  -1.348   0.999   1.343   0.037 0.1766460  -1.329
## 66   1991  -1.329   0.470  -1.348   0.999   1.343 0.1585660  -0.892
## 67   1991  -0.892  -1.329   0.470  -1.348   0.999 0.1718580   1.370
## 68   1991   1.370  -0.892  -1.329   0.470  -1.348 0.1486320   3.269
## 69   1991   3.269   1.370  -0.892  -1.329   0.470 0.2043200  -2.668
## 70   1991  -2.668   3.269   1.370  -0.892  -1.329 0.1757660   0.754
## 71   1991   0.754  -2.668   3.269   1.370  -0.892 0.1538140  -1.188
## 72   1991  -1.188   0.754  -2.668   3.269   1.370 0.1606320  -1.745
## 73   1991  -1.745  -1.188   0.754  -2.668   3.269 0.1615340   0.787
## 74   1991   0.787  -1.745  -1.188   0.754  -2.668 0.1338150   1.649
## 75   1991   1.649   0.787  -1.745  -1.188   0.754 0.1602280   1.044
## 76   1991   1.044   1.649   0.787  -1.745  -1.188 0.1863660  -0.856
## 77   1991  -0.856   1.044   1.649   0.787  -1.745 0.1483000   1.641
## 78   1991   1.641  -0.856   1.044   1.649   0.787 0.1609440  -0.015
## 79   1991  -0.015   1.641  -0.856   1.044   1.649 0.1564720  -0.398
## 80   1991  -0.398  -0.015   1.641  -0.856   1.044 0.1693200   2.228
## 81   1991   2.228  -0.398  -0.015   1.641  -0.856 0.2018520   0.320
## 82   1991   0.320   2.228  -0.398  -0.015   1.641 0.1485440  -1.601
## 83   1991  -1.601   0.320   2.228  -0.398  -0.015 0.1600150  -1.416
## 84   1991  -1.416  -1.601   0.320   2.228  -0.398 0.1473080   1.129
## 85   1991   1.129  -1.416  -1.601   0.320   2.228 0.1895540  -0.521
## 86   1991  -0.521   1.129  -1.416  -1.601   0.320 0.1579680  -1.205
## 87   1991  -1.205  -0.521   1.129  -1.416  -1.601 0.1630180   0.052
## 88   1991   0.052  -1.205  -0.521   1.129  -1.416 0.1650700   2.897
## 89   1991   2.897   0.052  -1.205  -0.521   1.129 0.1958320  -2.115
## 90   1991  -2.115   2.897   0.052  -1.205  -0.521 0.1760080   1.853
## 91   1991   1.853  -2.115   2.897   0.052  -1.205 0.1870600   0.401
## 92   1991   0.401   1.853  -2.115   2.897   0.052 0.1767860  -2.614
## 93   1991  -2.614   0.401   1.853  -2.115   2.897 0.1903460  -1.694
## 94   1991  -1.694  -2.614   0.401   1.853  -2.115 0.2123900  -0.245
## 95   1991  -0.245  -1.694  -2.614   0.401   1.853 0.1585575   1.034
## 96   1991   1.034  -0.245  -1.694  -2.614   0.401 0.1858220   1.417
## 97   1991   1.417   1.034  -0.245  -1.694  -2.614 0.1925060   0.668
## 98   1991   0.668   1.417   1.034  -0.245  -1.694 0.2144540   5.018
## 99   1991   5.018   0.668   1.417   1.034  -0.245 0.1746800   3.169
## 100  1992   3.169   5.018   0.668   1.417   1.034 0.2311300  -1.011
## 101  1992  -1.011   3.169   5.018   0.668   1.417 0.2646440   0.906
## 102  1992   0.906  -1.011   3.169   5.018   0.668 0.2809220  -0.807
## 103  1992  -0.807   0.906  -1.011   3.169   5.018 0.2152000  -1.613
## 104  1992  -1.613  -0.807   0.906  -1.011   3.169 0.2101220   0.565
## 105  1992   0.565  -1.613  -0.807   0.906  -1.011 0.2309160   0.338
## 106  1992   0.338   0.565  -1.613  -0.807   0.906 0.2133280  -0.255
## 107  1992  -0.255   0.338   0.565  -1.613  -0.807 0.2498925   0.309
## 108  1992   0.309  -0.255   0.338   0.565  -1.613 0.2093640  -2.001
## 109  1992  -2.001   0.309  -0.255   0.338   0.565 0.1958180   0.346
## 110  1992   0.346  -2.001   0.309  -0.255   0.338 0.1816380   1.345
## 111  1992   1.345   0.346  -2.001   0.309  -0.255 0.1956880  -1.896
## 112  1992  -1.896   1.345   0.346  -2.001   0.309 0.1768340  -0.483
## 113  1992  -0.483  -1.896   1.345   0.346  -2.001 0.1753340   0.682
## 114  1992   0.682  -0.483  -1.896   1.345   0.346 0.2130720   2.906
## 115  1992   2.906   0.682  -0.483  -1.896   1.345 0.2093025  -1.687
## 116  1992  -1.687   2.906   0.682  -0.483  -1.896 0.2120920   0.858
## 117  1992   0.858  -1.687   2.906   0.682  -0.483 0.1939760   0.853
## 118  1992   0.853   0.858  -1.687   2.906   0.682 0.1825480  -1.433
## 119  1992  -1.433   0.853   0.858  -1.687   2.906 0.1812680   0.958
## 120  1992   0.958  -1.433   0.853   0.858  -1.687 0.1736520   0.321
## 121  1992   0.321   0.958  -1.433   0.853   0.858 0.1948125  -0.450
## 122  1992  -0.450   0.321   0.958  -1.433   0.853 0.2005260  -0.900
## 123  1992  -0.900  -0.450   0.321   0.958  -1.433 0.1899600  -1.486
## 124  1992  -1.486  -0.900  -0.450   0.321   0.958 0.2090600  -0.054
## 125  1992  -0.054  -1.486  -0.900  -0.450   0.321 0.1779640   2.062
## 126  1992   2.062  -0.054  -1.486  -0.900  -0.450 0.2016825   0.692
## 127  1992   0.692   2.062  -0.054  -1.486  -0.900 0.1973500   0.241
## 128  1992   0.241   0.692   2.062  -0.054  -1.486 0.1900040  -0.967
## 129  1992  -0.967   0.241   0.692   2.062  -0.054 0.1738120   3.064
## 130  1992   3.064  -0.967   0.241   0.692   2.062 0.2049880  -1.256
## 131  1992  -1.256   3.064  -0.967   0.241   0.692 0.1751500   0.246
## 132  1992   0.246  -1.256   3.064  -0.967   0.241 0.1691100  -1.205
## 133  1992  -1.205   0.246  -1.256   3.064  -0.967 0.1799740  -0.002
## 134  1992  -0.002  -1.205   0.246  -1.256   3.064 0.1742340   0.540
## 135  1992   0.540  -0.002  -1.205   0.246  -1.256 0.1717040   0.599
## 136  1992   0.599   0.540  -0.002  -1.205   0.246 0.1856975   0.798
## 137  1992   0.798   0.599   0.540  -0.002  -1.205 0.2239920  -2.029
## 138  1992  -2.029   0.798   0.599   0.540  -0.002 0.1900160  -0.936
## 139  1992  -0.936  -2.029   0.798   0.599   0.540 0.1813580  -1.903
## 140  1992  -1.903  -0.936  -2.029   0.798   0.599 0.2116740   2.253
## 141  1992   2.253  -1.903  -0.936  -2.029   0.798 0.1877460   0.576
## 142  1992   0.576   2.253  -1.903  -0.936  -2.029 0.2229840   1.106
## 143  1992   1.106   0.576   2.253  -1.903  -0.936 0.2004360  -0.263
## 144  1992  -0.263   1.106   0.576   2.253  -1.903 0.2061720   1.161
## 145  1992   1.161  -0.263   1.106   0.576   2.253 0.2166900   0.999
## 146  1992   0.999   1.161  -0.263   1.106   0.576 0.2113040   0.823
## 147  1992   0.823   0.999   1.161  -0.263   1.106 0.1869475   0.442
## 148  1992   0.442   0.823   0.999   1.161  -0.263 0.2418440   0.387
## 149  1992   0.387   0.442   0.823   0.999   1.161 0.2174480   1.741
## 150  1992   1.741   0.387   0.442   0.823   0.999 0.2595760  -0.342
## 151  1992  -0.342   1.741   0.387   0.442   0.823 0.2011225  -0.923
## 152  1993  -0.923  -0.342   1.741   0.387   0.442 0.1768675  -1.529
## 153  1993  -1.529  -0.923  -0.342   1.741   0.387 0.2610240   1.888
## 154  1993   1.888  -1.529  -0.923  -0.342   1.741 0.2585360  -0.238
## 155  1993  -0.238   1.888  -1.529  -0.923  -0.342 0.2598000   0.612
## 156  1993   0.612  -0.238   1.888  -1.529  -0.923 0.2768100   2.313
## 157  1993   2.313   0.612  -0.238   1.888  -1.529 0.3062780  -0.969
## 158  1993  -0.969   2.313   0.612  -0.238   1.888 0.2419440  -2.330
## 159  1993  -2.330  -0.969   2.313   0.612  -0.238 0.3142350   2.110
## 160  1993   2.110  -2.330  -0.969   2.313   0.612 0.2888800   0.616
## 161  1993   0.616   2.110  -2.330  -0.969   2.313 0.2534580   0.834
## 162  1993   0.834   0.616   2.110  -2.330  -0.969 0.2668100   0.078
## 163  1993   0.078   0.834   0.616   2.110  -2.330 0.2473720  -0.533
## 164  1993  -0.533   0.078   0.834   0.616   2.110 0.2436800  -1.427
## 165  1993  -1.427  -0.533   0.078   0.834   0.616 0.2536420   0.102
## 166  1993   0.102  -1.427  -0.533   0.078   0.834 0.2935325   1.607
## 167  1993   1.607   0.102  -1.427  -0.533   0.078 0.2736760  -2.653
## 168  1993  -2.653   1.607   0.102  -1.427  -0.533 0.2840400   0.723
## 169  1993   0.723  -2.653   1.607   0.102  -1.427 0.2665200   0.482
## 170  1993   0.482   0.723  -2.653   1.607   0.102 0.2493100  -0.622
## 171  1993  -0.622   0.482   0.723  -2.653   1.607 0.2513140   1.429
## 172  1993   1.429  -0.622   0.482   0.723  -2.653 0.2805160   0.976
## 173  1993   0.976   1.429  -0.622   0.482   0.723 0.2405880  -0.029
## 174  1993  -0.029   0.976   1.429  -0.622   0.482 0.2593150  -0.622
## 175  1993  -0.622  -0.029   0.976   1.429  -0.622 0.2431880  -0.800
## 176  1993  -0.800  -0.622  -0.029   0.976   1.429 0.2504720   0.884
## 177  1993   0.884  -0.800  -0.622  -0.029   0.976 0.2478640  -0.393
## 178  1993  -0.393   0.884  -0.800  -0.622  -0.029 0.2624620   0.509
## 179  1993   0.509  -0.393   0.884  -0.800  -0.622 0.2515250  -0.527
## 180  1993  -0.527   0.509  -0.393   0.884  -0.800 0.2554740   0.303
## 181  1993   0.303  -0.527   0.509  -0.393   0.884 0.2488360   0.230
## 182  1993   0.230   0.303  -0.527   0.509  -0.393 0.2536180   0.123
## 183  1993   0.123   0.230   0.303  -0.527   0.509 0.2393200   0.325
## 184  1993   0.325   0.123   0.230   0.303  -0.527 0.2499000   1.337
## 185  1993   1.337   0.325   0.123   0.230   0.303 0.2756060   0.960
## 186  1993   0.960   1.337   0.325   0.123   0.230 0.2470120   0.174
## 187  1993   0.174   0.960   1.337   0.325   0.123 0.2298160   0.082
## 188  1993   0.082   0.174   0.960   1.337   0.325 0.2601550  -0.626
## 189  1993  -0.626   0.082   0.174   0.960   1.337 0.2818200  -0.262
## 190  1993  -0.262  -0.626   0.082   0.174   0.960 0.2708040   0.798
## 191  1993   0.798  -0.262  -0.626   0.082   0.174 0.2607580  -0.210
## 192  1993  -0.210   0.798  -0.262  -0.626   0.082 0.2599660   1.996
## 193  1993   1.996  -0.210   0.798  -0.262  -0.626 0.2913200  -1.327
## 194  1993  -1.327   1.996  -0.210   0.798  -0.262 0.3061380   0.984
## 195  1993   0.984  -1.327   1.996  -0.210   0.798 0.2792920  -1.766
## 196  1993  -1.766   0.984  -1.327   1.996  -0.210 0.3126480   1.266
## 197  1993   1.266  -1.766   0.984  -1.327   1.996 0.2808420  -0.599
## 198  1993  -0.599   1.266  -1.766   0.984  -1.327 0.2976820   0.099
## 199  1993   0.099  -0.599   1.266  -1.766   0.984 0.2153450   0.395
## 200  1993   0.395   0.099  -0.599   1.266  -1.766 0.2755940  -0.207
## 201  1993  -0.207   0.395   0.099  -0.599   1.266 0.2851420   0.528
## 202  1993   0.528  -0.207   0.395   0.099  -0.599 0.3023540   0.214
## 203  1993   0.214   0.528  -0.207   0.395   0.099 0.2572375  -0.199
## 204  1994  -0.199   0.214   0.528  -0.207   0.395 0.2012360   0.740
## 205  1994   0.740  -0.199   0.214   0.528  -0.207 0.3375300   1.066
## 206  1994   1.066   0.740  -0.199   0.214   0.528 0.3037120  -0.040
## 207  1994  -0.040   1.066   0.740  -0.199   0.214 0.3021980   0.838
## 208  1994   0.838  -0.040   1.066   0.740  -0.199 0.3174640  -1.857
## 209  1994  -1.857   0.838  -0.040   1.066   0.740 0.3342140   0.079
## 210  1994   0.079  -1.857   0.838  -0.040   1.066 0.3080220  -0.530
## 211  1994  -0.530   0.079  -1.857   0.838  -0.040 0.2997340  -0.346
## 212  1994  -0.346  -0.530   0.079  -1.857   0.838 0.2993575  -0.285
## 213  1994  -0.285  -0.346  -0.530   0.079  -1.857 0.3075820   0.366
## 214  1994   0.366  -0.285  -0.346  -0.530   0.079 0.3133540   0.990
## 215  1994   0.990   0.366  -0.285  -0.346  -0.530 0.3275420  -2.225
## 216  1994  -2.225   0.990   0.366  -0.285  -0.346 0.2729000  -3.216
## 217  1994  -3.216  -2.225   0.990   0.366  -0.285 0.3467025   0.298
## 218  1994   0.298  -3.216  -2.225   0.990   0.366 0.3131500  -0.206
## 219  1994  -0.206   0.298  -3.216  -2.225   0.990 0.2727760   0.325
## 220  1994   0.325  -0.206   0.298  -3.216  -2.225 0.3271540   0.733
## 221  1994   0.733   0.325  -0.206   0.298  -3.216 0.2924025  -0.685
## 222  1994  -0.685   0.733   0.325  -0.206   0.298 0.2799880  -0.822
## 223  1994  -0.822  -0.685   0.733   0.325  -0.206 0.2701540   2.427
## 224  1994   2.427  -0.822  -0.685   0.733   0.325 0.2965020   0.530
## 225  1994   0.530   2.427  -0.822  -0.685   0.733 0.2452100   0.612
## 226  1994   0.612   0.530   2.427  -0.822  -0.685 0.2599325  -0.317
## 227  1994  -0.317   0.612   0.530   2.427  -0.822 0.2450220  -0.048
## 228  1994  -0.048  -0.317   0.612   0.530   2.427 0.2863540  -3.414
## 229  1994  -3.414  -0.048  -0.317   0.612   0.530 0.2594200   0.768
## 230  1994   0.768  -3.414  -0.048  -0.317   0.612 0.2549380   0.751
## 231  1994   0.751   0.768  -3.414  -0.048  -0.317 0.2319750   1.025
## 232  1994   1.025   0.751   0.768  -3.414  -0.048 0.2678500  -0.231
## 233  1994  -0.231   1.025   0.751   0.768  -3.414 0.2601100   1.137
## 234  1994   1.137  -0.231   1.025   0.751   0.768 0.2426740  -0.255
## 235  1994  -0.255   1.137  -0.231   1.025   0.751 0.2712360   1.061
## 236  1994   1.061  -0.255   1.137  -0.231   1.025 0.2562580   0.377
## 237  1994   0.377   1.061  -0.255   1.137  -0.231 0.2806120   2.183
## 238  1994   2.183   0.377   1.061  -0.255   1.137 0.2885940  -0.593
## 239  1994  -0.593   2.183   0.377   1.061  -0.255 0.2828460  -0.597
## 240  1994  -0.597  -0.593   2.183   0.377   1.061 0.2695925   0.643
## 241  1994   0.643  -0.597  -0.593   2.183   0.377 0.3056400  -2.445
## 242  1994  -2.445   0.643  -0.597  -0.593   2.183 0.3120520   0.661
## 243  1994   0.661  -2.445   0.643  -0.597  -0.593 0.2974120  -1.645
## 244  1994  -1.645   0.661  -2.445   0.643  -0.597 0.3022540   3.076
## 245  1994   3.076  -1.645   0.661  -2.445   0.643 0.2855740  -0.897
## 246  1994  -0.897   3.076  -1.645   0.661  -2.445 0.2924040   1.910
## 247  1994   1.910  -0.897   3.076  -1.645   0.661 0.3281440  -2.425
## 248  1994  -2.425   1.910  -0.897   3.076  -1.645 0.3029700   0.015
## 249  1994   0.015  -2.425   1.910  -0.897   3.076 0.2770760  -0.190
## 250  1994  -0.190   0.015  -2.425   1.910  -0.897 0.3147460  -1.989
## 251  1994  -1.989  -0.190   0.015  -2.425   1.910 0.3073375   0.223
## 252  1994   0.223  -1.989  -0.190   0.015  -2.425 0.2842840  -1.399
## 253  1994  -1.399   0.223  -1.989  -0.190   0.015 0.3079280   2.649
## 254  1994   2.649  -1.399   0.223  -1.989  -0.190 0.3524980   0.224
## 255  1994   0.224   2.649  -1.399   0.223  -1.989 0.3028760  -0.122
## 256  1995  -0.122   0.224   2.649  -1.399   0.223 0.2410875   0.307
## 257  1995   0.307  -0.122   0.224   2.649  -1.399 0.2997700   1.148
## 258  1995   1.148   0.307  -0.122   0.224   2.649 0.3254660  -0.255
## 259  1995  -0.255   1.148   0.307  -0.122   0.224 0.3334800   1.207
## 260  1995   1.207  -0.255   1.148   0.307  -0.122 0.3256220   1.756
## 261  1995   1.756   1.207  -0.255   1.148   0.307 0.3777120   0.587
## 262  1995   0.587   1.756   1.207  -0.255   1.148 0.3159840   0.106
## 263  1995   0.106   0.587   1.756   1.207  -0.255 0.3287980   1.274
## 264  1995   1.274   0.106   0.587   1.756   1.207 0.3361900  -0.551
## 265  1995  -0.551   1.274   0.106   0.587   1.756 0.3252960   0.855
## 266  1995   0.855  -0.551   1.274   0.106   0.587 0.3412920   1.215
## 267  1995   1.215   0.855  -0.551   1.274   0.106 0.3370060   1.100
## 268  1995   1.100   1.215   0.855  -0.551   1.274 0.3317740  -0.052
## 269  1995  -0.052   1.100   1.215   0.855  -0.551 0.3437140   1.140
## 270  1995   1.140  -0.052   1.100   1.215   0.855 0.3154800   0.555
## 271  1995   0.555   1.140  -0.052   1.100   1.215 0.3002750  -0.145
## 272  1995  -0.145   0.555   1.140  -0.052   1.100 0.3656720   1.223
## 273  1995   1.223  -0.145   0.555   1.140  -0.052 0.3400340   1.051
## 274  1995   1.051   1.223  -0.145   0.555   1.140 0.3538260   1.044
## 275  1995   1.044   1.051   1.223  -0.145   0.555 0.3472000  -1.210
## 276  1995  -1.210   1.044   1.051   1.223  -0.145 0.3472520   0.859
## 277  1995   0.859  -1.210   1.044   1.051   1.223 0.3346200   1.692
## 278  1995   1.692   0.859  -1.210   1.044   1.051 0.3382800  -0.858
## 279  1995  -0.858   1.692   0.859  -1.210   1.044 0.3246500   2.252
## 280  1995   2.252  -0.858   1.692   0.859  -1.210 0.3475580   1.830
## 281  1995   1.830   2.252  -0.858   1.692   0.859 0.3692460  -0.902
## 282  1995  -0.902   1.830   2.252  -0.858   1.692 0.3272920   2.133
## 283  1995   2.133  -0.902   1.830   2.252  -0.858 0.3406975   0.633
## 284  1995   0.633   2.133  -0.902   1.830   2.252 0.3806520  -1.120
## 285  1995  -1.120   0.633   2.133  -0.902   1.830 0.3999660   1.682
## 286  1995   1.682  -1.120   0.633   2.133  -0.902 0.3500260  -0.709
## 287  1995  -0.709   1.682  -1.120   0.633   2.133 0.3332680  -0.685
## 288  1995  -0.685  -0.709   1.682  -1.120   0.633 0.2922080   0.739
## 289  1995   0.739  -0.685  -0.709   1.682  -1.120 0.3320220   0.159
## 290  1995   0.159   0.739  -0.685  -0.709   1.682 0.2882340   0.668
## 291  1995   0.668   0.159   0.739  -0.685  -0.709 0.2933280   1.568
## 292  1995   1.568   0.668   0.159   0.739  -0.685 0.3354675   1.863
## 293  1995   1.863   1.568   0.668   0.159   0.739 0.3736020  -0.278
## 294  1995  -0.278   1.863   1.568   0.668   0.159 0.3670400   0.461
## 295  1995   0.461  -0.278   1.863   1.568   0.668 0.3502040  -0.329
## 296  1995  -0.329   0.461  -0.278   1.863   1.568 0.3422940   0.345
## 297  1995   0.345  -0.329   0.461  -0.278   1.863 0.3495020   0.506
## 298  1995   0.506   0.345  -0.329   0.461  -0.278 0.3728760  -1.321
## 299  1995  -1.321   0.506   0.345  -0.329   0.461 0.4046820   1.875
## 300  1995   1.875  -1.321   0.506   0.345  -0.329 0.3640420   0.364
## 301  1995   0.364   1.875  -1.321   0.506   0.345 0.3426020   1.240
## 302  1995   1.240   0.364   1.875  -1.321   0.506 0.3773680  -0.017
## 303  1995  -0.017   1.240   0.364   1.875  -1.321 0.3180800   1.168
## 304  1995   1.168  -0.017   1.240   0.364   1.875 0.3999260   1.730
## 305  1995   1.730   1.168  -0.017   1.240   0.364 0.3935560  -0.185
## 306  1995  -0.185   1.730   1.168  -0.017   1.240 0.4418640  -0.712
## 307  1995  -0.712  -0.185   1.730   1.168  -0.017 0.4095280   0.650
## 308  1996   0.650  -0.712  -0.185   1.730   1.168 0.2698725   0.127
## 309  1996   0.127   0.650  -0.712  -0.185   1.730 0.4457050  -2.416
## 310  1996  -2.416   0.127   0.650  -0.712  -0.185 0.3673580   1.665
## 311  1996   1.665  -2.416   0.127   0.650  -0.712 0.4276500   1.600
## 312  1996   1.600   1.665  -2.416   0.127   0.650 0.4260600   2.288
## 313  1996   2.288   1.600   1.665  -2.416   0.127 0.4362680   3.229
## 314  1996   3.229   2.288   1.600   1.665  -2.416 0.4518080  -1.278
## 315  1996  -1.278   3.229   2.288   1.600   1.665 0.4244220   1.713
## 316  1996   1.713  -1.278   3.229   2.288   1.600 0.4389325  -2.232
## 317  1996  -2.232   1.713  -1.278   3.229   2.288 0.4406220  -1.687
## 318  1996  -1.687  -2.232   1.713  -1.278   3.229 0.4527060   1.252
## 319  1996   1.252  -1.687  -2.232   1.713  -1.278 0.4680220   1.433
## 320  1996   1.433   1.252  -1.687  -2.232   1.713 0.3963500  -0.787
## 321  1996  -0.787   1.433   1.252  -1.687  -2.232 0.3854660   1.605
## 322  1996   1.605  -0.787   1.433   1.252  -1.687 0.3921950  -2.920
## 323  1996  -2.920   1.605  -0.787   1.433   1.252 0.4493460   1.313
## 324  1996   1.313  -2.920   1.605  -0.787   1.433 0.4231440   1.301
## 325  1996   1.301   1.313  -2.920   1.605  -0.787 0.4413860  -1.810
## 326  1996  -1.810   1.301   1.313  -2.920   1.605 0.4038020   1.630
## 327  1996   1.630  -1.810   1.301   1.313  -2.920 0.4229460   2.579
## 328  1996   2.579   1.630  -1.810   1.301   1.313 0.4247240   1.435
## 329  1996   1.435   2.579   1.630  -1.810   1.301 0.3958560  -1.384
## 330  1996  -1.384   1.435   2.579   1.630  -1.810 0.3554800   0.626
## 331  1996   0.626  -1.384   1.435   2.579   1.630 0.3995040  -1.108
## 332  1996  -1.108   0.626  -1.384   1.435   2.579 0.3856620   0.149
## 333  1996   0.149  -1.108   0.626  -1.384   1.435 0.4033420   0.568
## 334  1996   0.568   0.149  -1.108   0.626  -1.384 0.3976600  -1.967
## 335  1996  -1.967   0.568   0.149  -1.108   0.626 0.3128700  -1.711
## 336  1996  -1.711  -1.967   0.568   0.149  -1.108 0.4212580  -1.154
## 337  1996  -1.154  -1.711  -1.967   0.568   0.149 0.4996720  -0.443
## 338  1996  -0.443  -1.154  -1.711  -1.967   0.568 0.3935040   4.181
## 339  1996   4.181  -0.443  -1.154  -1.711  -1.967 0.3814800  -0.059
## 340  1996  -0.059   4.181  -0.443  -1.154  -1.711 0.3421440   0.470
## 341  1996   0.470  -0.059   4.181  -0.443  -1.154 0.3359400   0.274
## 342  1996   0.274   0.470  -0.059   4.181  -0.443 0.3281640  -2.255
## 343  1996  -2.255   0.274   0.470  -0.059   4.181 0.2935780   0.566
## 344  1996   0.566  -2.255   0.274   0.470  -0.059 0.3517925   3.791
## 345  1996   3.791   0.566  -2.255   0.274   0.470 0.3897100   0.954
## 346  1996   0.954   3.791   0.566  -2.255   0.274 0.4389060  -0.122
## 347  1996  -0.122   0.954   3.791   0.566  -2.255 0.4250500   2.225
## 348  1996   2.225  -0.122   0.954   3.791   0.566 0.4201380  -0.114
## 349  1996  -0.114   2.225  -0.122   0.954   3.791 0.4030540   1.450
## 350  1996   1.450  -0.114   2.225  -0.122   0.954 0.4347920  -1.393
## 351  1996  -1.393   1.450  -0.114   2.225  -0.122 0.4108400   0.407
## 352  1996   0.407  -1.393   1.450  -0.114   2.225 0.4427260   3.844
## 353  1996   3.844   0.407  -1.393   1.450  -0.114 0.4599800   0.930
## 354  1996   0.930   3.844   0.407  -1.393   1.450 0.4529980   1.506
## 355  1996   1.506   0.930   3.844   0.407  -1.393 0.4676080   1.107
## 356  1996   1.107   1.506   0.930   3.844   0.407 0.3488525  -2.301
## 357  1996  -2.301   1.107   1.506   0.930   3.844 0.4822980  -1.482
## 358  1996  -1.482  -2.301   1.107   1.506   0.930 0.4546720   2.776
## 359  1996   2.776  -1.482  -2.301   1.107   1.506 0.5297280   1.058
## 360  1996   1.058   2.776  -1.482  -2.301   1.107 0.2542150  -1.158
## 361  1997  -1.158   1.058   2.776  -1.482  -2.301 0.4137550   1.533
## 362  1997   1.533  -1.158   1.058   2.776  -1.482 0.5456600   2.195
## 363  1997   2.195   1.533  -1.158   1.058   2.776 0.5147840  -0.728
## 364  1997  -0.728   2.195   1.533  -1.158   1.058 0.5657940   2.030
## 365  1997   2.030  -0.728   2.195   1.533  -1.158 0.5176880   0.432
## 366  1997   0.432   2.030  -0.728   2.195   1.533 0.5222440   2.396
## 367  1997   2.396   0.432   2.030  -0.728   2.195 0.5207640  -0.830
## 368  1997  -0.830   2.396   0.432   2.030  -0.728 0.4910325  -1.366
## 369  1997  -1.366  -0.830   2.396   0.432   2.030 0.5073520   1.789
## 370  1997   1.789  -1.366  -0.830   2.396   0.432 0.5112380  -1.466
## 371  1997  -1.466   1.789  -1.366  -0.830   2.396 0.4902660  -1.144
## 372  1997  -1.144  -1.466   1.789  -1.366  -0.830 0.5268820  -1.303
## 373  1997  -1.303  -1.144  -1.466   1.789  -1.366 0.4807375  -2.065
## 374  1997  -2.065  -1.303  -1.144  -1.466   1.789 0.5184900  -2.672
## 375  1997  -2.672  -2.065  -1.303  -1.144  -1.466 0.4444500   3.889
## 376  1997   3.889  -2.672  -2.065  -1.303  -1.144 0.4771380  -0.127
## 377  1997  -0.127   3.889  -2.672  -2.065  -1.303 0.4604280   6.219
## 378  1997   6.219  -0.127   3.889  -2.672  -2.065 0.4936760   1.453
## 379  1997   1.453   6.219  -0.127   3.889  -2.672 0.5286960   0.603
## 380  1997   0.603   1.453   6.219  -0.127   3.889 0.4798080   2.083
## 381  1997   2.083   0.603   1.453   6.219  -0.127 0.4361380   0.148
## 382  1997   0.148   2.083   0.603   1.453   6.219 0.4808225   1.147
## 383  1997   1.147   0.148   2.083   0.603   1.453 0.4742620   4.110
## 384  1997   4.110   1.147   0.148   2.083   0.603 0.5350140   0.608
## 385  1997   0.608   4.110   1.147   0.148   2.083 0.5278160  -1.268
## 386  1997  -1.268   0.608   4.110   1.147   0.148 0.5221900   3.338
## 387  1997   3.338  -1.268   0.608   4.110   1.147 0.5018450  -0.026
## 388  1997  -0.026   3.338  -1.268   0.608   4.110 0.5370580  -0.151
## 389  1997  -0.151  -0.026   3.338  -1.268   0.608 0.5901360   2.566
## 390  1997   2.566  -0.151  -0.026   3.338  -1.268 0.5497100   0.889
## 391  1997   0.889   2.566  -0.151  -0.026   3.338 0.5283020  -1.436
## 392  1997  -1.436   0.889   2.566  -0.151  -0.026 0.5372720  -3.506
## 393  1997  -3.506  -1.436   0.889   2.566  -0.151 0.5270280   2.523
## 394  1997   2.523  -3.506  -1.436   0.889   2.566 0.5080780  -2.606
## 395  1997  -2.606   2.523  -3.506  -1.436   0.889 0.4460920   3.289
## 396  1997   3.289  -2.606   2.523  -3.506  -1.436 0.5341600  -0.553
## 397  1997  -0.553   3.289  -2.606   2.523  -3.506 0.5210840   2.879
## 398  1997   2.879  -0.553   3.289  -2.606   2.523 0.5785660  -0.557
## 399  1997  -0.557   2.879  -0.553   3.289  -2.606 0.5367020   2.096
## 400  1997   2.096  -0.557   2.879  -0.553   3.289 0.5522780   0.202
## 401  1997   0.202   2.096  -0.557   2.879  -0.553 0.5346440  -2.360
## 402  1997  -2.360   0.202   2.096  -0.557   2.879 0.5184860  -0.267
## 403  1997  -0.267  -2.360   0.202   2.096  -0.557 0.6061160  -2.869
## 404  1997  -2.869  -0.267  -2.360   0.202   2.096 0.8048480   1.409
## 405  1997   1.409  -2.869  -0.267  -2.360   0.202 0.5529760   0.091
## 406  1997   0.091   1.409  -2.869  -0.267  -2.360 0.5549720   3.742
## 407  1997   3.742   0.091   1.409  -2.869  -0.267 0.5708500  -0.798
## 408  1997  -0.798   3.742   0.091   1.409  -2.869 0.4449075   2.972
## 409  1997   2.972  -0.798   3.742   0.091   1.409 0.5976180  -3.090
## 410  1997  -3.090   2.972  -0.798   3.742   0.091 0.5685580  -0.693
## 411  1997  -0.693  -3.090   2.972  -0.798   3.742 0.6502880  -1.090
## 412  1997  -1.090  -0.693  -3.090   2.972  -0.798 0.3666550   4.120
## 413  1998   4.120  -1.090  -0.693  -3.090   2.972 0.4441675  -4.856
## 414  1998  -4.856   4.120  -1.090  -0.693  -3.090 0.6624760   3.646
## 415  1998   3.646  -4.856   4.120  -1.090  -0.693 0.6389200  -0.408
## 416  1998  -0.408   3.646  -4.856   4.120  -1.090 0.6383225   2.369
## 417  1998   2.369  -0.408   3.646  -4.856   4.120 0.6613660   3.283
## 418  1998   3.283   2.369  -0.408   3.646  -4.856 0.6770980   0.754
## 419  1998   0.754   3.283   2.369  -0.408   3.646 0.5820660   1.384
## 420  1998   1.384   0.754   3.283   2.369  -0.408 0.5970025   1.463
## 421  1998   1.463   1.384   0.754   3.283   2.369 0.5945440   0.605
## 422  1998   0.605   1.463   1.384   0.754   3.283 0.6323760   1.224
## 423  1998   1.224   0.605   1.463   1.384   0.754 0.6209240   2.859
## 424  1998   2.859   1.224   0.605   1.463   1.384 0.6356360  -0.338
## 425  1998  -0.338   2.859   1.224   0.605   1.463 0.6205160   2.488
## 426  1998   2.488  -0.338   2.859   1.224   0.605 0.6355720  -1.072
## 427  1998  -1.072   2.488  -0.338   2.859   1.224 0.6154600   1.085
## 428  1998   1.085  -1.072   2.488  -0.338   2.859 0.6470180  -1.320
## 429  1998  -1.320   1.085  -1.072   2.488  -0.338 0.6509300   1.182
## 430  1998   1.182  -1.320   1.085  -1.072   2.488 0.6561840  -1.147
## 431  1998  -1.147   1.182  -1.320   1.085  -1.072 0.5784000   0.053
## 432  1998   0.053  -1.147   1.182  -1.320   1.085 0.5931280   0.157
## 433  1998   0.157   0.053  -1.147   1.182  -1.320 0.5336800  -1.770
## 434  1998  -1.770   0.157   0.053  -1.147   1.182 0.5922825   2.112
## 435  1998   2.112  -1.770   0.157   0.053  -1.147 0.5697960  -1.348
## 436  1998  -1.348   2.112  -1.770   0.157   0.053 0.5954360   0.165
## 437  1998   0.165  -1.348   2.112  -1.770   0.157 0.6621520   2.957
## 438  1998   2.957   0.165  -1.348   2.112  -1.770 0.6187000   1.167
## 439  1998   1.167   2.957   0.165  -1.348   2.112 0.6333400   1.562
## 440  1998   1.562   1.167   2.957   0.165  -1.348 0.5973100   1.926
## 441  1998   1.926   1.562   1.167   2.957   0.165 0.6589820  -3.872
## 442  1998  -3.872   1.926   1.562   1.167   2.957 0.6800560  -1.765
## 443  1998  -1.765  -3.872   1.926   1.562   1.167 0.6602500  -2.786
## 444  1998  -2.786  -1.765  -3.872   1.926   1.562 0.7704200  -2.451
## 445  1998  -2.451  -2.786  -1.765  -3.872   1.926 0.6740020   1.740
## 446  1998   1.740  -2.451  -2.786  -1.765  -3.872 0.6511880  -5.004
## 447  1998  -5.004   1.740  -2.451  -2.786  -1.765 0.7352000  -5.184
## 448  1998  -5.184  -5.004   1.740  -2.451  -2.786 0.9379000   3.611
## 449  1998   3.611  -5.184  -5.004   1.740  -2.451 0.8046250   1.093
## 450  1998   1.093   3.611  -5.184  -5.004   1.740 0.7451400   2.417
## 451  1998   2.417   1.093   3.611  -5.184  -5.004 0.7494360  -4.034
## 452  1998  -4.034   2.417   1.093   3.611  -5.184 0.8106600  -1.816
## 453  1998  -1.816  -4.034   2.417   1.093   3.611 0.9265800   7.317
## 454  1998   7.317  -1.816  -4.034   2.417   1.093 0.8390800   1.349
## 455  1998   1.349   7.317  -1.816  -4.034   2.417 0.7668880   2.615
## 456  1998   2.615   1.349   7.317  -1.816  -4.034 0.7072620   3.854
## 457  1998   3.854   2.615   1.349   7.317  -1.816 0.7545000  -1.340
## 458  1998  -1.340   3.854   2.615   1.349   7.317 0.6489120   3.361
## 459  1998   3.361  -1.340   3.854   2.615   1.349 0.6730980   2.473
## 460  1998   2.473   3.361  -1.340   3.854   2.615 0.5952075  -1.308
## 461  1998  -1.308   2.473   3.361  -1.340   3.854 0.7426600  -0.874
## 462  1998  -0.874  -1.308   2.473   3.361  -1.340 0.7061200   1.849
## 463  1998   1.849  -0.874  -1.308   2.473   3.361 0.7648400   3.219
## 464  1998   3.219   1.849  -0.874  -1.308   2.473 0.5924450   0.241
## 465  1999   0.241   3.219   1.849  -0.874  -1.308 0.6078675   3.731
## 466  1999   3.731   0.241   3.219   1.849  -0.874 0.8879400  -2.496
## 467  1999  -2.496   3.731   0.241   3.219   1.849 0.8290000  -1.453
## 468  1999  -1.453  -2.496   3.731   0.241   3.219 0.8372250   4.444
## 469  1999   4.444  -1.453  -2.496   3.731   0.241 0.8559800  -3.145
## 470  1999  -3.145   4.444  -1.453  -2.496   3.731 0.8495600  -0.748
## 471  1999  -0.748  -3.145   4.444  -1.453  -2.496 0.7340200   0.739
## 472  1999   0.739  -0.748  -3.145   4.444  -1.453 0.7078150  -0.072
## 473  1999  -0.072   0.739  -0.748  -3.145   4.444 0.7613400   2.999
## 474  1999   2.999  -0.072   0.739  -0.748  -3.145 0.7621200   1.499
## 475  1999   1.499   2.999  -0.072   0.739  -0.748 0.8181600   0.363
## 476  1999   0.363   1.499   2.999  -0.072   0.739 0.7954200  -1.269
## 477  1999  -1.269   0.363   1.499   2.999  -0.072 0.7445600   0.851
## 478  1999   0.851  -1.269   0.363   1.499   2.999 0.7760500   4.223
## 479  1999   4.223   0.851  -1.269   0.363   1.499 0.7732600  -2.177
## 480  1999  -2.177   4.223   0.851  -1.269   0.363 0.9331600   2.870
## 481  1999   2.870  -2.177   4.223   0.851  -1.269 0.9585200  -1.597
## 482  1999  -1.597   2.870  -2.177   4.223   0.851 0.8991000   0.735
## 483  1999   0.735  -1.597   2.870  -2.177   4.223 0.8696600  -0.535
## 484  1999  -0.535   0.735  -1.597   2.870  -2.177 0.7919200  -0.561
## 485  1999  -0.561  -0.535   0.735  -1.597   2.870 0.7317600  -2.139
## 486  1999  -2.139  -0.561  -0.535   0.735  -1.597 0.7827120   1.990
## 487  1999   1.990  -2.139  -0.561  -0.535   0.735 0.7064750  -2.569
## 488  1999  -2.569   1.990  -2.139  -0.561  -0.535 0.6853800   3.803
## 489  1999   3.803  -2.569   1.990  -2.139  -0.561 0.7575200  -2.050
## 490  1999  -2.050   3.803  -2.569   1.990  -2.139 0.6897520   5.771
## 491  1999   5.771  -2.050   3.803  -2.569   1.990 0.8093960   0.867
## 492  1999   0.867   5.771  -2.050   3.803  -2.569 0.7614250   1.105
## 493  1999   1.105   0.867   5.771  -2.050   3.803 0.7420600  -4.359
## 494  1999  -4.359   1.105   0.867   5.771  -2.050 0.7169820  -2.080
## 495  1999  -2.080  -4.359   1.105   0.867   5.771 0.7070100  -2.140
## 496  1999  -2.140  -2.080  -4.359   1.105   0.867 0.7473300   2.106
## 497  1999   2.106  -2.140  -2.080  -4.359   1.105 0.7500200   0.673
## 498  1999   0.673   2.106  -2.140  -2.080  -4.359 0.6606500   0.872
## 499  1999   0.872   0.673   2.106  -2.140  -2.080 0.7137900   0.665
## 500  1999   0.665   0.872   0.673   2.106  -2.140 0.7036200  -0.411
## 501  1999  -0.411   0.665   0.872   0.673   2.106 0.7722250  -1.201
## 502  1999  -1.201  -0.411   0.665   0.872   0.673 0.7561200  -4.348
## 503  1999  -4.348  -1.201  -0.411   0.665   0.872 0.7942200   0.427
## 504  1999   0.427  -4.348  -1.201  -0.411   0.665 0.8871600   4.148
## 505  1999   4.148   0.427  -4.348  -1.201  -0.411 0.8778600  -6.632
## 506  1999  -6.632   4.148   0.427  -4.348  -1.201 0.8121200   4.348
## 507  1999   4.348  -6.632   4.148   0.427  -4.348 0.9249800   4.708
## 508  1999   4.708   4.348  -6.632   4.148   0.427 0.9722000   0.536
## 509  1999   0.536   4.708   4.348  -6.632   4.148 0.9337800   1.885
## 510  1999   1.885   0.536   4.708   4.348  -6.632 0.8874600   1.858
## 511  1999   1.858   1.885   0.536   4.708   4.348 0.9229000  -0.378
## 512  1999  -0.378   1.858   1.885   0.536   4.708 0.7116300   1.177
## 513  1999   1.177  -0.378   1.858   1.885   0.536 0.9217400  -1.134
## 514  1999  -1.134   1.177  -0.378   1.858   1.885 1.0137800   0.282
## 515  1999   0.282  -1.134   1.177  -0.378   1.858 1.0918800   2.626
## 516  1999   2.626   0.282  -1.134   1.177  -0.378 0.8616750   0.748
## 517  2000   0.748   2.626   0.282  -1.134   1.177 0.5749180  -1.891
## 518  2000  -1.891   0.748   2.626   0.282  -1.134 1.0687600   1.643
## 519  2000   1.643  -1.891   0.748   2.626   0.282 1.0339400  -1.624
## 520  2000  -1.624   1.643  -1.891   0.748   2.626 1.1137500  -5.634
## 521  2000  -5.634  -1.624   1.643  -1.891   0.748 1.1064200   4.721
## 522  2000   4.721  -5.634  -1.624   1.643  -1.891 1.0410000  -2.615
## 523  2000  -2.615   4.721  -5.634  -1.624   1.643 1.0201600  -2.958
## 524  2000  -2.958  -2.615   4.721  -5.634  -1.624 1.0230600  -0.946
## 525  2000  -0.946  -2.958  -2.615   4.721  -5.634 1.0634750   5.686
## 526  2000   5.686  -0.946  -2.958  -2.615   4.721 1.1707600  -1.001
## 527  2000  -1.001   5.686  -0.946  -2.958  -2.615 1.1615800   4.975
## 528  2000   4.975  -1.001   5.686  -0.946  -2.958 1.2380600   4.301
## 529  2000   4.301   4.975  -1.001   5.686  -0.946 1.0384400  -1.891
## 530  2000  -1.891   4.301   4.975  -1.001   5.686 1.0685600   1.186
## 531  2000   1.186  -1.891   4.301   4.975  -1.001 1.1094120 -10.538
## 532  2000 -10.538   1.186  -1.891   4.301   4.975 1.0625400   5.748
## 533  2000   5.748 -10.538   1.186  -1.891   4.301 1.0529250   1.247
## 534  2000   1.247   5.748 -10.538   1.186  -1.891 1.0070000  -1.363
## 535  2000  -1.363   1.247   5.748 -10.538   1.186 0.9401400  -0.815
## 536  2000  -0.815  -1.363   1.247   5.748 -10.538 0.9004800  -0.986
## 537  2000  -0.986  -0.815  -1.363   1.247   5.748 0.8584400  -2.056
## 538  2000  -2.056  -0.986  -0.815  -1.363   1.247 0.9196600   7.202
## 539  2000   7.202  -2.056  -0.986  -0.815  -1.363 0.9818000  -1.375
## 540  2000  -1.375   7.202  -2.056  -0.986  -0.815 0.8567200   0.515
## 541  2000   0.515  -1.375   7.202  -2.056  -0.986 0.9803800  -1.569
## 542  2000  -1.569   0.515  -1.375   7.202  -2.056 0.9666200   0.910
## 543  2000   0.910  -1.569   0.515  -1.375   7.202 1.1194400   1.671
## 544  2000   1.671   0.910  -1.569   0.515  -1.375 0.8375500   2.102
## 545  2000   2.102   1.671   0.910  -1.569   0.515 0.9615600  -1.973
## 546  2000  -1.973   2.102   1.671   0.910  -1.569 0.9513200  -4.074
## 547  2000  -4.074  -1.973   2.102   1.671   0.910 1.0443800   3.031
## 548  2000   3.031  -4.074  -1.973   2.102   1.671 0.9874800   0.609
## 549  2000   0.609   3.031  -4.074  -1.973   2.102 0.9354600   1.351
## 550  2000   1.351   0.609   3.031  -4.074  -1.973 0.8706600   0.987
## 551  2000   0.987   1.351   0.609   3.031  -4.074 0.7888200   0.951
## 552  2000   0.951   0.987   1.351   0.609   3.031 0.8343800  -1.727
## 553  2000  -1.727   0.951   0.987   1.351   0.609 0.9450250  -1.920
## 554  2000  -1.920  -1.727   0.951   0.987   1.351 1.0482400  -1.166
## 555  2000  -1.166  -1.920  -1.727   0.951   0.987 1.0764600  -0.843
## 556  2000  -0.843  -1.166  -1.920  -1.727   0.951 1.1334000  -1.916
## 557  2000  -1.916  -0.843  -1.166  -1.920  -1.727 1.1285800  -2.471
## 558  2000  -2.471  -1.916  -0.843  -1.166  -1.920 1.1521200   1.656
## 559  2000   1.656  -2.471  -1.916  -0.843  -1.166 1.2167800  -1.242
## 560  2000  -1.242   1.656  -2.471  -1.916  -0.843 1.1822200   3.415
## 561  2000   3.415  -1.242   1.656  -2.471  -1.916 1.1850200  -4.255
## 562  2000  -4.255   3.415  -1.242   1.656  -2.471 0.9589200   0.127
## 563  2000   0.127  -4.255   3.415  -1.242   1.656 1.0683200  -1.897
## 564  2000  -1.897   0.127  -4.255   3.415  -1.242 0.8652425  -1.978
## 565  2000  -1.978  -1.897   0.127  -4.255   3.415 0.9516260   4.156
## 566  2000   4.156  -1.978  -1.897   0.127  -4.255 1.1777800  -4.215
## 567  2000  -4.215   4.156  -1.978  -1.897   0.127 1.2206600  -0.473
## 568  2000  -0.473  -4.215   4.156  -1.978  -1.897 1.2946800   1.097
## 569  2001   1.097  -0.473  -4.215   4.156  -1.978 0.9875000  -1.661
## 570  2001  -1.661   1.097  -0.473  -4.215   4.156 1.6429750   1.556
## 571  2001   1.556  -1.661   1.097  -0.473  -4.215 1.2581000   1.819
## 572  2001   1.819   1.556  -1.661   1.097  -0.473 1.3519000   0.924
## 573  2001   0.924   1.819   1.556  -1.661   1.097 1.2123200  -0.404
## 574  2001  -0.404   0.924   1.819   1.556  -1.661 1.1330800  -2.572
## 575  2001  -2.572  -0.404   0.924   1.819   1.556 1.0827200  -1.006
## 576  2001  -1.006  -2.572  -0.404   0.924   1.819 1.1351000  -4.277
## 577  2001  -4.277  -1.006  -2.572  -0.404   0.924 1.2294750  -0.938
## 578  2001  -0.938  -4.277  -1.006  -2.572  -0.404 1.2118200  -0.062
## 579  2001  -0.062  -0.938  -4.277  -1.006  -2.572 1.0706400  -6.720
## 580  2001  -6.720  -0.062  -0.938  -4.277  -1.006 1.3580720  -0.930
## 581  2001  -0.930  -6.720  -0.062  -0.938  -4.277 1.3594500   1.799
## 582  2001   1.799  -0.930  -6.720  -0.062  -0.938 1.2553800  -2.749
## 583  2001  -2.749   1.799  -0.930  -6.720  -0.062 1.3402780   4.880
## 584  2001   4.880  -2.749   1.799  -0.930  -6.720 1.2011750   5.026
## 585  2001   5.026   4.880  -2.749   1.799  -0.930 1.3535800   0.810
## 586  2001   0.810   5.026   4.880  -2.749   1.799 1.1738400   1.082
## 587  2001   1.082   0.810   5.026   4.880  -2.749 1.2020600  -1.653
## 588  2001  -1.653   1.082   0.810   5.026   4.880 1.0101200   3.716
## 589  2001   3.716  -1.653   1.082   0.810   5.026 1.1643400  -1.089
## 590  2001  -1.089   3.716  -1.653   1.082   0.810 1.0997800  -1.348
## 591  2001  -1.348  -1.089   3.716  -1.653   1.082 1.1065500   0.340
## 592  2001   0.340  -1.348  -1.089   3.716  -1.653 0.9662000  -4.000
## 593  2001  -4.000   0.340  -1.348  -1.089   3.716 1.1897300   0.905
## 594  2001   0.905  -4.000   0.340  -1.348  -1.089 1.2765240  -0.079
## 595  2001  -0.079   0.905  -4.000   0.340  -1.348 1.3141520  -2.760
## 596  2001  -2.760  -0.079   0.905  -4.000   0.340 0.9355025   2.107
## 597  2001   2.107  -2.760  -0.079   0.905  -4.000 1.2418600  -0.397
## 598  2001  -0.397   2.107  -2.760  -0.079   0.905 1.2217200  -0.415
## 599  2001  -0.415  -0.397   2.107  -2.760  -0.079 1.1391000   0.707
## 600  2001   0.707  -0.415  -0.397   2.107  -2.760 1.1073600  -1.992
## 601  2001  -1.992   0.707  -0.415  -0.397   2.107 1.0026800  -2.369
## 602  2001  -2.369  -1.992   0.707  -0.415  -0.397 0.9795000   1.976
## 603  2001   1.976  -2.369  -1.992   0.707  -0.415 1.0158600  -4.334
## 604  2001  -4.334   1.976  -2.369  -1.992   0.707 0.9741000  -4.217
## 605  2001  -4.217  -4.334   1.976  -2.369  -1.992 1.3367000 -11.050
## 606  2001 -11.050  -4.217  -4.334   1.976  -2.369 1.9500816   7.780
## 607  2001   7.780 -11.050  -4.217  -4.334   1.976 1.5956000   2.924
## 608  2001   2.924   7.780 -11.050  -4.217  -4.334 1.4053600   1.892
## 609  2001   1.892   2.924   7.780 -11.050  -4.217 1.3110360  -1.664
## 610  2001  -1.664   1.892   2.924   7.780 -11.050 1.2490400   2.900
## 611  2001   2.900  -1.664   1.892   2.924   7.780 1.2736200  -1.576
## 612  2001  -1.576   2.900  -1.664   1.892   2.924 1.2390600   3.045
## 613  2001   3.045  -1.576   2.900  -1.664   1.892 1.3292600   1.637
## 614  2001   1.637   3.045  -1.576   2.900  -1.664 1.3194000   1.027
## 615  2001   1.027   1.637   3.045  -1.576   2.900 1.0216500  -0.947
## 616  2001  -0.947   1.027   1.637   3.045  -1.576 1.3121600   1.655
## 617  2001   1.655  -0.947   1.027   1.637   3.045 1.4045600  -3.041
## 618  2001  -3.041   1.655  -0.947   1.027   1.637 1.3707800   1.941
## 619  2001   1.941  -3.041   1.655  -0.947   1.027 1.4567600   1.409
## 620  2001   1.409   1.941  -3.041   1.655  -0.947 0.7561175   0.990
## 621  2002   0.990   1.409   1.941  -3.041   1.655 1.2566250  -2.295
## 622  2002  -2.295   0.990   1.409   1.941  -3.041 1.3060000  -1.573
## 623  2002  -1.573  -2.295   0.990   1.409   1.941 1.3738400   0.506
## 624  2002   0.506  -1.573  -2.295   0.990   1.409 1.4221750  -0.978
## 625  2002  -0.978   0.506  -1.573  -2.295   0.990 1.5885200  -2.315
## 626  2002  -2.315  -0.978   0.506  -1.573  -2.295 1.5390400   0.726
## 627  2002   0.726  -2.315  -0.978   0.506  -1.573 1.2202400  -1.299
## 628  2002  -1.299   0.726  -2.315  -0.978   0.506 1.3553500   3.848
## 629  2002   3.848  -1.299   0.726  -2.315  -0.978 1.3838200   2.874
## 630  2002   2.874   3.848  -1.299   0.726  -2.315 1.5228600   0.159
## 631  2002   0.159   2.874   3.848  -1.299   0.726 1.3142600  -1.497
## 632  2002  -1.497   0.159   2.874   3.848  -1.299 1.2623800  -0.114
## 633  2002  -0.114  -1.497   0.159   2.874   3.848 1.1523000  -2.149
## 634  2002  -2.149  -0.114  -1.497   0.159   2.874 1.1682600  -1.044
## 635  2002  -1.044  -2.149  -0.114  -1.497   0.159 1.3132600   1.275
## 636  2002   1.275  -1.044  -2.149  -0.114  -1.497 1.2765800  -4.342
## 637  2002  -4.342   1.275  -1.044  -2.149  -0.114 1.3670200  -0.269
## 638  2002  -0.269  -4.342   1.275  -1.044  -2.149 1.4086400  -1.718
## 639  2002  -1.718  -0.269  -4.342   1.275  -1.044 1.2608400   4.891
## 640  2002   4.891  -1.718  -0.269  -4.342   1.275 1.2908600  -2.058
## 641  2002  -2.058   4.891  -1.718  -0.269  -4.342 1.0809800  -1.539
## 642  2002  -1.539  -2.058   4.891  -1.718  -0.269 1.1605500  -3.712
## 643  2002  -3.712  -1.539  -2.058   4.891  -1.718 1.4067600  -1.972
## 644  2002  -1.972  -3.712  -1.539  -2.058   4.891 1.4377640  -1.800
## 645  2002  -1.800  -1.972  -3.712  -1.539  -2.058 1.3305400   0.069
## 646  2002   0.069  -1.800  -1.972  -3.712  -1.539 1.8212380  -0.080
## 647  2002  -0.080   0.069  -1.800  -1.972  -3.712 1.3689250  -6.839
## 648  2002  -6.839  -0.080   0.069  -1.800  -1.972 1.6076160  -7.992
## 649  2002  -7.992  -6.839  -0.080   0.069  -1.800 2.2750800   0.600
## 650  2002   0.600  -7.992  -6.839  -0.080   0.069 2.3370880   1.337
## 651  2002   1.337   0.600  -7.992  -6.839  -0.080 1.7728800   5.137
## 652  2002   5.137   1.337   0.600  -7.992  -6.839 1.4743200   2.215
## 653  2002   2.215   5.137   1.337   0.600  -7.992 1.3276800   1.302
## 654  2002   1.302   2.215   5.137   1.337   0.600 1.2811800  -2.635
## 655  2002  -2.635   1.302   2.215   5.137   1.337 1.1344400  -2.418
## 656  2002  -2.418  -2.635   1.302   2.215   5.137 1.3119250  -0.460
## 657  2002  -0.460  -2.418  -2.635   1.302   2.215 1.1252400  -4.992
## 658  2002  -4.992  -0.460  -2.418  -2.635   1.302 1.4535600  -2.132
## 659  2002  -2.132  -4.992  -0.460  -2.418  -2.635 1.5720280  -3.238
## 660  2002  -3.238  -2.132  -4.992  -0.460  -2.418 1.7364200   4.339
## 661  2002   4.339  -3.238  -2.132  -4.992  -0.460 1.8688640   5.874
## 662  2002   5.874   4.339  -3.238  -2.132  -4.992 1.5889580   1.499
## 663  2002   1.499   5.874   4.339  -3.238  -2.132 1.5262140   0.369
## 664  2002   0.369   1.499   5.874   4.339  -3.238 1.4852600  -0.690
## 665  2002  -0.690   0.369   1.499   5.874   4.339 1.5174800   1.687
## 666  2002   1.687  -0.690   0.369   1.499   5.874 1.3745200   2.277
## 667  2002   2.277   1.687  -0.690   0.369   1.499 1.6358400   0.619
## 668  2002   0.619   2.277   1.687  -0.690   0.369 1.2778400  -2.572
## 669  2002  -2.572   0.619   2.277   1.687  -0.690 1.4361200  -2.494
## 670  2002  -2.494  -2.572   0.619   2.277   1.687 1.2957200   0.706
## 671  2002   0.706  -2.494  -2.572   0.619   2.277 1.4276460  -2.273
## 672  2002  -2.273   0.706  -2.494  -2.572   0.619 0.7624775   3.791
## 673  2003   3.791  -2.273   0.706  -2.494  -2.572 1.1265750   2.089
## 674  2003   2.089   3.791  -2.273   0.706  -2.494 1.4988800  -2.780
## 675  2003  -2.780   2.089   3.791  -2.273   0.706 1.4201200  -4.478
## 676  2003  -4.478  -2.780   2.089   3.791  -2.273 1.5538375  -0.662
## 677  2003  -0.662  -4.478  -2.780   2.089   3.791 1.5158460  -3.040
## 678  2003  -3.040  -0.662  -4.478  -2.780   2.089 1.3737200   0.627
## 679  2003   0.627  -3.040  -0.662  -4.478  -2.780 1.3399200   1.591
## 680  2003   1.591   0.627  -3.040  -0.662  -4.478 1.2296750  -0.828
## 681  2003  -0.828   1.591   0.627  -3.040  -0.662 1.3496800  -1.458
## 682  2003  -1.458  -0.828   1.591   0.627  -3.040 1.2931800   0.528
## 683  2003   0.528  -1.458  -0.828   1.591   0.627 1.5321800   7.503
## 684  2003   7.503   0.528  -1.458  -0.828   1.591 1.6103460  -3.605
## 685  2003  -3.605   7.503   0.528  -1.458  -0.828 1.2812000   1.778
## 686  2003   1.778  -3.605   7.503   0.528  -1.458 1.4255200  -1.200
## 687  2003  -1.200   1.778  -3.605   7.503   0.528 1.2880000   2.911
## 688  2003   2.911  -1.200   1.778  -3.605   7.503 1.4023500   0.585
## 689  2003   0.585   2.911  -1.200   1.778  -3.605 1.4802000   3.479
## 690  2003   3.479   0.585   2.911  -1.200   1.778 1.5077820   0.358
## 691  2003   0.358   3.479   0.585   2.911  -1.200 1.4667000   1.167
## 692  2003   1.167   0.358   3.479   0.585   2.911 1.4425800  -1.173
## 693  2003  -1.173   1.167   0.358   3.479   0.585 1.3976600   3.254
## 694  2003   3.254  -1.173   1.167   0.358   3.479 1.6164000   2.508
## 695  2003   2.508   3.254  -1.173   1.167   0.358 1.6523400   0.086
## 696  2003   0.086   2.508   3.254  -1.173   1.167 1.3854200   0.716
## 697  2003   0.716   0.086   2.508   3.254  -1.173 1.5085200  -1.955
## 698  2003  -1.955   0.716   0.086   2.508   3.254 1.3801600   0.971
## 699  2003   0.971  -1.955   0.716   0.086   2.508 1.3356500   1.262
## 700  2003   1.262   0.971  -1.955   0.716   0.086 1.4582400  -0.483
## 701  2003  -0.483   1.262   0.971  -1.955   0.716 1.5312200   0.540
## 702  2003   0.540  -0.483   1.262   0.971  -1.955 1.4026200  -1.855
## 703  2003  -1.855   0.540  -0.483   1.262   0.971 1.4456000  -0.261
## 704  2003  -0.261  -1.855   0.540  -0.483   1.262 1.3274600   1.338
## 705  2003   1.338  -0.261  -1.855   0.540  -0.483 1.0372940   0.241
## 706  2003   0.241   1.338  -0.261  -1.855   0.540 1.2710000   1.505
## 707  2003   1.505   0.241   1.338  -0.261  -1.855 1.0624200   1.327
## 708  2003   1.327   1.505   0.241   1.338  -0.261 1.5163000  -0.270
## 709  2003  -0.270   1.327   1.505   0.241   1.338 1.3737600   1.735
## 710  2003   1.735  -0.270   1.327   1.505   0.241 1.3820220  -3.807
## 711  2003  -3.807   1.735  -0.270   1.327   1.505 1.4278000   3.310
## 712  2003   3.310  -3.807   1.735  -0.270   1.327 1.4726200   0.797
## 713  2003   0.797   3.310  -3.807   1.735  -0.270 1.2509200   0.121
## 714  2003   0.121   0.797   3.310  -3.807   1.735 1.3206400  -1.002
## 715  2003  -1.002   0.121   0.797   3.310  -3.807 1.4684800   2.119
## 716  2003   2.119  -1.002   0.121   0.797   3.310 1.5384400   0.238
## 717  2003   0.238   2.119  -1.002   0.121   0.797 1.4184000  -0.272
## 718  2003  -0.272   0.238   2.119  -1.002   0.121 1.2989000  -1.435
## 719  2003  -1.435  -0.272   0.238   2.119  -1.002 1.3310600   2.214
## 720  2003   2.214  -1.435  -0.272   0.238   2.119 1.0553550   0.312
## 721  2003   0.312   2.214  -1.435  -0.272   0.238 1.3857800   1.191
## 722  2003   1.191   0.312   2.214  -1.435  -0.272 1.3585200   1.352
## 723  2003   1.352   1.191   0.312   2.214  -1.435 1.5495200   0.664
## 724  2003   0.664   1.352   1.191   0.312   2.214 0.8177825   1.149
## 725  2004   1.149   0.664   1.352   1.191   0.312 1.0630250   1.207
## 726  2004   1.207   1.149   0.664   1.352   1.191 1.6733400   1.602
## 727  2004   1.602   1.207   1.149   0.664   1.352 1.6073600   0.151
## 728  2004   0.151   1.602   1.207   1.149   0.664 1.6776750  -0.913
## 729  2004  -0.913   0.151   1.602   1.207   1.149 1.7105200   1.028
## 730  2004   1.028  -0.913   0.151   1.602   1.207 1.5510200   0.267
## 731  2004   0.267   1.028  -0.913   0.151   1.602 1.4400400  -0.148
## 732  2004  -0.148   0.267   1.028  -0.913   0.151 1.4553250   0.073
## 733  2004   0.073  -0.148   0.267   1.028  -0.913 1.4418000   1.041
## 734  2004   1.041   0.073  -0.148   0.267   1.028 1.3943200  -3.137
## 735  2004  -3.137   1.041   0.073  -0.148   0.267 1.5361200  -0.963
## 736  2004  -0.963  -3.137   1.041   0.073  -0.148 1.4836000  -0.155
## 737  2004  -0.155  -0.963  -3.137   1.041   0.073 1.4458200   3.046
## 738  2004   3.046  -0.155  -0.963  -3.137   1.041 1.4977000  -0.218
## 739  2004  -0.218   3.046  -0.155  -0.963  -3.137 1.3675000  -0.413
## 740  2004  -0.413  -0.218   3.046  -0.155  -0.963 1.4259600   0.528
## 741  2004   0.528  -0.413  -0.218   3.046  -0.155 1.5328600  -2.920
## 742  2004  -2.920   0.528  -0.413  -0.218   3.046 1.6315800  -0.777
## 743  2004  -0.777  -2.920   0.528  -0.413  -0.218 1.5731200  -0.273
## 744  2004  -0.273  -0.777  -2.920   0.528  -0.413 1.5793600  -0.195
## 745  2004  -0.195  -0.273  -0.777  -2.920   0.528 1.3602600   2.480
## 746  2004   2.480  -0.195  -0.273  -0.777  -2.920 1.3525400   0.162
## 747  2004   0.162   2.480  -0.195  -0.273  -0.777 1.2093500   1.245
## 748  2004   1.245   0.162   2.480  -0.195  -0.273 1.2098750  -0.128
## 749  2004  -0.128   1.245   0.162   2.480  -0.195 1.2982000  -0.052
## 750  2004  -0.052  -0.128   1.245   0.162   2.480 1.4316400  -0.798
## 751  2004  -0.798  -0.052  -0.128   1.245   0.162 1.3568200  -1.117
## 752  2004  -1.117  -0.798  -0.052  -0.128   1.245 1.2998250  -1.026
## 753  2004  -1.026  -1.117  -0.798  -0.052  -0.128 1.3270600  -1.379
## 754  2004  -1.379  -1.026  -1.117  -0.798  -0.052 1.4927000   1.429
## 755  2004   1.429  -1.379  -1.026  -1.117  -0.798 1.4813600  -3.426
## 756  2004  -3.426   1.429  -1.379  -1.026  -1.117 1.3803800   0.078
## 757  2004   0.078  -3.426   1.429  -1.379  -1.026 1.2644400   3.151
## 758  2004   3.151   0.078  -3.426   1.429  -1.379 1.2411600   0.858
## 759  2004   0.858   3.151   0.078  -3.426   1.429 1.0351200   0.529
## 760  2004   0.529   0.858   3.151   0.078  -3.426 1.0331940   0.924
## 761  2004   0.924   0.529   0.858   3.151   0.078 1.2733000   0.412
## 762  2004   0.412   0.924   0.529   0.858   3.151 1.2593600  -1.634
## 763  2004  -1.634   0.412   0.924   0.529   0.858 1.2888400   1.927
## 764  2004   1.927  -1.634   0.412   0.924   0.529 1.4786400  -0.827
## 765  2004  -0.827   1.927  -1.634   0.412   0.924 1.4216400  -1.242
## 766  2004  -1.242  -0.827   1.927  -1.634   0.412 1.3889400  -1.124
## 767  2004  -1.124  -1.242  -0.827   1.927  -1.634 1.5878200   3.145
## 768  2004   3.145  -1.124  -1.242  -0.827   1.927 1.5873600   3.183
## 769  2004   3.183   3.145  -1.124  -1.242  -0.827 1.6659000   1.544
## 770  2004   1.544   3.183   3.145  -1.124  -1.242 1.4476800  -1.168
## 771  2004  -1.168   1.544   3.183   3.145  -1.124 1.4970400   1.052
## 772  2004   1.052  -1.168   1.544   3.183   3.145 1.1187950   0.720
## 773  2004   0.720   1.052  -1.168   1.544   3.183 1.6092800  -0.266
## 774  2004  -0.266   0.720   1.052  -1.168   1.544 1.4963800   0.522
## 775  2004   0.522  -0.266   0.720   1.052  -1.168 1.7610400   1.334
## 776  2004   1.334   0.522  -0.266   0.720   1.052 1.3133500   0.148
## 777  2005   0.148   1.334   0.522  -0.266   0.720 0.8895200  -2.123
## 778  2005  -2.123   0.148   1.334   0.522  -0.266 1.6035400  -0.141
## 779  2005  -0.141  -2.123   0.148   1.334   0.522 1.4774000  -1.406
## 780  2005  -1.406  -0.141  -2.123   0.148   1.334 1.6077500   0.299
## 781  2005   0.299  -1.406  -0.141  -2.123   0.148 1.5966600   2.704
## 782  2005   2.704   0.299  -1.406  -0.141  -2.123 1.6252280   0.189
## 783  2005   0.189   2.704   0.299  -1.406  -0.141 1.4656900  -0.308
## 784  2005  -0.308   0.189   2.704   0.299  -1.406 1.4877360   0.814
## 785  2005   0.814  -0.308   0.189   2.704   0.299 1.5721150   0.887
## 786  2005   0.887   0.814  -0.308   0.189   2.704 1.6650280  -1.803
## 787  2005  -1.803   0.887   0.814  -0.308   0.189 1.5541460  -0.869
## 788  2005  -0.869  -1.803   0.887   0.814  -0.308 1.7060900  -1.532
## 789  2005  -1.532  -0.869  -1.803   0.887   0.814 1.9756250   0.128
## 790  2005   0.128  -1.532  -0.869  -1.803   0.887 2.0899000   0.706
## 791  2005   0.706   0.128  -1.532  -0.869  -1.803 1.8619840  -3.266
## 792  2005  -3.266   0.706   0.128  -1.532  -0.869 2.1199760   0.831
## 793  2005   0.831  -3.266   0.706   0.128  -1.532 2.1789720   0.411
## 794  2005   0.411   0.831  -3.266   0.706   0.128 2.0901840   1.253
## 795  2005   1.253   0.411   0.831  -3.266   0.706 2.0315680  -1.477
## 796  2005  -1.477   1.253   0.411   0.831  -3.266 1.9531060   3.053
## 797  2005   3.053  -1.477   1.253   0.411   0.831 1.8836100   0.799
## 798  2005   0.799   3.053  -1.477   1.253   0.411 1.6279780  -0.230
## 799  2005  -0.230   0.799   3.053  -1.477   1.253 1.7730225   0.175
## 800  2005   0.175  -0.230   0.799   3.053  -1.477 1.7204560   1.573
## 801  2005   1.573   0.175  -0.230   0.799   3.053 1.8766700  -2.086
## 802  2005  -2.086   1.573   0.175  -0.230   0.799 1.9414400   0.241
## 803  2005   0.241  -2.086   1.573   0.175  -0.230 1.7967240   1.458
## 804  2005   1.458   0.241  -2.086   1.573   0.175 1.8856350   1.325
## 805  2005   1.325   1.458   0.241  -2.086   1.573 1.8711840   0.469
## 806  2005   0.469   1.325   1.458   0.241  -2.086 1.9167100   0.041
## 807  2005   0.041   0.469   1.325   1.458   0.241 1.8777680  -0.629
## 808  2005  -0.629   0.041   0.469   1.325   1.458 1.9342940   0.324
## 809  2005   0.324  -0.629   0.041   0.469   1.325 1.9049680  -0.868
## 810  2005  -0.868   0.324  -0.629   0.041   0.469 1.7218800  -1.198
## 811  2005  -1.198  -0.868   0.324  -0.629   0.041 1.6685900   1.072
## 812  2005   1.072  -1.198  -0.868   0.324  -0.629 1.9502900   1.926
## 813  2005   1.926   1.072  -1.198  -0.868   0.324 1.9869325  -0.288
## 814  2005  -0.288   1.926   1.072  -1.198  -0.868 2.2477940  -1.827
## 815  2005  -1.827  -0.288   1.926   1.072  -1.198 2.2683360   1.112
## 816  2005   1.112  -1.827  -0.288   1.926   1.072 2.0758220  -2.678
## 817  2005  -2.678   1.112  -1.827  -0.288   1.926 2.3807600  -0.780
## 818  2005  -0.780  -2.678   1.112  -1.827  -0.288 2.3052800  -0.588
## 819  2005  -0.588  -0.780  -2.678   1.112  -1.827 2.4086680   1.595
## 820  2005   1.595  -0.588  -0.780  -2.678   1.112 2.3505560   1.813
## 821  2005   1.813   1.595  -0.588  -0.780  -2.678 2.4881100   1.195
## 822  2005   1.195   1.813   1.595  -0.588  -0.780 2.0637380   1.097
## 823  2005   1.097   1.195   1.813   1.595  -0.588 2.2264120   1.601
## 824  2005   1.601   1.097   1.195   1.813   1.595 1.7797775  -0.250
## 825  2005  -0.250   1.601   1.097   1.195   1.813 2.2800680  -0.451
## 826  2005  -0.451  -0.250   1.601   1.097   1.195 2.1210000   0.631
## 827  2005   0.631  -0.451  -0.250   1.601   1.097 2.2353740   0.106
## 828  2005   0.106   0.631  -0.451  -0.250   1.601 1.8889960  -1.606
## 829  2006  -1.606   0.106   0.631  -0.451  -0.250 1.4472175   2.977
## 830  2006   2.977  -1.606   0.106   0.631  -0.451 2.4874500   0.168
## 831  2006   0.168   2.977  -1.606   0.106   0.631 2.3211120  -2.029
## 832  2006  -2.029   0.168   2.977  -1.606   0.106 2.4257500   1.762
## 833  2006   1.762  -2.029   0.168   2.977  -1.606 2.5924500  -1.534
## 834  2006  -1.534   1.762  -2.029   0.168   2.977 2.4855920   0.234
## 835  2006   0.234  -1.534   1.762  -2.029   0.168 2.3375120   1.598
## 836  2006   1.598   0.234  -1.534   1.762  -2.029 2.1970720   0.170
## 837  2006   0.170   1.598   0.234  -1.534   1.762 2.1009800  -0.171
## 838  2006  -0.171   0.170   1.598   0.234  -1.534 2.2604080  -0.451
## 839  2006  -0.451  -0.171   0.170   1.598   0.234 2.2509340   2.016
## 840  2006   2.016  -0.451  -0.171   0.170   1.598 2.2740800  -0.329
## 841  2006  -0.329   2.016  -0.451  -0.171   0.170 2.0942040  -0.620
## 842  2006  -0.620  -0.329   2.016  -0.451  -0.171 2.1706180   0.049
## 843  2006   0.049  -0.620  -0.329   2.016  -0.451 2.2851820  -0.492
## 844  2006  -0.492   0.049  -0.620  -0.329   2.016 1.9903100   1.719
## 845  2006   1.719  -0.492   0.049  -0.620  -0.329 2.3485900  -0.051
## 846  2006  -0.051   1.719  -0.492   0.049  -0.620 2.4356660   1.156
## 847  2006   1.156  -0.051   1.719  -0.492   0.049 2.3923900  -2.604
## 848  2006  -2.604   1.156  -0.051   1.719  -0.492 2.3353260  -1.875
## 849  2006  -1.875  -2.604   1.156  -0.051   1.719 2.6483720   1.036
## 850  2006   1.036  -1.875  -2.604   1.156  -0.051 2.5128080   0.630
## 851  2006   0.630   1.036  -1.875  -2.604   1.156 2.3810125  -2.788
## 852  2006  -2.788   0.630   1.036  -1.875  -2.604 2.6826160  -0.061
## 853  2006  -0.061  -2.788   0.630   1.036  -1.875 2.7379280  -0.563
## 854  2006  -0.563  -0.061  -2.788   0.630   1.036 2.2553660   2.065
## 855  2006   2.065  -0.563  -0.061  -2.788   0.630 2.3676020  -0.372
## 856  2006  -0.372   2.065  -0.563  -0.061  -2.788 1.8192125  -2.314
## 857  2006  -2.314  -0.372   2.065  -0.563  -0.061 2.2857540   0.331
## 858  2006   0.331  -2.314  -0.372   2.065  -0.563 2.4759620   3.085
## 859  2006   3.085   0.331  -2.314  -0.372   2.065 2.5602980   0.063
## 860  2006   0.063   3.085   0.331  -2.314  -0.372 2.5718300  -0.986
## 861  2006  -0.986   0.063   3.085   0.331  -2.314 2.2930820   2.807
## 862  2006   2.807  -0.986   0.063   3.085   0.331 2.2997880  -0.554
## 863  2006  -0.554   2.807  -0.986   0.063   3.085 1.8319100   1.229
## 864  2006   1.229  -0.554   2.807  -0.986   0.063 1.9528780  -0.922
## 865  2006  -0.922   1.229  -0.554   2.807  -0.986 2.2257725   1.597
## 866  2006   1.597  -0.922   1.229  -0.554   2.807 2.6888960  -0.370
## 867  2006  -0.370   1.597  -0.922   1.229  -0.554 2.4098640   1.603
## 868  2006   1.603  -0.370   1.597  -0.922   1.229 2.5608060   1.029
## 869  2006   1.029   1.603  -0.370   1.597  -0.922 2.6394580   1.188
## 870  2006   1.188   1.029   1.603  -0.370   1.597 2.3659160   0.218
## 871  2006   0.218   1.188   1.029   1.603  -0.370 2.5261240   0.639
## 872  2006   0.639   0.218   1.188   1.029   1.603 2.7125320  -0.947
## 873  2006  -0.947   0.639   0.218   1.188   1.029 2.6921080   1.217
## 874  2006   1.217  -0.947   0.639   0.218   1.188 2.6574020   1.470
## 875  2006   1.470   1.217  -0.947   0.639   0.218 2.7613560  -0.018
## 876  2006  -0.018   1.470   1.217  -0.947   0.639 2.0537275  -0.303
## 877  2006  -0.303  -0.018   1.470   1.217  -0.947 2.9898280   0.940
## 878  2006   0.940  -0.303  -0.018   1.470   1.217 2.6861820   1.224
## 879  2006   1.224   0.940  -0.303  -0.018   1.470 2.7079220  -1.144
## 880  2006  -1.144   1.224   0.940  -0.303  -0.018 2.3285660   0.534
## 881  2007   0.534  -1.144   1.224   0.940  -0.303 1.5411125  -0.606
## 882  2007  -0.606   0.534  -1.144   1.224   0.940 3.1176733   1.491
## 883  2007   1.491  -0.606   0.534  -1.144   1.224 2.8221460  -0.016
## 884  2007  -0.016   1.491  -0.606   0.534  -1.144 2.7224275  -0.582
## 885  2007  -0.582  -0.016   1.491  -0.606   0.534 2.7838640   1.843
## 886  2007   1.843  -0.582  -0.016   1.491  -0.606 2.7795520  -0.713
## 887  2007  -0.713   1.843  -0.582  -0.016   1.491 2.6869900   1.216
## 888  2007   1.216  -0.713   1.843  -0.582  -0.016 2.5274980  -0.299
## 889  2007  -0.299   1.216  -0.713   1.843  -0.582 2.3688900  -4.412
## 890  2007  -4.412  -0.299   1.216  -0.713   1.843 3.5999640   1.130
## 891  2007   1.130  -4.412  -0.299   1.216  -0.713 3.1235860  -1.133
## 892  2007  -1.133   1.130  -4.412  -0.299   1.216 3.2246920   3.544
## 893  2007   3.544  -1.133   1.130  -4.412  -0.299 2.9013760  -1.062
## 894  2007  -1.062   3.544  -1.133   1.130  -4.412 2.8373620   1.612
## 895  2007   1.612  -1.062   3.544  -1.133   1.130 2.6927975   0.630
## 896  2007   0.630   1.612  -1.062   3.544  -1.133 2.6540600   2.168
## 897  2007   2.168   0.630   1.612  -1.062   3.544 3.0011180   0.655
## 898  2007   0.655   2.168   0.630   1.612  -1.062 2.9783940   0.773
## 899  2007   0.773   0.655   2.168   0.630   1.612 3.0906940   0.015
## 900  2007   0.015   0.773   0.655   2.168   0.630 2.8056760   1.122
## 901  2007   1.122   0.015   0.773   0.655   2.168 2.9180380  -0.461
## 902  2007  -0.461   1.122   0.015   0.773   0.655 3.0183800   1.360
## 903  2007   1.360  -0.461   1.122   0.015   0.773 2.9536375  -1.866
## 904  2007  -1.866   1.360  -0.461   1.122   0.015 3.0349000   1.674
## 905  2007   1.674  -1.866   1.360  -0.461   1.122 2.9758140  -1.980
## 906  2007  -1.980   1.674  -1.866   1.360  -0.461 3.2172320   0.053
## 907  2007   0.053  -1.980   1.674  -1.866   1.360 3.2512100   1.802
## 908  2007   1.802   0.053  -1.980   1.674  -1.866 2.3175625   1.441
## 909  2007   1.441   1.802   0.053  -1.980   1.674 3.0666500  -1.185
## 910  2007  -1.185   1.441   1.802   0.053  -1.980 3.2635400  -4.899
## 911  2007  -4.899  -1.185   1.441   1.802   0.053 4.1517860  -1.775
## 912  2007  -1.775  -4.899  -1.185   1.441   1.802 4.5102080   1.436
## 913  2007   1.436  -1.775  -4.899  -1.185   1.441 5.3423060  -0.530
## 914  2007  -0.530   1.436  -1.775  -4.899  -1.185 4.3762360   2.312
## 915  2007   2.312  -0.530   1.436  -1.775  -4.899 3.0536800  -0.364
## 916  2007  -0.364   2.312  -0.530   1.436  -1.775 2.7245820  -1.387
## 917  2007  -1.387  -0.364   2.312  -0.530   1.436 2.8522175   2.112
## 918  2007   2.112  -1.387  -0.364   2.312  -0.530 2.8511180   2.796
## 919  2007   2.796   2.112  -1.387  -0.364   2.312 3.3582480   0.066
## 920  2007   0.066   2.796   2.112  -1.387  -0.364 3.0708000   2.020
## 921  2007   2.020   0.066   2.796   2.112  -1.387 3.0117360   0.270
## 922  2007   0.270   2.020   0.066   2.796   2.112 2.9434800  -3.917
## 923  2007  -3.917   0.270   2.020   0.066   2.796 3.4752200   2.309
## 924  2007   2.309  -3.917   0.270   2.020   0.066 3.7160660  -1.669
## 925  2007  -1.669   2.309  -3.917   0.270   2.020 3.7635060  -3.706
## 926  2007  -3.706  -1.669   2.309  -3.917   0.270 4.4156840   0.347
## 927  2007   0.347  -3.706  -1.669   2.309  -3.917 4.0950360  -1.237
## 928  2007  -1.237   0.347  -3.706  -1.669   2.309 3.6709375   2.807
## 929  2007   2.807  -1.237   0.347  -3.706  -1.669 4.0964280   1.588
## 930  2007   1.588   2.807  -1.237   0.347  -3.706 3.4153620  -2.440
## 931  2007  -2.440   1.588   2.807  -1.237   0.347 3.7020560   1.125
## 932  2007   1.125  -2.440   1.588   2.807  -1.237 3.7459000  -0.402
## 933  2007  -0.402   1.125  -2.440   1.588   2.807 2.0160500  -4.522
## 934  2008  -4.522  -0.402   1.125  -2.440   1.588 3.3722575  -0.752
## 935  2008  -0.752  -4.522  -0.402   1.125  -2.440 4.7888020  -5.412
## 936  2008  -5.412  -0.752  -4.522  -0.402   1.125 5.0064640   0.409
## 937  2008   0.409  -5.412  -0.752  -4.522  -0.402 5.1009800   4.871
## 938  2008   4.871   0.409  -5.412  -0.752  -4.522 4.5395420  -4.596
## 939  2008  -4.596   4.871   0.409  -5.412  -0.752 4.0354580   1.405
## 940  2008   1.405  -4.596   4.871   0.409  -5.412 3.7444520   0.231
## 941  2008   0.231   1.405  -4.596   4.871   0.409 3.6883475  -1.661
## 942  2008  -1.661   0.231   1.405  -4.596   4.871 4.0464840  -2.800
## 943  2008  -2.800  -1.661   0.231   1.405  -4.596 4.4082660  -0.404
## 944  2008  -0.404  -2.800  -1.661   0.231   1.405 4.8023480   3.212
## 945  2008   3.212  -0.404  -2.800  -1.661   0.231 4.5919225  -1.075
## 946  2008  -1.075   3.212  -0.404  -2.800  -1.661 4.0849400   4.195
## 947  2008   4.195  -1.075   3.212  -0.404  -2.800 4.1755500  -2.742
## 948  2008  -2.742   4.195  -1.075   3.212  -0.404 3.6633780   4.314
## 949  2008   4.314  -2.742   4.195  -1.075   3.212 3.8685760   0.540
## 950  2008   0.540   4.314  -2.742   4.195  -1.075 3.9397780   1.149
## 951  2008   1.149   0.540   4.314  -2.742   4.195 4.0666040  -1.812
## 952  2008  -1.812   1.149   0.540   4.314  -2.742 3.7512440   2.670
## 953  2008   2.670  -1.812   1.149   0.540   4.314 3.8095320  -3.467
## 954  2008  -3.467   2.670  -1.812   1.149   0.540 3.9057240   1.777
## 955  2008   1.777  -3.467   2.670  -1.812   1.149 3.8140425  -2.835
## 956  2008  -2.835   1.777  -3.467   2.670  -1.812 4.3143580  -0.048
## 957  2008  -0.048  -2.835   1.777  -3.467   2.670 4.5268560  -3.096
## 958  2008  -3.096  -0.048  -2.835   1.777  -3.467 4.4438080  -3.001
## 959  2008  -3.001  -3.096  -0.048  -2.835   1.777 5.0313200  -1.211
## 960  2008  -1.211  -3.001  -3.096  -0.048  -2.835 4.8505750  -1.854
## 961  2008  -1.854  -1.211  -3.001  -3.096  -0.048 5.8126320   1.710
## 962  2008   1.710  -1.854  -1.211  -3.001  -3.096 6.5111240  -0.232
## 963  2008  -0.232   1.710  -1.854  -1.211  -3.001 5.6634480   0.203
## 964  2008   0.203  -0.232   1.710  -1.854  -1.211 5.0718900   2.857
## 965  2008   2.857   0.203  -0.232   1.710  -1.854 4.1882400   0.145
## 966  2008   0.145   2.857   0.203  -0.232   1.710 4.5344040  -0.462
## 967  2008  -0.462   0.145   2.857   0.203  -0.232 4.0635480  -0.725
## 968  2008  -0.725  -0.462   0.145   2.857   0.203 3.5300360  -3.159
## 969  2008  -3.159  -0.725  -0.462   0.145   2.857 5.0175300   0.756
## 970  2008   0.756  -3.159  -0.725  -0.462   0.145 6.8835840   0.270
## 971  2008   0.270   0.756  -3.159  -0.725  -0.462 9.3282140  -3.331
## 972  2008  -3.331   0.270   0.756  -3.159  -0.725 5.3198940  -9.399
## 973  2008  -9.399  -3.331   0.270   0.756  -3.159 6.2053260 -18.195
## 974  2008 -18.195  -9.399  -3.331   0.270   0.756 8.4033579   4.596
## 975  2008   4.596 -18.195  -9.399  -3.331   0.270 7.3067940  -6.781
## 976  2008  -6.781   4.596 -18.195  -9.399  -3.331 6.0359360  10.491
## 977  2008  10.491  -6.781   4.596 -18.195  -9.399 6.4605960  -3.898
## 978  2008  -3.898  10.491  -6.781   4.596 -18.195 5.2968160  -6.198
## 979  2008  -6.198  -3.898  10.491  -6.781   4.596 5.8129340  -8.389
## 980  2008  -8.389  -6.198  -3.898  10.491  -6.781 7.3490400  12.026
## 981  2008  12.026  -8.389  -6.198  -3.898  10.491 5.8415650  -2.251
## 982  2008  -2.251  12.026  -8.389  -6.198  -3.898 6.0939500   0.418
## 983  2008   0.418  -2.251  12.026  -8.389  -6.198 5.9324540   0.926
## 984  2008   0.926   0.418  -2.251  12.026  -8.389 5.8559720  -1.698
## 985  2008  -1.698   0.926   0.418  -2.251  12.026 3.0871050   6.760
## 986  2009   6.760  -1.698   0.926   0.418  -2.251 3.7931100  -4.448
## 987  2009  -4.448   6.760  -1.698   0.926   0.418 5.0439040  -4.518
## 988  2009  -4.518  -4.448   6.760  -1.698   0.926 5.9487580  -2.137
## 989  2009  -2.137  -4.518  -4.448   6.760  -1.698 6.1297625  -0.730
## 990  2009  -0.730  -2.137  -4.518  -4.448   6.760 5.6020040   5.173
## 991  2009   5.173  -0.730  -2.137  -4.518  -4.448 6.2176320  -4.808
## 992  2009  -4.808   5.173  -0.730  -2.137  -4.518 6.0088219  -6.868
## 993  2009  -6.868  -4.808   5.173  -0.730  -2.137 6.4015151  -4.540
## 994  2009  -4.540  -6.868  -4.808   5.173  -0.730 7.5507758  -7.035
## 995  2009  -7.035  -4.540  -6.868  -4.808   5.173 7.5928440  10.707
## 996  2009  10.707  -7.035  -4.540  -6.868  -4.808 7.4594358   1.585
## 997  2009   1.585  10.707  -7.035  -4.540  -6.868 7.9632760   6.168
## 998  2009   6.168   1.585  10.707  -7.035  -4.540 6.9528199   3.255
## 999  2009   3.255   6.168   1.585  10.707  -7.035 6.2868699   1.669
## 1000 2009   1.669   3.255   6.168   1.585  10.707 6.2261876   1.522
## 1001 2009   1.522   1.669   3.255   6.168   1.585 6.8393019  -0.388
## 1002 2009  -0.388   1.522   1.669   3.255   6.168 7.0831699   1.303
## 1003 2009   1.303  -0.388   1.522   1.669   3.255 6.0435580   5.893
## 1004 2009   5.893   1.303  -0.388   1.522   1.669 7.9520240  -4.988
## 1005 2009  -4.988   5.893   1.303  -0.388   1.522 6.3377520   0.467
## 1006 2009   0.467  -4.988   5.893   1.303  -0.388 6.3397280   3.623
## 1007 2009   3.623   0.467  -4.988   5.893   1.303 5.7888125   2.279
## 1008 2009   2.279   3.623   0.467  -4.988   5.893 5.6624700   0.651
## 1009 2009   0.651   2.279   3.623   0.467  -4.988 4.8663520  -2.640
## 1010 2009  -2.640   0.651   2.279   3.623   0.467 5.1140260  -0.253
## 1011 2009  -0.253  -2.640   0.651   2.279   3.623 5.1199160  -2.446
## 1012 2009  -2.446  -0.253  -2.640   0.651   2.279 4.1724325  -1.929
## 1013 2009  -1.929  -2.446  -0.253  -2.640   0.651 4.6733820   6.967
## 1014 2009   6.967  -1.929  -2.446  -0.253  -2.640 4.7854640   4.134
## 1015 2009   4.134   6.967  -1.929  -2.446  -0.253 5.0033000   0.839
## 1016 2009   0.839   4.134   6.967  -1.929  -2.446 5.2949320   2.329
## 1017 2009   2.329   0.839   4.134   6.967  -1.929 6.4279459  -0.632
## 1018 2009  -0.632   2.329   0.839   4.134   6.967 5.3737640   2.195
## 1019 2009   2.195  -0.632   2.329   0.839   4.134 4.6646500   0.273
## 1020 2009   0.273   2.195  -0.632   2.329   0.839 5.7445820  -1.218
## 1021 2009  -1.218   0.273   2.195  -0.632   2.329 5.2862600   2.591
## 1022 2009   2.591  -1.218   0.273   2.195  -0.632 5.1379225   2.452
## 1023 2009   2.452   2.591  -1.218   0.273   2.195 6.0469679  -2.239
## 1024 2009  -2.239   2.452   2.591  -1.218   0.273 5.0813020  -1.836
## 1025 2009  -1.836  -2.239   2.452   2.591  -1.218 5.2100800   4.514
## 1026 2009   4.514  -1.836  -2.239   2.452   2.591 4.4667100   1.511
## 1027 2009   1.511   4.514  -1.836  -2.239   2.452 4.7403700  -0.743
## 1028 2009  -0.743   1.511   4.514  -1.836  -2.239 5.1184660  -4.021
## 1029 2009  -4.021  -0.743   1.511   4.514  -1.836 6.0817140   3.195
## 1030 2009   3.195  -4.021  -0.743   1.511   4.514 5.2902260   2.261
## 1031 2009   2.261   3.195  -4.021  -0.743   1.511 4.2188720  -0.192
## 1032 2009  -0.192   2.261   3.195  -4.021  -0.743 4.1225040   0.010
## 1033 2009   0.010  -0.192   2.261   3.195  -4.021 3.2320000   1.328
## 1034 2009   1.328   0.010  -0.192   2.261   3.195 4.5354680   0.039
## 1035 2009   0.039   1.328   0.010  -0.192   2.261 4.1508760  -0.356
## 1036 2009  -0.356   0.039   1.328   0.010  -0.192 5.6728740   2.178
## 1037 2009   2.178  -0.356   0.039   1.328   0.010 3.0132625  -1.010
## 1038 2010  -1.010   2.178  -0.356   0.039   1.328 2.3904275   2.680
## 1039 2010   2.680  -1.010   2.178  -0.356   0.039 4.2230700  -0.782
## 1040 2010  -0.782   2.680  -1.010   2.178  -0.356 4.3632460  -3.897
## 1041 2010  -3.897  -0.782   2.680  -1.010   2.178 5.6545824  -1.639
## 1042 2010  -1.639  -3.897  -0.782   2.680  -1.010 5.0795340  -0.715
## 1043 2010  -0.715  -1.639  -3.897  -0.782   2.680 5.0822380   0.874
## 1044 2010   0.874  -0.715  -1.639  -3.897  -0.782 4.4034160   3.130
## 1045 2010   3.130   0.874  -0.715  -1.639  -3.897 4.0407250  -0.422
## 1046 2010  -0.422   3.130   0.874  -0.715  -1.639 4.1940340   3.097
## 1047 2010   3.097  -0.422   3.130   0.874  -0.715 4.0023300   0.991
## 1048 2010   0.991   3.097  -0.422   3.130   0.874 4.8053180   0.862
## 1049 2010   0.862   0.991   3.097  -0.422   3.130 4.5888000   0.577
## 1050 2010   0.577   0.862   0.991   3.097  -0.422 4.7512780   0.987
## 1051 2010   0.987   0.577   0.862   0.991   3.097 4.2379475   1.381
## 1052 2010   1.381   0.987   0.577   0.862   0.991 4.4615540  -0.188
## 1053 2010  -0.188   1.381   0.987   0.577   0.862 5.9749020   2.110
## 1054 2010   2.110  -0.188   1.381   0.987   0.577 5.8000960  -2.513
## 1055 2010  -2.513   2.110  -0.188   1.381   0.987 6.3104560  -6.388
## 1056 2010  -6.388  -2.513   2.110  -0.188   1.381 7.6838860   2.232
## 1057 2010   2.232  -6.388  -2.513   2.110  -0.188 5.7917500  -4.226
## 1058 2010  -4.226   2.232  -6.388  -2.513   2.110 6.5280519   0.158
## 1059 2010   0.158  -4.226   2.232  -6.388  -2.513 5.5288680  -2.252
## 1060 2010  -2.252   0.158  -4.226   2.232  -6.388 5.3685975   2.509
## 1061 2010   2.509  -2.252   0.158  -4.226   2.232 5.3695140   2.374
## 1062 2010   2.374   2.509  -2.252   0.158  -4.226 4.6372080  -3.646
## 1063 2010  -3.646   2.374   2.509  -2.252   0.158 4.6997120  -5.032
## 1064 2010  -5.032  -3.646   2.374   2.509  -2.252 5.1008920   5.416
## 1065 2010   5.416  -5.032  -3.646   2.374   2.509 4.4193725  -1.213
## 1066 2010  -1.213   5.416  -5.032  -3.646   2.374 4.4876640   3.548
## 1067 2010   3.548  -1.213   5.416  -5.032  -3.646 4.5802860  -0.096
## 1068 2010  -0.096   3.548  -1.213   5.416  -5.032 4.2713200   1.819
## 1069 2010   1.819  -0.096   3.548  -1.213   5.416 3.9634600  -3.779
## 1070 2010  -3.779   1.819  -0.096   3.548  -1.213 3.9065580  -0.700
## 1071 2010  -0.700  -3.779   1.819  -0.096   3.548 3.7774060  -0.663
## 1072 2010  -0.663  -0.700  -3.779   1.819  -0.096 3.9513280   3.750
## 1073 2010   3.750  -0.663  -0.700  -3.779   1.819 3.7184700   0.456
## 1074 2010   0.456   3.750  -0.663  -0.700  -3.779 3.1952375   1.446
## 1075 2010   1.446   0.456   3.750  -0.663  -0.700 3.9724320   2.050
## 1076 2010   2.050   1.446   0.456   3.750  -0.663 3.8845220  -0.212
## 1077 2010  -0.212   2.050   1.446   0.456   3.750 4.0374100   1.650
## 1078 2010   1.650  -0.212   2.050   1.446   0.456 3.9056160   0.948
## 1079 2010   0.948   1.650  -0.212   2.050   1.446 4.4491600   0.586
## 1080 2010   0.586   0.948   1.650  -0.212   2.050 4.5762820   0.015
## 1081 2010   0.015   0.586   0.948   1.650  -0.212 4.1164140   3.599
## 1082 2010   3.599   0.015   0.586   0.948   1.650 4.7987580  -2.173
## 1083 2010  -2.173   3.599   0.015   0.586   0.948 4.2982620   0.043
## 1084 2010   0.043  -2.173   3.599   0.015   0.586 4.1774360  -0.861
## 1085 2010  -0.861   0.043  -2.173   3.599   0.015 3.2051600   2.969
## 1086 2010   2.969  -0.861   0.043  -2.173   3.599 4.2425680   1.281
## 1087 2010   1.281   2.969  -0.861   0.043  -2.173 4.8350820   0.283
## 1088 2010   0.283   1.281   2.969  -0.861   0.043 4.4540440   1.034
## 1089 2010   1.034   0.283   1.281   2.969  -0.861 2.7071050   0.069
##      Direction
## 1         Down
## 2         Down
## 3           Up
## 4           Up
## 5           Up
## 6         Down
## 7           Up
## 8           Up
## 9           Up
## 10        Down
## 11        Down
## 12          Up
## 13          Up
## 14          Up
## 15        Down
## 16          Up
## 17        Down
## 18          Up
## 19        Down
## 20          Up
## 21          Up
## 22          Up
## 23        Down
## 24        Down
## 25        Down
## 26        Down
## 27        Down
## 28        Down
## 29          Up
## 30          Up
## 31        Down
## 32        Down
## 33        Down
## 34          Up
## 35        Down
## 36          Up
## 37        Down
## 38          Up
## 39          Up
## 40          Up
## 41        Down
## 42          Up
## 43          Up
## 44        Down
## 45          Up
## 46        Down
## 47        Down
## 48        Down
## 49          Up
## 50          Up
## 51          Up
## 52          Up
## 53          Up
## 54        Down
## 55          Up
## 56          Up
## 57        Down
## 58        Down
## 59          Up
## 60          Up
## 61          Up
## 62          Up
## 63        Down
## 64          Up
## 65        Down
## 66        Down
## 67          Up
## 68          Up
## 69        Down
## 70          Up
## 71        Down
## 72        Down
## 73          Up
## 74          Up
## 75          Up
## 76        Down
## 77          Up
## 78        Down
## 79        Down
## 80          Up
## 81          Up
## 82        Down
## 83        Down
## 84          Up
## 85        Down
## 86        Down
## 87          Up
## 88          Up
## 89        Down
## 90          Up
## 91          Up
## 92        Down
## 93        Down
## 94        Down
## 95          Up
## 96          Up
## 97          Up
## 98          Up
## 99          Up
## 100       Down
## 101         Up
## 102       Down
## 103       Down
## 104         Up
## 105         Up
## 106       Down
## 107         Up
## 108       Down
## 109         Up
## 110         Up
## 111       Down
## 112       Down
## 113         Up
## 114         Up
## 115       Down
## 116         Up
## 117         Up
## 118       Down
## 119         Up
## 120         Up
## 121       Down
## 122       Down
## 123       Down
## 124       Down
## 125         Up
## 126         Up
## 127         Up
## 128       Down
## 129         Up
## 130       Down
## 131         Up
## 132       Down
## 133       Down
## 134         Up
## 135         Up
## 136         Up
## 137       Down
## 138       Down
## 139       Down
## 140         Up
## 141         Up
## 142         Up
## 143       Down
## 144         Up
## 145         Up
## 146         Up
## 147         Up
## 148         Up
## 149         Up
## 150       Down
## 151       Down
## 152       Down
## 153         Up
## 154       Down
## 155         Up
## 156         Up
## 157       Down
## 158       Down
## 159         Up
## 160         Up
## 161         Up
## 162         Up
## 163       Down
## 164       Down
## 165         Up
## 166         Up
## 167       Down
## 168         Up
## 169         Up
## 170       Down
## 171         Up
## 172         Up
## 173       Down
## 174       Down
## 175       Down
## 176         Up
## 177       Down
## 178         Up
## 179       Down
## 180         Up
## 181         Up
## 182         Up
## 183         Up
## 184         Up
## 185         Up
## 186         Up
## 187         Up
## 188       Down
## 189       Down
## 190         Up
## 191       Down
## 192         Up
## 193       Down
## 194         Up
## 195       Down
## 196         Up
## 197       Down
## 198         Up
## 199         Up
## 200       Down
## 201         Up
## 202         Up
## 203       Down
## 204         Up
## 205         Up
## 206       Down
## 207         Up
## 208       Down
## 209         Up
## 210       Down
## 211       Down
## 212       Down
## 213         Up
## 214         Up
## 215       Down
## 216       Down
## 217         Up
## 218       Down
## 219         Up
## 220         Up
## 221       Down
## 222       Down
## 223         Up
## 224         Up
## 225         Up
## 226       Down
## 227       Down
## 228       Down
## 229         Up
## 230         Up
## 231         Up
## 232       Down
## 233         Up
## 234       Down
## 235         Up
## 236         Up
## 237         Up
## 238       Down
## 239       Down
## 240         Up
## 241       Down
## 242         Up
## 243       Down
## 244         Up
## 245       Down
## 246         Up
## 247       Down
## 248         Up
## 249       Down
## 250       Down
## 251         Up
## 252       Down
## 253         Up
## 254         Up
## 255       Down
## 256         Up
## 257         Up
## 258       Down
## 259         Up
## 260         Up
## 261         Up
## 262         Up
## 263         Up
## 264       Down
## 265         Up
## 266         Up
## 267         Up
## 268       Down
## 269         Up
## 270         Up
## 271       Down
## 272         Up
## 273         Up
## 274         Up
## 275       Down
## 276         Up
## 277         Up
## 278       Down
## 279         Up
## 280         Up
## 281       Down
## 282         Up
## 283         Up
## 284       Down
## 285         Up
## 286       Down
## 287       Down
## 288         Up
## 289         Up
## 290         Up
## 291         Up
## 292         Up
## 293       Down
## 294         Up
## 295       Down
## 296         Up
## 297         Up
## 298       Down
## 299         Up
## 300         Up
## 301         Up
## 302       Down
## 303         Up
## 304         Up
## 305       Down
## 306       Down
## 307         Up
## 308         Up
## 309       Down
## 310         Up
## 311         Up
## 312         Up
## 313         Up
## 314       Down
## 315         Up
## 316       Down
## 317       Down
## 318         Up
## 319         Up
## 320       Down
## 321         Up
## 322       Down
## 323         Up
## 324         Up
## 325       Down
## 326         Up
## 327         Up
## 328         Up
## 329       Down
## 330         Up
## 331       Down
## 332         Up
## 333         Up
## 334       Down
## 335       Down
## 336       Down
## 337       Down
## 338         Up
## 339       Down
## 340         Up
## 341         Up
## 342       Down
## 343         Up
## 344         Up
## 345         Up
## 346       Down
## 347         Up
## 348       Down
## 349         Up
## 350       Down
## 351         Up
## 352         Up
## 353         Up
## 354         Up
## 355         Up
## 356       Down
## 357       Down
## 358         Up
## 359         Up
## 360       Down
## 361         Up
## 362         Up
## 363       Down
## 364         Up
## 365         Up
## 366         Up
## 367       Down
## 368       Down
## 369         Up
## 370       Down
## 371       Down
## 372       Down
## 373       Down
## 374       Down
## 375         Up
## 376       Down
## 377         Up
## 378         Up
## 379         Up
## 380         Up
## 381         Up
## 382         Up
## 383         Up
## 384         Up
## 385       Down
## 386         Up
## 387       Down
## 388       Down
## 389         Up
## 390         Up
## 391       Down
## 392       Down
## 393         Up
## 394       Down
## 395         Up
## 396       Down
## 397         Up
## 398       Down
## 399         Up
## 400         Up
## 401       Down
## 402       Down
## 403       Down
## 404         Up
## 405         Up
## 406         Up
## 407       Down
## 408         Up
## 409       Down
## 410       Down
## 411       Down
## 412         Up
## 413       Down
## 414         Up
## 415       Down
## 416         Up
## 417         Up
## 418         Up
## 419         Up
## 420         Up
## 421         Up
## 422         Up
## 423         Up
## 424       Down
## 425         Up
## 426       Down
## 427         Up
## 428       Down
## 429         Up
## 430       Down
## 431         Up
## 432         Up
## 433       Down
## 434         Up
## 435       Down
## 436         Up
## 437         Up
## 438         Up
## 439         Up
## 440         Up
## 441       Down
## 442       Down
## 443       Down
## 444       Down
## 445         Up
## 446       Down
## 447       Down
## 448         Up
## 449         Up
## 450         Up
## 451       Down
## 452       Down
## 453         Up
## 454         Up
## 455         Up
## 456         Up
## 457       Down
## 458         Up
## 459         Up
## 460       Down
## 461       Down
## 462         Up
## 463         Up
## 464         Up
## 465         Up
## 466       Down
## 467       Down
## 468         Up
## 469       Down
## 470       Down
## 471         Up
## 472       Down
## 473         Up
## 474         Up
## 475         Up
## 476       Down
## 477         Up
## 478         Up
## 479       Down
## 480         Up
## 481       Down
## 482         Up
## 483       Down
## 484       Down
## 485       Down
## 486         Up
## 487       Down
## 488         Up
## 489       Down
## 490         Up
## 491         Up
## 492         Up
## 493       Down
## 494       Down
## 495       Down
## 496         Up
## 497         Up
## 498         Up
## 499         Up
## 500       Down
## 501       Down
## 502       Down
## 503         Up
## 504         Up
## 505       Down
## 506         Up
## 507         Up
## 508         Up
## 509         Up
## 510         Up
## 511       Down
## 512         Up
## 513       Down
## 514         Up
## 515         Up
## 516         Up
## 517       Down
## 518         Up
## 519       Down
## 520       Down
## 521         Up
## 522       Down
## 523       Down
## 524       Down
## 525         Up
## 526       Down
## 527         Up
## 528         Up
## 529       Down
## 530         Up
## 531       Down
## 532         Up
## 533         Up
## 534       Down
## 535       Down
## 536       Down
## 537       Down
## 538         Up
## 539       Down
## 540         Up
## 541       Down
## 542         Up
## 543         Up
## 544         Up
## 545       Down
## 546       Down
## 547         Up
## 548         Up
## 549         Up
## 550         Up
## 551         Up
## 552       Down
## 553       Down
## 554       Down
## 555       Down
## 556       Down
## 557       Down
## 558         Up
## 559       Down
## 560         Up
## 561       Down
## 562         Up
## 563       Down
## 564       Down
## 565         Up
## 566       Down
## 567       Down
## 568         Up
## 569       Down
## 570         Up
## 571         Up
## 572         Up
## 573       Down
## 574       Down
## 575       Down
## 576       Down
## 577       Down
## 578       Down
## 579       Down
## 580       Down
## 581         Up
## 582       Down
## 583         Up
## 584         Up
## 585         Up
## 586         Up
## 587       Down
## 588         Up
## 589       Down
## 590       Down
## 591         Up
## 592       Down
## 593         Up
## 594       Down
## 595       Down
## 596         Up
## 597       Down
## 598       Down
## 599         Up
## 600       Down
## 601       Down
## 602         Up
## 603       Down
## 604       Down
## 605       Down
## 606         Up
## 607         Up
## 608         Up
## 609       Down
## 610         Up
## 611       Down
## 612         Up
## 613         Up
## 614         Up
## 615       Down
## 616         Up
## 617       Down
## 618         Up
## 619         Up
## 620         Up
## 621       Down
## 622       Down
## 623         Up
## 624       Down
## 625       Down
## 626         Up
## 627       Down
## 628         Up
## 629         Up
## 630         Up
## 631       Down
## 632       Down
## 633       Down
## 634       Down
## 635         Up
## 636       Down
## 637       Down
## 638       Down
## 639         Up
## 640       Down
## 641       Down
## 642       Down
## 643       Down
## 644       Down
## 645         Up
## 646       Down
## 647       Down
## 648       Down
## 649         Up
## 650         Up
## 651         Up
## 652         Up
## 653         Up
## 654       Down
## 655       Down
## 656       Down
## 657       Down
## 658       Down
## 659       Down
## 660         Up
## 661         Up
## 662         Up
## 663         Up
## 664       Down
## 665         Up
## 666         Up
## 667         Up
## 668       Down
## 669       Down
## 670         Up
## 671       Down
## 672         Up
## 673         Up
## 674       Down
## 675       Down
## 676       Down
## 677       Down
## 678         Up
## 679         Up
## 680       Down
## 681       Down
## 682         Up
## 683         Up
## 684       Down
## 685         Up
## 686       Down
## 687         Up
## 688         Up
## 689         Up
## 690         Up
## 691         Up
## 692       Down
## 693         Up
## 694         Up
## 695         Up
## 696         Up
## 697       Down
## 698         Up
## 699         Up
## 700       Down
## 701         Up
## 702       Down
## 703       Down
## 704         Up
## 705         Up
## 706         Up
## 707         Up
## 708       Down
## 709         Up
## 710       Down
## 711         Up
## 712         Up
## 713         Up
## 714       Down
## 715         Up
## 716         Up
## 717       Down
## 718       Down
## 719         Up
## 720         Up
## 721         Up
## 722         Up
## 723         Up
## 724         Up
## 725         Up
## 726         Up
## 727         Up
## 728       Down
## 729         Up
## 730         Up
## 731       Down
## 732         Up
## 733         Up
## 734       Down
## 735       Down
## 736       Down
## 737         Up
## 738       Down
## 739       Down
## 740         Up
## 741       Down
## 742       Down
## 743       Down
## 744       Down
## 745         Up
## 746         Up
## 747         Up
## 748       Down
## 749       Down
## 750       Down
## 751       Down
## 752       Down
## 753       Down
## 754         Up
## 755       Down
## 756         Up
## 757         Up
## 758         Up
## 759         Up
## 760         Up
## 761         Up
## 762       Down
## 763         Up
## 764       Down
## 765       Down
## 766       Down
## 767         Up
## 768         Up
## 769         Up
## 770       Down
## 771         Up
## 772         Up
## 773       Down
## 774         Up
## 775         Up
## 776         Up
## 777       Down
## 778       Down
## 779       Down
## 780         Up
## 781         Up
## 782         Up
## 783       Down
## 784         Up
## 785         Up
## 786       Down
## 787       Down
## 788       Down
## 789         Up
## 790         Up
## 791       Down
## 792         Up
## 793         Up
## 794         Up
## 795       Down
## 796         Up
## 797         Up
## 798       Down
## 799         Up
## 800         Up
## 801       Down
## 802         Up
## 803         Up
## 804         Up
## 805         Up
## 806         Up
## 807       Down
## 808         Up
## 809       Down
## 810       Down
## 811         Up
## 812         Up
## 813       Down
## 814       Down
## 815         Up
## 816       Down
## 817       Down
## 818       Down
## 819         Up
## 820         Up
## 821         Up
## 822         Up
## 823         Up
## 824       Down
## 825       Down
## 826         Up
## 827         Up
## 828       Down
## 829         Up
## 830         Up
## 831       Down
## 832         Up
## 833       Down
## 834         Up
## 835         Up
## 836         Up
## 837       Down
## 838       Down
## 839         Up
## 840       Down
## 841       Down
## 842         Up
## 843       Down
## 844         Up
## 845       Down
## 846         Up
## 847       Down
## 848       Down
## 849         Up
## 850         Up
## 851       Down
## 852       Down
## 853       Down
## 854         Up
## 855       Down
## 856       Down
## 857         Up
## 858         Up
## 859         Up
## 860       Down
## 861         Up
## 862       Down
## 863         Up
## 864       Down
## 865         Up
## 866       Down
## 867         Up
## 868         Up
## 869         Up
## 870         Up
## 871         Up
## 872       Down
## 873         Up
## 874         Up
## 875       Down
## 876       Down
## 877         Up
## 878         Up
## 879       Down
## 880         Up
## 881       Down
## 882         Up
## 883       Down
## 884       Down
## 885         Up
## 886       Down
## 887         Up
## 888       Down
## 889       Down
## 890         Up
## 891       Down
## 892         Up
## 893       Down
## 894         Up
## 895         Up
## 896         Up
## 897         Up
## 898         Up
## 899         Up
## 900         Up
## 901       Down
## 902         Up
## 903       Down
## 904         Up
## 905       Down
## 906         Up
## 907         Up
## 908         Up
## 909       Down
## 910       Down
## 911       Down
## 912         Up
## 913       Down
## 914         Up
## 915       Down
## 916       Down
## 917         Up
## 918         Up
## 919         Up
## 920         Up
## 921         Up
## 922       Down
## 923         Up
## 924       Down
## 925       Down
## 926         Up
## 927       Down
## 928         Up
## 929         Up
## 930       Down
## 931         Up
## 932       Down
## 933       Down
## 934       Down
## 935       Down
## 936         Up
## 937         Up
## 938       Down
## 939         Up
## 940         Up
## 941       Down
## 942       Down
## 943       Down
## 944         Up
## 945       Down
## 946         Up
## 947       Down
## 948         Up
## 949         Up
## 950         Up
## 951       Down
## 952         Up
## 953       Down
## 954         Up
## 955       Down
## 956       Down
## 957       Down
## 958       Down
## 959       Down
## 960       Down
## 961         Up
## 962       Down
## 963         Up
## 964         Up
## 965         Up
## 966       Down
## 967       Down
## 968       Down
## 969         Up
## 970         Up
## 971       Down
## 972       Down
## 973       Down
## 974         Up
## 975       Down
## 976         Up
## 977       Down
## 978       Down
## 979       Down
## 980         Up
## 981       Down
## 982         Up
## 983         Up
## 984       Down
## 985         Up
## 986       Down
## 987       Down
## 988       Down
## 989       Down
## 990         Up
## 991       Down
## 992       Down
## 993       Down
## 994       Down
## 995         Up
## 996         Up
## 997         Up
## 998         Up
## 999         Up
## 1000        Up
## 1001      Down
## 1002        Up
## 1003        Up
## 1004      Down
## 1005        Up
## 1006        Up
## 1007        Up
## 1008        Up
## 1009      Down
## 1010      Down
## 1011      Down
## 1012      Down
## 1013        Up
## 1014        Up
## 1015        Up
## 1016        Up
## 1017      Down
## 1018        Up
## 1019        Up
## 1020      Down
## 1021        Up
## 1022        Up
## 1023      Down
## 1024      Down
## 1025        Up
## 1026        Up
## 1027      Down
## 1028      Down
## 1029        Up
## 1030        Up
## 1031      Down
## 1032        Up
## 1033        Up
## 1034        Up
## 1035      Down
## 1036        Up
## 1037      Down
## 1038        Up
## 1039      Down
## 1040      Down
## 1041      Down
## 1042      Down
## 1043        Up
## 1044        Up
## 1045      Down
## 1046        Up
## 1047        Up
## 1048        Up
## 1049        Up
## 1050        Up
## 1051        Up
## 1052      Down
## 1053        Up
## 1054      Down
## 1055      Down
## 1056        Up
## 1057      Down
## 1058        Up
## 1059      Down
## 1060        Up
## 1061        Up
## 1062      Down
## 1063      Down
## 1064        Up
## 1065      Down
## 1066        Up
## 1067      Down
## 1068        Up
## 1069      Down
## 1070      Down
## 1071      Down
## 1072        Up
## 1073        Up
## 1074        Up
## 1075        Up
## 1076      Down
## 1077        Up
## 1078        Up
## 1079        Up
## 1080        Up
## 1081        Up
## 1082      Down
## 1083        Up
## 1084      Down
## 1085        Up
## 1086        Up
## 1087        Up
## 1088        Up
## 1089        Up
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs}\NormalTok{(Weekly)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-19-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(Weekly)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Year           Lag1               Lag2               Lag3         
##  Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950  
##  1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580  
##  Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410  
##  Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472  
##  3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090  
##  Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260  
##       Lag4               Lag5              Volume       
##  Min.   :-18.1950   Min.   :-18.1950   Min.   :0.08747  
##  1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.:0.33202  
##  Median :  0.2380   Median :  0.2340   Median :1.00268  
##  Mean   :  0.1458   Mean   :  0.1399   Mean   :1.57462  
##  3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:2.05373  
##  Max.   : 12.0260   Max.   : 12.0260   Max.   :9.32821  
##      Today          Direction 
##  Min.   :-18.1950   Down:484  
##  1st Qu.: -1.1540   Up  :605  
##  Median :  0.2410             
##  Mean   :  0.1499             
##  3rd Qu.:  1.4050             
##  Max.   : 12.0260
\end{verbatim}

Volume is positively correleted with year.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Use the full data set to perform a logistic regression with Direction
  as the response and the five lag variables plus Volume as predictors.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Weekly_Model1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Direction}\OperatorTok{~}\NormalTok{Lag1}\OperatorTok{+}\NormalTok{Lag2}\OperatorTok{+}\NormalTok{Lag3}\OperatorTok{+}\NormalTok{Lag4}\OperatorTok{+}\NormalTok{Lag5}\OperatorTok{+}\NormalTok{Volume, }\DataTypeTok{data=}\NormalTok{Weekly,}\DataTypeTok{family=}\NormalTok{binomial)}
\end{Highlighting}
\end{Shaded}

Use the summary function to print the results. Do any of the predictors
appear to be statistically significant? If so, which ones?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(Weekly_Model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
##     Volume, family = binomial, data = Weekly)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6949  -1.2565   0.9913   1.0849   1.4579  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept)  0.26686    0.08593   3.106   0.0019 **
## Lag1        -0.04127    0.02641  -1.563   0.1181   
## Lag2         0.05844    0.02686   2.175   0.0296 * 
## Lag3        -0.01606    0.02666  -0.602   0.5469   
## Lag4        -0.02779    0.02646  -1.050   0.2937   
## Lag5        -0.01447    0.02638  -0.549   0.5833   
## Volume      -0.02274    0.03690  -0.616   0.5377   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1496.2  on 1088  degrees of freedom
## Residual deviance: 1486.4  on 1082  degrees of freedom
## AIC: 1500.4
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

Lag2 is the only significant predictor.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Compute the confusion matrix and overall fraction of correct
  predictions. Explain what the confusion matrix is telling you about
  the types of mistakes made by logistic regression.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probs <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Weekly_Model1, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{preds <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Down"}\NormalTok{, }\DecValTok{1089}\NormalTok{)}
\NormalTok{preds[probs }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{] =}\StringTok{ "Up"}
\KeywordTok{table}\NormalTok{(preds, Weekly}\OperatorTok{$}\NormalTok{Direction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## preds  Down  Up
##   Down   54  48
##   Up    430 557
\end{verbatim}

Investors are much more likely to bet on a valuation increasing (and end
up overvaluing an option in the market) vs.~undervaluing a stock that
value actually goes up. However, over 50\% of predictions are correct
(430 ``false positives''). This is statistically not bad but I would
argue dangerous when actually applied to the markets.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Now fit the logistic regression model using a training data period
  from 1990 to 2008, with Lag2 as the only predictor. Compute the
  confusion matrix and the overall fraction of correct predictions for
  the held out data (that is, the data from 2009 and 2010).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Create training data}
\NormalTok{Weekly_training <-}\StringTok{ }\NormalTok{Weekly[Weekly}\OperatorTok{$}\NormalTok{Year}\OperatorTok{<}\DecValTok{2009}\NormalTok{,]}
\NormalTok{Weekly_test <-}\StringTok{ }\NormalTok{Weekly[Weekly}\OperatorTok{$}\NormalTok{Year}\OperatorTok{>}\DecValTok{2008}\NormalTok{,]}
\NormalTok{Weekly_training_glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Direction}\OperatorTok{~}\NormalTok{Lag2, }\DataTypeTok{data=}\NormalTok{ Weekly_training, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(Weekly_training_glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Direction ~ Lag2, family = "binomial", data = Weekly_training)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.536  -1.264   1.021   1.091   1.368  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept)  0.20326    0.06428   3.162  0.00157 **
## Lag2         0.05810    0.02870   2.024  0.04298 * 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1354.7  on 984  degrees of freedom
## Residual deviance: 1350.5  on 983  degrees of freedom
## AIC: 1354.5
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Confusion Matrix}
\NormalTok{probs <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Weekly_training_glm, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{preds <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Down"}\NormalTok{, }\DecValTok{985}\NormalTok{)}
\NormalTok{preds[probs }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{] =}\StringTok{ "Up"}
\KeywordTok{table}\NormalTok{(preds, Weekly_training}\OperatorTok{$}\NormalTok{Direction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## preds  Down  Up
##   Down   23  20
##   Up    418 524
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{524}\OperatorTok{+}\DecValTok{23}\NormalTok{)}\OperatorTok{/}\DecValTok{985}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5553299
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Repeat (d) using LDA.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Weekly_training_lda <-}\StringTok{ }\KeywordTok{lda}\NormalTok{(Direction}\OperatorTok{~}\NormalTok{Lag2, }\DataTypeTok{data=}\NormalTok{Weekly_training)}
\NormalTok{Weekly_training_lda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lda(Direction ~ Lag2, data = Weekly_training)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4477157 0.5522843 
## 
## Group means:
##             Lag2
## Down -0.03568254
## Up    0.26036581
## 
## Coefficients of linear discriminants:
##            LD1
## Lag2 0.4414162
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Weekly_training_lda,}\DataTypeTok{newdata =}\NormalTok{ Weekly_test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{class <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{table}\NormalTok{(class,Weekly_test}\OperatorTok{$}\NormalTok{Direction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## class  Down Up
##   Down    9  5
##   Up     34 56
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{56}\OperatorTok{+}\DecValTok{9}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{56}\OperatorTok{+}\DecValTok{5}\OperatorTok{+}\DecValTok{9}\OperatorTok{+}\DecValTok{34}\NormalTok{)}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.625
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  Repeat (d) using QDA.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Weekly_training_qda <-}\StringTok{ }\KeywordTok{qda}\NormalTok{(Direction}\OperatorTok{~}\NormalTok{Lag2, }\DataTypeTok{data=}\NormalTok{Weekly_training)}
\NormalTok{Weekly_training_qda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## qda(Direction ~ Lag2, data = Weekly_training)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4477157 0.5522843 
## 
## Group means:
##             Lag2
## Down -0.03568254
## Up    0.26036581
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Weekly_training_qda,}\DataTypeTok{newdata =}\NormalTok{ Weekly_test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{class <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{table}\NormalTok{(class,Weekly_test}\OperatorTok{$}\NormalTok{Direction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## class  Down Up
##   Down    0  0
##   Up     43 61
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{43}\OperatorTok{+}\DecValTok{61}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{43}\OperatorTok{+}\DecValTok{61}\NormalTok{)}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  Repeat (d) using KNN with K = 1.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(class)}
\NormalTok{Weekly_training_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(Weekly_training}\OperatorTok{$}\NormalTok{Lag2)}
\NormalTok{Weekly_test_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(Weekly_test}\OperatorTok{$}\NormalTok{Lag2)}
\NormalTok{Weekly_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(Weekly_training}\OperatorTok{$}\NormalTok{Direction)}
\NormalTok{Weekly_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Weekly_training_predictor, Weekly_test_predictor, Weekly_training_outcome, }\DataTypeTok{k=}\DecValTok{1}\NormalTok{)}
\KeywordTok{table}\NormalTok{(Weekly_Model_knn, Weekly_test}\OperatorTok{$}\NormalTok{Direction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 
## Weekly_Model_knn Down Up
##                1   21 30
##                2   22 31
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{31}\OperatorTok{+}\DecValTok{21}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{21}\OperatorTok{+}\DecValTok{22}\OperatorTok{+}\DecValTok{30}\OperatorTok{+}\DecValTok{31}\NormalTok{)}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{7}
\tightlist
\item
  Which of these methods appears to provide the best results on this
  data?
\end{enumerate}

The QDA method appears to perform the best! With 100\% accuracy.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  Experiment with different combinations of predictors, including
  possible transformations and interactions, for each of the methods.
  Report the variables, method, and associated confusion matrix that
  appears to provide the best results on the held out data. Note that
  you should also experiment with values for K in the KNN classifier
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#EDA}
\KeywordTok{cor}\NormalTok{(Weekly[,}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Year         Lag1        Lag2        Lag3         Lag4
## Year    1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923
## Lag1   -0.03228927  1.000000000 -0.07485305  0.05863568 -0.071273876
## Lag2   -0.03339001 -0.074853051  1.00000000 -0.07572091  0.058381535
## Lag3   -0.03000649  0.058635682 -0.07572091  1.00000000 -0.075395865
## Lag4   -0.03112792 -0.071273876  0.05838153 -0.07539587  1.000000000
## Lag5   -0.03051910 -0.008183096 -0.07249948  0.06065717 -0.075675027
## Volume  0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617
## Today  -0.03245989 -0.075031842  0.05916672 -0.07124364 -0.007825873
##                Lag5      Volume        Today
## Year   -0.030519101  0.84194162 -0.032459894
## Lag1   -0.008183096 -0.06495131 -0.075031842
## Lag2   -0.072499482 -0.08551314  0.059166717
## Lag3    0.060657175 -0.06928771 -0.071243639
## Lag4   -0.075675027 -0.06107462 -0.007825873
## Lag5    1.000000000 -0.05851741  0.011012698
## Volume -0.058517414  1.00000000 -0.033077783
## Today   0.011012698 -0.03307778  1.000000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"Hmisc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'Hmisc' was built under R version 3.5.3
\end{verbatim}

\begin{verbatim}
## Loading required package: lattice
\end{verbatim}

\begin{verbatim}
## Loading required package: survival
\end{verbatim}

\begin{verbatim}
## Loading required package: Formula
\end{verbatim}

\begin{verbatim}
## Loading required package: ggplot2
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'Hmisc'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     format.pval, units
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res2 <-}\StringTok{ }\KeywordTok{rcorr}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(Weekly[,}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{]))}
\NormalTok{res2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Year  Lag1  Lag2  Lag3  Lag4  Lag5 Volume Today
## Year    1.00 -0.03 -0.03 -0.03 -0.03 -0.03   0.84 -0.03
## Lag1   -0.03  1.00 -0.07  0.06 -0.07 -0.01  -0.06 -0.08
## Lag2   -0.03 -0.07  1.00 -0.08  0.06 -0.07  -0.09  0.06
## Lag3   -0.03  0.06 -0.08  1.00 -0.08  0.06  -0.07 -0.07
## Lag4   -0.03 -0.07  0.06 -0.08  1.00 -0.08  -0.06 -0.01
## Lag5   -0.03 -0.01 -0.07  0.06 -0.08  1.00  -0.06  0.01
## Volume  0.84 -0.06 -0.09 -0.07 -0.06 -0.06   1.00 -0.03
## Today  -0.03 -0.08  0.06 -0.07 -0.01  0.01  -0.03  1.00
## 
## n= 1089 
## 
## 
## P
##        Year   Lag1   Lag2   Lag3   Lag4   Lag5   Volume Today 
## Year          0.2871 0.2709 0.3225 0.3048 0.3143 0.0000 0.2845
## Lag1   0.2871        0.0135 0.0531 0.0187 0.7874 0.0321 0.0133
## Lag2   0.2709 0.0135        0.0124 0.0541 0.0167 0.0047 0.0509
## Lag3   0.3225 0.0531 0.0124        0.0128 0.0454 0.0222 0.0187
## Lag4   0.3048 0.0187 0.0541 0.0128        0.0125 0.0439 0.7964
## Lag5   0.3143 0.7874 0.0167 0.0454 0.0125        0.0535 0.7166
## Volume 0.0000 0.0321 0.0047 0.0222 0.0439 0.0535        0.2754
## Today  0.2845 0.0133 0.0509 0.0187 0.7964 0.7166 0.2754
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Chose to add predictors closest to significant }

\CommentTok{#QDA}
\NormalTok{Weekly_training_qda <-}\StringTok{ }\KeywordTok{qda}\NormalTok{(Direction}\OperatorTok{~}\NormalTok{Lag2}\OperatorTok{+}\NormalTok{Lag1}\OperatorTok{+}\NormalTok{Lag4, }\DataTypeTok{data=}\NormalTok{Weekly_training)}
\NormalTok{Weekly_training_qda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## qda(Direction ~ Lag2 + Lag1 + Lag4, data = Weekly_training)
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4477157 0.5522843 
## 
## Group means:
##             Lag2         Lag1       Lag4
## Down -0.03568254  0.289444444 0.15925624
## Up    0.26036581 -0.009213235 0.09220956
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Weekly_training_qda,}\DataTypeTok{newdata =}\NormalTok{ Weekly_test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{class <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{table}\NormalTok{(class,Weekly_test}\OperatorTok{$}\NormalTok{Direction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       
## class  Down Up
##   Down    9 20
##   Up     34 41
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#KNN}
\NormalTok{Weekly_training_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(Weekly_training}\OperatorTok{$}\NormalTok{Lag2,Weekly_training}\OperatorTok{$}\NormalTok{Lag1,Weekly_training}\OperatorTok{$}\NormalTok{Lag4)}
\NormalTok{Weekly_test_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(Weekly_test}\OperatorTok{$}\NormalTok{Lag2,Weekly_training}\OperatorTok{$}\NormalTok{Lag1,Weekly_training}\OperatorTok{$}\NormalTok{Lag4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in cbind(Weekly_test$Lag2, Weekly_training$Lag1,
## Weekly_training$Lag4): number of rows of result is not a multiple of vector
## length (arg 1)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Weekly_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(Weekly_training}\OperatorTok{$}\NormalTok{Direction)}
\NormalTok{Weekly_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Weekly_training_predictor, Weekly_test_predictor, Weekly_training_outcome, }\DataTypeTok{k=}\DecValTok{1}\NormalTok{)}
\CommentTok{#table(Weekly_Model_knn, Weekly_test$Direction)}
\end{Highlighting}
\end{Shaded}

I stopped here as we can see that adding predictors increased our error
rate and did not help us in our prediction.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\item
  (15 pts) Question 11 in Section 4.7.
\item
  In this problem, you will develop a model to predict whether a given
  car gets high or low gas mileage based on the Auto data set.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Create a binary variable, mpg01, that contains a 1 if mpg contains a
  value above its median, and a 0 if mpg contains a value below its
  median. You can compute the median using the median() function. Note
  you may ï¬nd it helpful to use the data.frame() function to create a
  single data set containing both mpg01 and the other Auto variables.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(Auto}\OperatorTok{$}\NormalTok{mpg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 22.75
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Auto}\OperatorTok{$}\NormalTok{mpg01 <-}\StringTok{ }\DecValTok{0}  
\NormalTok{Auto}\OperatorTok{$}\NormalTok{mpg01[Auto}\OperatorTok{$}\NormalTok{mpg}\OperatorTok{>}\KeywordTok{median}\NormalTok{(Auto}\OperatorTok{$}\NormalTok{mpg)] <-}\StringTok{ }\DecValTok{1}
\KeywordTok{head}\NormalTok{(Auto,}\DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    mpg cylinders displacement horsepower weight acceleration year origin
## 1   18         8          307        130   3504         12.0   70      1
## 2   15         8          350        165   3693         11.5   70      1
## 3   18         8          318        150   3436         11.0   70      1
## 4   16         8          304        150   3433         12.0   70      1
## 5   17         8          302        140   3449         10.5   70      1
## 6   15         8          429        198   4341         10.0   70      1
## 7   14         8          454        220   4354          9.0   70      1
## 8   14         8          440        215   4312          8.5   70      1
## 9   14         8          455        225   4425         10.0   70      1
## 10  15         8          390        190   3850          8.5   70      1
## 11  15         8          383        170   3563         10.0   70      1
## 12  14         8          340        160   3609          8.0   70      1
## 13  15         8          400        150   3761          9.5   70      1
## 14  14         8          455        225   3086         10.0   70      1
## 15  24         4          113         95   2372         15.0   70      3
## 16  22         6          198         95   2833         15.5   70      1
## 17  18         6          199         97   2774         15.5   70      1
## 18  21         6          200         85   2587         16.0   70      1
## 19  27         4           97         88   2130         14.5   70      3
## 20  26         4           97         46   1835         20.5   70      2
## 21  25         4          110         87   2672         17.5   70      2
## 22  24         4          107         90   2430         14.5   70      2
## 23  25         4          104         95   2375         17.5   70      2
## 24  26         4          121        113   2234         12.5   70      2
## 25  21         6          199         90   2648         15.0   70      1
## 26  10         8          360        215   4615         14.0   70      1
## 27  10         8          307        200   4376         15.0   70      1
## 28  11         8          318        210   4382         13.5   70      1
## 29   9         8          304        193   4732         18.5   70      1
## 30  27         4           97         88   2130         14.5   71      3
##                            name mpg01
## 1     chevrolet chevelle malibu     0
## 2             buick skylark 320     0
## 3            plymouth satellite     0
## 4                 amc rebel sst     0
## 5                   ford torino     0
## 6              ford galaxie 500     0
## 7              chevrolet impala     0
## 8             plymouth fury iii     0
## 9              pontiac catalina     0
## 10           amc ambassador dpl     0
## 11          dodge challenger se     0
## 12           plymouth 'cuda 340     0
## 13        chevrolet monte carlo     0
## 14      buick estate wagon (sw)     0
## 15        toyota corona mark ii     1
## 16              plymouth duster     0
## 17                   amc hornet     0
## 18                ford maverick     0
## 19                 datsun pl510     1
## 20 volkswagen 1131 deluxe sedan     1
## 21                  peugeot 504     1
## 22                  audi 100 ls     1
## 23                     saab 99e     1
## 24                     bmw 2002     1
## 25                  amc gremlin     0
## 26                    ford f250     0
## 27                    chevy c20     0
## 28                   dodge d200     0
## 29                     hi 1200d     0
## 30                 datsun pl510     1
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Explore the data graphically in order to investigate the association
  between mpg01 and the other features. Which of the other features seem
  most likely to be useful in predicting mpg01? Scatterplots and
  boxplots may be useful tools to answer this question. Describe your
  findings.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lapply}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\StringTok{"displacement"}\NormalTok{,}\StringTok{"cylinders"}\NormalTok{,}\StringTok{"horsepower"}\NormalTok{,}\StringTok{"weight"}\NormalTok{,}\StringTok{"acceleration"}\NormalTok{,}\StringTok{"year"}\NormalTok{,}\StringTok{"origin"}\NormalTok{), }
       \ControlFlowTok{function}\NormalTok{(i) }\KeywordTok{ggplot}\NormalTok{(Auto, }\KeywordTok{aes_string}\NormalTok{(}\DataTypeTok{x=}\NormalTok{i, }\DataTypeTok{y=}\StringTok{"mpg01"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-29-1.pdf}

\begin{verbatim}
## 
## [[2]]
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-29-2.pdf}

\begin{verbatim}
## 
## [[3]]
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-29-3.pdf}

\begin{verbatim}
## 
## [[4]]
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-29-4.pdf}

\begin{verbatim}
## 
## [[5]]
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-29-5.pdf}

\begin{verbatim}
## 
## [[6]]
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-29-6.pdf}

\begin{verbatim}
## 
## [[7]]
\end{verbatim}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-29-7.pdf}

-Cars with lower displacement, on average, have a higher mpg -Cylinders
and mpg have no real relationship -Cars with lower horsepower tend to
have higher mpg -Lighter cars have a higher mpg -As acceleration
increases, on avg, so does mpg

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Split the data into a training set and a test set.
\end{enumerate}

To do this I used a new package I found: caTools

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caTools)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{#  set seed to ensure you always have same random numbers generated}
\NormalTok{sample =}\StringTok{ }\KeywordTok{sample.split}\NormalTok{(Auto,}\DataTypeTok{SplitRatio =} \FloatTok{0.75}\NormalTok{) }\CommentTok{# splits the data in the ratio mentioned in SplitRatio. After splitting marks these rows as logical TRUE and the the remaining are marked as logical FALSE}
\NormalTok{train1 =}\KeywordTok{subset}\NormalTok{(Auto,sample }\OperatorTok{==}\OtherTok{TRUE}\NormalTok{) }\CommentTok{# creates a training dataset named train1 with rows which are marked as TRUE}
\NormalTok{test1=}\KeywordTok{subset}\NormalTok{(Auto, sample}\OperatorTok{==}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Perform LDA on the training data in order to predict mpg01 using the
  variables that seemed most associated with mpg01 in (b). What is the
  test error of the model obtained?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Auto_training_lda <-}\StringTok{ }\KeywordTok{lda}\NormalTok{(mpg01}\OperatorTok{~}\NormalTok{acceleration}\OperatorTok{+}\NormalTok{horsepower}\OperatorTok{+}\NormalTok{weight, }\DataTypeTok{data=}\NormalTok{train1)}
\NormalTok{Auto_training_lda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lda(mpg01 ~ acceleration + horsepower + weight, data = train1)
## 
## Prior probabilities of groups:
##         0         1 
## 0.4981818 0.5018182 
## 
## Group means:
##   acceleration horsepower   weight
## 0     14.61971  128.94891 3586.978
## 1     16.37536   79.21739 2346.471
## 
## Coefficients of linear discriminants:
##                        LD1
## acceleration  0.0299487831
## horsepower   -0.0002462394
## weight       -0.0017314158
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Auto_training_lda,}\DataTypeTok{newdata =}\NormalTok{ test1, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{class <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{table}\NormalTok{(class,test1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
## class  0  1
##     0 51  2
##     1  8 56
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{56}\OperatorTok{+}\DecValTok{51}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{10}\OperatorTok{+}\DecValTok{56}\OperatorTok{+}\DecValTok{51}\NormalTok{)}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9145299
\end{verbatim}

The test error is (1-correctpred)

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{-}\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08547009
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Perform QDA on the training data in order to predict mpg01 using the
  variables that seemed most associated with mpg01 in (b). What is the
  test error of the model obtained?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Auto_training_qda <-}\StringTok{ }\KeywordTok{qda}\NormalTok{(mpg01}\OperatorTok{~}\NormalTok{acceleration}\OperatorTok{+}\NormalTok{horsepower}\OperatorTok{+}\NormalTok{weight, }\DataTypeTok{data=}\NormalTok{train1)}
\NormalTok{Auto_training_qda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## qda(mpg01 ~ acceleration + horsepower + weight, data = train1)
## 
## Prior probabilities of groups:
##         0         1 
## 0.4981818 0.5018182 
## 
## Group means:
##   acceleration horsepower   weight
## 0     14.61971  128.94891 3586.978
## 1     16.37536   79.21739 2346.471
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Auto_training_qda,}\DataTypeTok{newdata =}\NormalTok{ test1, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{class <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{table}\NormalTok{(class,test1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
## class  0  1
##     0 52  3
##     1  7 55
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{55}\OperatorTok{+}\DecValTok{52}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{55}\OperatorTok{+}\DecValTok{52}\OperatorTok{+}\DecValTok{10}\NormalTok{)}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9145299
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{error_rate <-}\StringTok{ }\DecValTok{1}\OperatorTok{-}\NormalTok{correctpred}
\NormalTok{error_rate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08547009
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  Perform logistic regression on the training data in order to predict
  mpg01 using the variables that seemed most associated with mpg01 in
  (b). What is the test error of the model obtained?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Auto_training_glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(mpg01}\OperatorTok{~}\NormalTok{acceleration}\OperatorTok{+}\NormalTok{horsepower}\OperatorTok{+}\NormalTok{weight, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{data=}\NormalTok{train1)}
\KeywordTok{summary}\NormalTok{(Auto_training_glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = mpg01 ~ acceleration + horsepower + weight, family = "binomial", 
##     data = train1)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.19516  -0.26238   0.09789   0.42866   2.92539  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  12.4643477  3.1080838   4.010 6.06e-05 ***
## acceleration -0.0102397  0.1458716  -0.070   0.9440    
## horsepower   -0.0484565  0.0228826  -2.118   0.0342 *  
## weight       -0.0026908  0.0006848  -3.930 8.51e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 381.23  on 274  degrees of freedom
## Residual deviance: 165.50  on 271  degrees of freedom
## AIC: 173.5
## 
## Number of Fisher Scoring iterations: 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Confusion Matrix}
\NormalTok{probs <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Auto_training_glm, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{preds <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"0"}\NormalTok{, }\DecValTok{275}\NormalTok{)}
\NormalTok{preds[probs }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{] =}\StringTok{ "1"}
\KeywordTok{table}\NormalTok{(preds, train1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
## preds   0   1
##     0 116  14
##     1  21 124
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{124}\OperatorTok{+}\DecValTok{116}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{124}\OperatorTok{+}\DecValTok{116}\OperatorTok{+}\DecValTok{21}\OperatorTok{+}\DecValTok{14}\NormalTok{)}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8727273
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{errorrate <-}\StringTok{ }\DecValTok{1}\OperatorTok{-}\NormalTok{correctpred}
\NormalTok{errorrate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1272727
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  Perform KNN on the training data, with several values of K, in order
  to predict mpg01. Use only the variables that seemed most associated
  with mpg01 in (b). What test errors do you obtain? Which value of K
  seems to perform the best on this data set?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(class)}
\CommentTok{#k=1}
\NormalTok{Auto_training_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{acceleration,train1}\OperatorTok{$}\NormalTok{horsepower,train1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_test_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(test1}\OperatorTok{$}\NormalTok{acceleration,test1}\OperatorTok{$}\NormalTok{horsepower,test1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{mpg01)}
\NormalTok{Auto_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, }\DataTypeTok{k=}\DecValTok{1}\NormalTok{)}

\KeywordTok{print}\NormalTok{(}\StringTok{"K=1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "K=1"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(Auto_Model_knn, test1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               
## Auto_Model_knn  0  1
##              0 51  7
##              1  8 51
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#k=2}
\NormalTok{Auto_training_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{acceleration,train1}\OperatorTok{$}\NormalTok{horsepower,train1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_test_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(test1}\OperatorTok{$}\NormalTok{acceleration,test1}\OperatorTok{$}\NormalTok{horsepower,test1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{mpg01)}
\NormalTok{Auto_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, }\DataTypeTok{k=}\DecValTok{2}\NormalTok{)}

\KeywordTok{print}\NormalTok{(}\StringTok{"K=2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "K=2"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(Auto_Model_knn, test1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               
## Auto_Model_knn  0  1
##              0 53  7
##              1  6 51
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#k=3}
\NormalTok{Auto_training_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{acceleration,train1}\OperatorTok{$}\NormalTok{horsepower,train1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_test_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(test1}\OperatorTok{$}\NormalTok{acceleration,test1}\OperatorTok{$}\NormalTok{horsepower,test1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{mpg01)}
\NormalTok{Auto_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, }\DataTypeTok{k=}\DecValTok{3}\NormalTok{)}

\KeywordTok{print}\NormalTok{(}\StringTok{"K=3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "K=3"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(Auto_Model_knn, test1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               
## Auto_Model_knn  0  1
##              0 54  4
##              1  5 54
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#k=5}
\NormalTok{Auto_training_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{acceleration,train1}\OperatorTok{$}\NormalTok{horsepower,train1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_test_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(test1}\OperatorTok{$}\NormalTok{acceleration,test1}\OperatorTok{$}\NormalTok{horsepower,test1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{mpg01)}
\NormalTok{Auto_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}

\KeywordTok{print}\NormalTok{(}\StringTok{"K=5"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "K=5"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(Auto_Model_knn, test1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               
## Auto_Model_knn  0  1
##              0 54  5
##              1  5 53
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#K=10}
\NormalTok{Auto_training_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{acceleration,train1}\OperatorTok{$}\NormalTok{horsepower,train1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_test_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(test1}\OperatorTok{$}\NormalTok{acceleration,test1}\OperatorTok{$}\NormalTok{horsepower,test1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{mpg01)}
\NormalTok{Auto_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, }\DataTypeTok{k=}\DecValTok{10}\NormalTok{)}

\KeywordTok{print}\NormalTok{(}\StringTok{"K=10"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "K=10"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(Auto_Model_knn, test1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               
## Auto_Model_knn  0  1
##              0 54  4
##              1  5 54
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#k=21}
\NormalTok{Auto_training_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{acceleration,train1}\OperatorTok{$}\NormalTok{horsepower,train1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_test_predictor <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(test1}\OperatorTok{$}\NormalTok{acceleration,test1}\OperatorTok{$}\NormalTok{horsepower,test1}\OperatorTok{$}\NormalTok{weight)}
\NormalTok{Auto_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train1}\OperatorTok{$}\NormalTok{mpg01)}
\NormalTok{Auto_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Auto_training_predictor, Auto_test_predictor, Auto_training_outcome, }\DataTypeTok{k=}\DecValTok{21}\NormalTok{)}

\KeywordTok{print}\NormalTok{(}\StringTok{"K=21"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "K=21"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(Auto_Model_knn, test1}\OperatorTok{$}\NormalTok{mpg01)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               
## Auto_Model_knn  0  1
##              0 54  3
##              1  5 55
\end{verbatim}

K=21 performed the best with thee samples, but not significantly
different from K=5=3=10.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  (10 pts) Download the csv files zipcode\_train.csv and
  zipcode\_test.csv. Load and visualize the files using the following
  command lines.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"zipcode_train.csv"}\NormalTok{)}
\NormalTok{train.dat}\OperatorTok{$}\NormalTok{Y <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(train.dat}\OperatorTok{$}\NormalTok{Y)}
\NormalTok{test.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"zipcode_test.csv"}\NormalTok{)}
\NormalTok{test.dat}\OperatorTok{$}\NormalTok{Y <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(test.dat}\OperatorTok{$}\NormalTok{Y)}
\NormalTok{COLORS <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"white"}\NormalTok{, }\StringTok{"black"}\NormalTok{)}
\NormalTok{CUSTOM_COLORS <-}\StringTok{ }\KeywordTok{colorRampPalette}\NormalTok{(}\DataTypeTok{colors =}\NormalTok{ COLORS)}
\NormalTok{vis <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(i)\{}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{pty =} \StringTok{"s"}\NormalTok{, }\DataTypeTok{mar =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{xaxt =} \StringTok{"n"}\NormalTok{, }\DataTypeTok{yaxt =} \StringTok{"n"}\NormalTok{)}
\NormalTok{z <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(train.dat[i,}\DecValTok{1}\OperatorTok{:}\DecValTok{256}\NormalTok{]), }\DecValTok{16}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\KeywordTok{image}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{,}\DecValTok{1}\OperatorTok{:}\DecValTok{16}\NormalTok{,z[,}\DecValTok{16}\OperatorTok{:}\DecValTok{1}\NormalTok{], }\DataTypeTok{col =} \KeywordTok{CUSTOM_COLORS}\NormalTok{(}\DecValTok{256}\NormalTok{))}
\NormalTok{\}}
\KeywordTok{vis}\NormalTok{(}\DecValTok{2}\NormalTok{) }\CommentTok{# hand written 1 (from 1 to 1005)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-37-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vis}\NormalTok{(}\DecValTok{1500}\NormalTok{) }\CommentTok{# hand written 2 (from 1006 to 1736)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STT481Hw2_files/figure-latex/unnamed-chunk-37-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(test.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       p1     p2     p3     p4     p5     p6     p7     p8     p9    p10
## 1 -0.996  0.572  0.396  0.063 -0.506 -0.847 -1.000 -1.000 -1.000 -1.000
## 2 -1.000 -1.000  0.469  0.413  1.000  1.000  0.462 -0.116 -0.937 -1.000
## 3 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.586  0.693  1.000  0.802
## 4 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
## 5 -1.000 -1.000 -1.000 -0.831  0.047  0.140  0.947  0.813  0.012 -0.768
## 6 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.665  0.603  1.000  0.646
##      p11 p12 p13 p14 p15 p16 p17    p18    p19    p20    p21    p22   p23
## 1 -1.000  -1  -1  -1  -1  -1  -1 -0.391  0.974  1.000  1.000  0.954 0.356
## 2 -1.000  -1  -1  -1  -1  -1  -1 -1.000 -0.393  0.822  0.840  0.996 1.000
## 3 -0.524  -1  -1  -1  -1  -1  -1 -1.000 -1.000 -1.000 -1.000 -0.998 0.582
## 4 -1.000  -1  -1  -1  -1  -1  -1 -1.000 -1.000 -1.000 -0.969 -0.286 0.487
## 5 -1.000  -1  -1  -1  -1  -1  -1 -1.000 -0.563  0.715  1.000  1.000 1.000
## 6 -0.836  -1  -1  -1  -1  -1  -1 -1.000 -1.000 -1.000 -1.000 -0.232 0.848
##      p24    p25    p26    p27    p28 p29 p30 p31 p32 p33 p34    p35    p36
## 1 -0.470 -1.000 -1.000 -1.000 -1.000  -1  -1  -1  -1  -1  -1 -0.716 -0.170
## 2  1.000  0.697 -0.597 -1.000 -1.000  -1  -1  -1  -1  -1  -1 -1.000 -1.000
## 3  1.000  1.000  0.798 -0.446 -1.000  -1  -1  -1  -1  -1  -1 -1.000 -1.000
## 4  0.934  0.856 -0.269 -0.869 -1.000  -1  -1  -1  -1  -1  -1 -1.000 -0.719
## 5  1.000  1.000  0.976  0.039 -0.905  -1  -1  -1  -1  -1  -1  0.056  1.000
## 6  0.915  0.585  1.000  0.683 -0.799  -1  -1  -1  -1  -1  -1 -1.000 -1.000
##      p37    p38    p39    p40    p41    p42    p43    p44 p45 p46 p47 p48
## 1  0.307  0.851  1.000  0.955 -0.228 -1.000 -1.000 -1.000  -1  -1  -1  -1
## 2 -1.000 -0.567 -0.405  0.376  0.919  0.945 -0.536 -1.000  -1  -1  -1  -1
## 3 -1.000 -0.856  0.932  1.000  0.859 -0.771 -1.000 -1.000  -1  -1  -1  -1
## 4  0.612  0.996  1.000  1.000  1.000  1.000  0.716 -0.600  -1  -1  -1  -1
## 5  1.000  1.000  1.000  1.000  1.000  1.000  1.000 -0.034  -1  -1  -1  -1
## 6  0.293  1.000  0.613 -0.781 -1.000 -0.081  1.000  0.081  -1  -1  -1  -1
##   p49 p50    p51    p52    p53    p54    p55    p56    p57    p58    p59
## 1  -1  -1 -1.000 -1.000 -1.000 -0.975 -0.422  0.581  0.996 -0.129 -0.993
## 2  -1  -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.660  0.678  0.858
## 3  -1  -1 -1.000 -1.000 -1.000 -0.432  1.000  1.000  0.079 -1.000 -1.000
## 4  -1  -1 -0.997  0.415  1.000  1.000  1.000  1.000  1.000  1.000  1.000
## 5  -1  -1  0.538  1.000  1.000  1.000  0.692 -0.081 -0.067  0.932  1.000
## 6  -1  -1 -1.000 -0.592  0.937  0.774 -0.744 -1.000 -1.000 -0.881  0.851
##      p60    p61 p62 p63 p64 p65 p66   p67    p68    p69    p70    p71
## 1 -1.000 -1.000  -1  -1  -1  -1  -1 -1.00 -1.000 -1.000 -1.000 -1.000
## 2 -0.704 -1.000  -1  -1  -1  -1  -1 -1.00 -1.000 -1.000 -1.000 -1.000
## 3 -1.000 -1.000  -1  -1  -1  -1  -1 -1.00 -1.000 -1.000 -0.518  1.000
## 4  0.537 -1.000  -1  -1  -1  -1  -1 -1.00  0.372  1.000  0.884  0.089
## 5  0.843 -0.911  -1  -1  -1  -1  -1 -0.16  0.942  0.716  0.125 -0.849
## 6  0.828 -0.923  -1  -1  -1  -1  -1 -1.00 -0.478  0.970 -0.492 -1.000
##      p72    p73    p74    p75    p76    p77 p78 p79 p80 p81 p82 p83    p84
## 1 -0.867  0.494  1.000 -0.366 -1.000 -1.000  -1  -1  -1  -1  -1  -1 -1.000
## 2 -1.000 -1.000 -0.561  0.911  0.663 -0.968  -1  -1  -1  -1  -1  -1 -1.000
## 3  1.000 -0.413 -1.000 -1.000 -1.000 -1.000  -1  -1  -1  -1  -1  -1 -1.000
## 4 -0.383 -0.741  0.211  1.000  0.998 -0.640  -1  -1  -1  -1  -1  -1 -0.959
## 5 -1.000 -0.855  0.805  1.000  1.000 -0.101  -1  -1  -1  -1  -1  -1 -0.706
## 6 -1.000 -1.000 -1.000  0.273  1.000 -0.398  -1  -1  -1  -1  -1  -1 -1.000
##      p85    p86 p87 p88    p89    p90    p91   p92    p93 p94 p95 p96 p97
## 1 -1.000 -1.000  -1  -1 -0.826  0.967  0.631 -0.99 -1.000  -1  -1  -1  -1
## 2 -1.000 -1.000  -1  -1 -1.000 -1.000  0.014  1.00 -0.174  -1  -1  -1  -1
## 3 -1.000 -0.469   1   1 -0.512 -1.000 -1.000 -1.00 -1.000  -1  -1  -1  -1
## 4 -0.419 -0.723  -1  -1 -1.000  0.563  1.000  1.00 -0.370  -1  -1  -1  -1
## 5 -0.803 -1.000  -1  -1 -0.381  1.000  1.000  1.00  0.164  -1  -1  -1  -1
## 6 -1.000 -1.000  -1  -1 -1.000 -1.000 -0.425  1.00  0.175  -1  -1  -1  -1
##   p98 p99 p100 p101   p102 p103 p104   p105   p106   p107   p108   p109
## 1  -1  -1   -1   -1 -1.000   -1   -1 -0.938  0.915  1.000 -0.386 -1.000
## 2  -1  -1   -1   -1 -1.000   -1   -1 -1.000 -1.000 -0.995  0.648  0.964
## 3  -1  -1   -1   -1 -0.235    1    1 -0.352 -1.000 -1.000 -1.000 -1.000
## 4  -1  -1   -1   -1 -1.000   -1   -1 -0.973  0.488  1.000  1.000 -0.198
## 5  -1  -1   -1   -1 -1.000   -1   -1  0.149  1.000  1.000  1.000 -0.295
## 6  -1  -1   -1   -1 -1.000   -1   -1 -1.000 -1.000 -0.865  0.992  0.740
##     p110 p111 p112 p113 p114 p115 p116 p117   p118   p119   p120   p121
## 1 -1.000   -1   -1   -1   -1   -1   -1   -1 -1.000 -1.000 -1.000 -0.999
## 2 -0.777   -1   -1   -1   -1   -1   -1   -1 -1.000 -1.000 -1.000 -1.000
## 3 -1.000   -1   -1   -1   -1   -1   -1   -1 -0.602  0.998  1.000 -0.376
## 4 -1.000   -1   -1   -1   -1   -1   -1   -1 -1.000 -1.000 -1.000 -0.404
## 5 -1.000   -1   -1   -1   -1   -1   -1   -1 -1.000 -1.000 -0.722  0.915
## 6 -0.992   -1   -1   -1   -1   -1   -1   -1 -1.000 -1.000 -1.000 -1.000
##     p122 p123   p124   p125   p126 p127 p128   p129 p130  p131   p132
## 1  0.629    1 -0.039 -1.000 -1.000   -1   -1 -1.000   -1 -1.00 -1.000
## 2 -1.000   -1  0.392  1.000  0.062   -1   -1 -1.000   -1 -1.00 -1.000
## 3 -1.000   -1 -1.000 -1.000 -1.000   -1   -1 -1.000   -1 -1.00 -1.000
## 4  0.996    1  0.980 -0.605 -1.000   -1   -1 -0.825   -1 -0.89 -0.548
## 5  1.000    1  0.968 -0.512 -1.000   -1   -1 -1.000   -1 -1.00 -1.000
## 6 -1.000   -1  0.929  1.000 -0.929   -1   -1 -1.000   -1 -1.00 -1.000
##     p133   p134   p135   p136   p137   p138   p139   p140   p141   p142
## 1 -1.000 -1.000 -1.000 -1.000 -1.000  0.283  1.000 -0.022 -1.000 -1.000
## 2 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.227  1.000  0.297
## 3 -1.000 -0.902  0.928  1.000 -0.204 -1.000 -1.000 -1.000 -1.000 -1.000
## 4 -0.548 -0.548 -0.892 -0.727  0.746  1.000  1.000  0.783 -0.886 -1.000
## 5 -1.000 -1.000 -1.000  0.190  1.000  1.000  1.000  0.445 -0.986 -1.000
## 6 -1.000 -0.331  0.216  0.500  0.500 -0.260 -0.918  0.899  0.918 -0.899
##   p143 p144   p145   p146   p147   p148   p149   p150   p151   p152   p153
## 1   -1   -1 -1.000 -1.000 -1.000 -0.956 -0.450  0.274  0.283  0.283  0.232
## 2   -1   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
## 3   -1   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -0.997  0.679  1.000 -0.229
## 4   -1   -1  0.236 -0.724  0.595  1.000  1.000  1.000  0.780  0.845  1.000
## 5   -1   -1 -0.981  0.404  0.838  0.838  0.684 -0.027  0.735  0.953  1.000
## 6   -1   -1 -1.000 -1.000 -1.000 -1.000  0.033  1.000  1.000  1.000  1.000
##     p154   p155   p156   p157   p158   p159 p160   p161   p162   p163
## 1  0.399  1.000  0.051 -1.000 -1.000 -1.000   -1 -1.000 -1.000 -0.582
## 2 -1.000 -1.000 -0.436  1.000  0.878 -0.961   -1 -1.000 -1.000 -1.000
## 3 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000   -1 -1.000 -1.000 -1.000
## 4  1.000  0.824 -0.704 -1.000 -1.000 -1.000   -1 -0.418  0.751  1.000
## 5  1.000  0.974 -0.559 -1.000 -1.000 -1.000   -1 -0.456  0.999  1.000
## 6  1.000  0.750  1.000  0.467 -1.000 -1.000   -1 -1.000 -1.000 -0.954
##     p164 p165 p166   p167 p168   p169   p170   p171   p172   p173   p174
## 1  0.716    1  1.0  1.000    1  1.000  1.000  1.000  0.283 -0.987 -1.000
## 2 -1.000   -1 -1.0 -1.000   -1 -1.000 -1.000 -1.000 -0.364  1.000  0.981
## 3 -1.000   -1 -1.0  0.461    1  0.128 -1.000 -1.000 -1.000 -1.000 -1.000
## 4  1.000    1  1.0  1.000    1  1.000  0.986 -0.491 -1.000 -1.000 -1.000
## 5  1.000    1  1.0  1.000    1  1.000  0.971 -0.161 -1.000 -1.000 -1.000
## 6  0.289    1  0.1 -0.809   -1 -0.816  0.400  1.000  1.000  0.165 -1.000
##     p175 p176   p177   p178   p179   p180   p181   p182   p183   p184
## 1 -1.000   -1 -1.000 -0.230  0.842  1.000  1.000  0.675 -0.022 -0.068
## 2 -0.855   -1 -1.000 -1.000 -0.407  0.357  0.600  0.562 -0.480 -1.000
## 3 -1.000   -1 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  0.313  1.000
## 4 -1.000   -1 -0.982  0.743  1.000  1.000  1.000  1.000  1.000  1.000
## 5 -1.000   -1 -0.114  1.000  1.000  1.000  1.000  1.000  1.000  1.000
## 6 -1.000   -1 -1.000 -1.000 -0.863  1.000  0.613 -1.000 -1.000 -1.000
##     p185   p186   p187   p188   p189   p190  p191 p192   p193   p194
## 1  1.000  1.000  1.000  1.000  0.417 -0.929 -1.00   -1 -1.000  0.428
## 2 -1.000 -1.000 -1.000 -0.113  1.000  0.918 -0.89   -1 -1.000 -0.843
## 3  0.325 -1.000 -1.000 -1.000 -1.000 -1.000 -1.00   -1 -1.000 -1.000
## 4  1.000  1.000  0.326 -0.543 -0.965 -1.000 -1.00   -1 -1.000 -0.299
## 5  1.000  0.998  0.204 -0.845 -1.000 -1.000 -1.00   -1  0.275  1.000
## 6 -1.000 -0.363  1.000  1.000 -0.137 -1.000 -1.00   -1 -1.000 -1.000
##     p195   p196   p197   p198   p199   p200   p201   p202   p203   p204
## 1  1.000  0.764 -0.277 -0.916 -0.101  0.768  1.000  0.975 -0.224 -0.145
## 2  0.951  1.000  1.000  1.000  0.966  0.104 -0.914 -1.000 -0.436  0.778
## 3 -1.000 -1.000 -1.000 -1.000  0.141  1.000  0.595 -0.999 -1.000 -1.000
## 4  0.964  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000
## 5  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  0.777
## 6 -0.436  1.000  0.061 -1.000 -1.000 -0.352  0.541  1.000  1.000  1.000
##     p205   p206   p207   p208   p209   p210   p211   p212   p213   p214
## 1  0.923  0.642 -0.256 -1.000 -1.000  0.719  1.000 -0.163  0.039  0.824
## 2  1.000  0.749 -0.972 -1.000 -1.000 -0.917  0.917  1.000  1.000  0.886
## 3 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
## 4  0.596  0.485  0.229 -0.576 -1.000 -1.000 -0.152  0.896  1.000  1.000
## 5 -0.472 -0.946 -0.955 -1.000 -0.595  0.997  1.000  1.000  1.000  1.000
## 6  0.561 -1.000 -1.000 -1.000 -1.000 -1.000 -0.870  0.986  0.750  1.000
##     p215  p216   p217   p218   p219 p220   p221   p222   p223   p224 p225
## 1  1.000 1.000  0.940  0.030 -1.000   -1 -0.749 -0.016  0.040 -0.996   -1
## 2  0.807 1.000  0.828  0.760  0.951    1  1.000 -0.243 -1.000 -1.000   -1
## 3 -0.351 1.000  0.731 -0.988 -1.000   -1 -1.000 -1.000 -1.000 -1.000   -1
## 4  0.903 0.143 -0.753  0.348  0.909    1  1.000  1.000  1.000  0.448   -1
## 5  1.000 0.693  0.675  0.929  1.000    1  1.000  1.000  0.938 -0.286   -1
## 6  1.000 1.000  1.000  0.638 -0.014    1  0.995 -0.611 -1.000 -1.000   -1
##     p226   p227   p228   p229   p230   p231   p232   p233   p234   p235
## 1  0.554  1.000  1.000  1.000  1.000  0.791  0.325 -0.673 -1.000 -1.000
## 2 -0.999  0.432  1.000  1.000  0.973  0.925  1.000  1.000  1.000  1.000
## 3 -1.000 -1.000 -1.000 -1.000 -1.000 -0.843  0.851  0.977 -0.593 -1.000
## 4 -1.000 -1.000 -0.634  0.325  0.593 -0.245 -1.000 -1.000 -1.000 -0.735
## 5 -0.196  1.000  1.000  1.000  1.000  0.066 -0.914 -1.000 -0.634  0.097
## 6 -1.000 -1.000 -0.305  0.722  1.000  0.652  0.398 -0.232 -0.986 -0.582
##     p236   p237   p238  p239   p240 p241   p242   p243   p244   p245
## 1 -1.000 -1.000 -1.000 -1.00 -1.000   -1 -0.605  0.718  0.972  0.398
## 2  0.793 -0.136 -0.969 -1.00 -1.000   -1 -1.000 -0.979 -0.114  0.552
## 3 -1.000 -1.000 -1.000 -1.00 -1.000   -1 -1.000 -1.000 -1.000 -1.000
## 4  0.000  0.160  0.160 -0.38 -0.867   -1 -1.000 -1.000 -1.000 -1.000
## 5  0.763  1.000  1.000  1.00  0.338   -1 -0.996  0.226  1.000  0.936
## 6  1.000  1.000 -0.543 -1.00 -1.000   -1 -1.000 -1.000 -1.000 -1.000
##     p246   p247   p248   p249   p250  p251   p252   p253   p254   p255
## 1  0.165 -0.668 -1.000 -1.000 -1.000 -1.00 -1.000 -1.000 -1.000 -1.000
## 2  1.000  1.000  1.000  1.000  0.270 -0.28 -0.855 -1.000 -1.000 -1.000
## 3 -1.000 -1.000 -0.601  0.592  0.219 -1.00 -1.000 -1.000 -1.000 -1.000
## 4 -1.000 -1.000 -1.000 -1.000 -1.000 -1.00 -1.000 -1.000 -1.000 -1.000
## 5 -0.221 -0.915 -1.000 -1.000 -1.000 -1.00 -0.866 -0.672  0.131  0.135
## 6 -1.000 -1.000 -1.000 -1.000 -1.000 -1.00  0.720  0.711 -0.932 -1.000
##     p256 Y
## 1 -1.000 2
## 2 -1.000 2
## 3 -1.000 1
## 4 -1.000 2
## 5 -0.318 2
## 6 -1.000 2
\end{verbatim}

In the dataset, you have 1736 training images and 462 test images, where
each image is a handwritten digit and it can be either 1 or 2. The
description of columns is below:

p1-256: the gray scale from -1 to 1. Y: the digit, which is either 1 or
2.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Perform logistic regression, LDA, and KNN models.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"LDA"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "LDA"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.dat2 <-}\StringTok{ }\NormalTok{test.dat[,}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{32}\NormalTok{)]}
\NormalTok{train.dat2 <-}\StringTok{ }\NormalTok{train.dat[,}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{32}\NormalTok{)] }
\CommentTok{# Remove co linear variables }
\NormalTok{image_training_lda <-}\StringTok{ }\KeywordTok{lda}\NormalTok{(Y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{train.dat2)}

\CommentTok{#Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(image_training_lda,}\DataTypeTok{newdata =}\NormalTok{ test.dat2, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{class <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{table}\NormalTok{(class,test.dat2}\OperatorTok{$}\NormalTok{Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
## class   1   2
##     1 259   2
##     2   5 196
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{196}\OperatorTok{+}\DecValTok{259}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{196}\OperatorTok{+}\DecValTok{259}\OperatorTok{+}\DecValTok{7}\NormalTok{)}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9848485
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"glm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "glm"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{image_training_glm <-}\StringTok{ }\KeywordTok{lda}\NormalTok{(Y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{train.dat2, }\DataTypeTok{family=}\NormalTok{binomial)}

\CommentTok{#Confusion Matrix}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(image_training_glm,}\DataTypeTok{newdata =}\NormalTok{ test.dat, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{class <-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{class}
\KeywordTok{table}\NormalTok{(class,test.dat}\OperatorTok{$}\NormalTok{Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      
## class   1   2
##     1 259   2
##     2   5 196
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correctpred <-}\StringTok{ }\NormalTok{(}\DecValTok{196}\OperatorTok{+}\DecValTok{259}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{196}\OperatorTok{+}\DecValTok{259}\OperatorTok{+}\DecValTok{7}\NormalTok{)}
\NormalTok{correctpred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9848485
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\StringTok{"KNN, K=1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "KNN, K=1"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training_predictordf <-}\StringTok{ }\NormalTok{train.dat[,}\DecValTok{1}\OperatorTok{:}\DecValTok{256}\NormalTok{]}
\NormalTok{test_predictordf <-}\StringTok{ }\NormalTok{test.dat[,}\DecValTok{1}\OperatorTok{:}\DecValTok{256}\NormalTok{]}

\NormalTok{image_training_outcome <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(train.dat}\OperatorTok{$}\NormalTok{Y)}
\NormalTok{image_Model_knn <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(training_predictordf, test_predictordf, image_training_outcome, }\DataTypeTok{k=}\DecValTok{1}\NormalTok{)}


\KeywordTok{print}\NormalTok{(}\StringTok{"K=1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "K=1"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(image_Model_knn, test.dat}\OperatorTok{$}\NormalTok{Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                
## image_Model_knn   1   2
##               1 260   2
##               2   4 196
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Using the test.dat, which of these methods appears to provide the best
  results on the test data?
\end{enumerate}

The Knn with k=1 performs the best and has the lowest error rate


\end{document}
